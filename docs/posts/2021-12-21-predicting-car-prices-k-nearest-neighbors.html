<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Migs Germar">
<meta name="dcterms.date" content="2021-12-21">
<meta name="description" content="I use various machine learning workflow techniques to arrive at the optimal K Nearest Neighbors (KNN) regression model for predicting car prices.">

<title>Migs Germar’s Data Science Blog - Predicting Car Prices using the K Nearest Neighbors Algorithm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../posts/images/favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-99SLFW8P3J"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-99SLFW8P3J', { 'anonymize_ip': true});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Migs Germar’s Data Science Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/MiguelAHG" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/migs-germar/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.facebook.com/miguelantonio.germar" rel="" target=""><i class="bi bi-facebook" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Predicting Car Prices using the K Nearest Neighbors Algorithm</h1>
                  <div>
        <div class="description">
          I use various machine learning workflow techniques to arrive at the optimal K Nearest Neighbors (KNN) regression model for predicting car prices.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">python</div>
                <div class="quarto-category">pandas</div>
                <div class="quarto-category">matplotlib</div>
                <div class="quarto-category">seaborn</div>
                <div class="quarto-category">scipy</div>
                <div class="quarto-category">sklearn</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Migs Germar </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 21, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#data-inspection-and-cleaning" id="toc-data-inspection-and-cleaning" class="nav-link" data-scroll-target="#data-inspection-and-cleaning">Data Inspection and Cleaning</a></li>
  <li><a href="#the-k-nearest-neighbors-algorithm" id="toc-the-k-nearest-neighbors-algorithm" class="nav-link" data-scroll-target="#the-k-nearest-neighbors-algorithm">The K Nearest Neighbors Algorithm</a></li>
  <li><a href="#techniques-for-implementation" id="toc-techniques-for-implementation" class="nav-link" data-scroll-target="#techniques-for-implementation">Techniques for Implementation</a>
  <ul class="collapse">
  <li><a href="#standardization" id="toc-standardization" class="nav-link" data-scroll-target="#standardization">Standardization</a></li>
  <li><a href="#feature-selection" id="toc-feature-selection" class="nav-link" data-scroll-target="#feature-selection">Feature Selection</a></li>
  <li><a href="#train-test-split-with-stratification" id="toc-train-test-split-with-stratification" class="nav-link" data-scroll-target="#train-test-split-with-stratification">Train-Test Split with Stratification</a></li>
  <li><a href="#hyperparameter-optimization" id="toc-hyperparameter-optimization" class="nav-link" data-scroll-target="#hyperparameter-optimization">Hyperparameter Optimization</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation">K-Fold Cross-Validation</a></li>
  </ul></li>
  <li><a href="#combining-techniques" id="toc-combining-techniques" class="nav-link" data-scroll-target="#combining-techniques">Combining Techniques</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a>
  <ul class="collapse">
  <li><a href="#data-source" id="toc-data-source" class="nav-link" data-scroll-target="#data-source">Data Source</a></li>
  <li><a href="#information-sources" id="toc-information-sources" class="nav-link" data-scroll-target="#information-sources">Information Sources</a></li>
  <li><a href="#image-source" id="toc-image-source" class="nav-link" data-scroll-target="#image-source">Image Source</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/notebook-images/knn-car-prices/two-cars.jfif" class="img-fluid figure-img"></p>
</figure>
</div>
<center>
<a href="https://unsplash.com/photos/gKXKBY-C-Dk">Wheelscene | Chris Smith</a>
</center>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>K Nearest Neighbors or KNN is an an algorithm that can make predictions based on the similarity between different observations. In this project, I used KNN to predict the price of a car based on how similar its features are to those of other cars. Towards this end, I applied various machine learning techniques, such as standardization, feature selection, train-test split, hyperparameter optimization, and k-fold cross validation.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I wrote this notebook by following a guided project on the <a href="https://www.dataquest.io/">Dataquest</a> platform, specifically the <a href="https://app.dataquest.io/c/36/m/155/guided-project%3A-predicting-car-prices/1/introduction-to-the-data-set">Guided Project: Predicting Car Prices</a>. The general project flow and research questions were guided by Dataquest. Furthermore, though the mathematical explanations in this post were written in my own words, I learned the theory from Dataquest.</p>
</div>
</div>
<p>Below are the packages used in this project.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> zscore</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, cross_val_score, train_test_split</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> f_regression, SelectKBest</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="data-inspection-and-cleaning" class="level1">
<h1>Data Inspection and Cleaning</h1>
<p>The dataset for this project is the Automobile Data Set by Schlimmer (1987), from the UCI Machine Learning Repository. The data and its description can be obtained <a href="https://archive.ics.uci.edu/ml/datasets/automobile">here</a>.</p>
<p>The dataset describes 26 features of hundreds of cars. A summary of the features and their data types is shown below.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data dictionary from documentation.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>data_dict <span class="op">=</span> <span class="st">"""1. symboling: -3, -2, -1, 0, 1, 2, 3.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="st">2. normalized-losses: continuous from 65 to 256.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="st">3. make:</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="st">alfa-romero, audi, bmw, chevrolet, dodge, honda,</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="st">isuzu, jaguar, mazda, mercedes-benz, mercury,</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="st">mitsubishi, nissan, peugot, plymouth, porsche,</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="st">renault, saab, subaru, toyota, volkswagen, volvo</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="st">4. fuel-type: diesel, gas.</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="st">5. aspiration: std, turbo.</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="st">6. num-of-doors: four, two.</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="st">7. body-style: hardtop, wagon, sedan, hatchback, convertible.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="st">8. drive-wheels: 4wd, fwd, rwd.</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="st">9. engine-location: front, rear.</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="st">10. wheel-base: continuous from 86.6 120.9.</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="st">11. length: continuous from 141.1 to 208.1.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="st">12. width: continuous from 60.3 to 72.3.</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="st">13. height: continuous from 47.8 to 59.8.</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="st">14. curb-weight: continuous from 1488 to 4066.</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="st">15. engine-type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="st">16. num-of-cylinders: eight, five, four, six, three, twelve, two.</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="st">17. engine-size: continuous from 61 to 326.</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="st">18. fuel-system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="st">19. bore: continuous from 2.54 to 3.94.</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="st">20. stroke: continuous from 2.07 to 4.17.</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="st">21. compression-ratio: continuous from 7 to 23.</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="st">22. horsepower: continuous from 48 to 288.</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="st">23. peak-rpm: continuous from 4150 to 6600.</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="st">24. city-mpg: continuous from 13 to 49.</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="st">25. highway-mpg: continuous from 16 to 54.</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="st">26. price: continuous from 5118 to 45400."""</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Use regex to extract column names from data dictionary.</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>col_names <span class="op">=</span> re.findall(</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    pattern <span class="op">=</span> <span class="vs">r"^[0-9]{1,2}\. ([a-z\-]+):"</span>,</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    string <span class="op">=</span> data_dict,</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use multiline flag so that ^ indicates the start of a line.</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    flags <span class="op">=</span> re.MULTILINE,</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Read data file and add column names.</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>cars_df <span class="op">=</span> pd.read_csv(</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="st">"./private/Car-Prices-KNN-Files/imports-85.data"</span>,</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    names <span class="op">=</span> col_names,</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>cars_df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 205 entries, 0 to 204
Data columns (total 26 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   symboling          205 non-null    int64  
 1   normalized-losses  205 non-null    object 
 2   make               205 non-null    object 
 3   fuel-type          205 non-null    object 
 4   aspiration         205 non-null    object 
 5   num-of-doors       205 non-null    object 
 6   body-style         205 non-null    object 
 7   drive-wheels       205 non-null    object 
 8   engine-location    205 non-null    object 
 9   wheel-base         205 non-null    float64
 10  length             205 non-null    float64
 11  width              205 non-null    float64
 12  height             205 non-null    float64
 13  curb-weight        205 non-null    int64  
 14  engine-type        205 non-null    object 
 15  num-of-cylinders   205 non-null    object 
 16  engine-size        205 non-null    int64  
 17  fuel-system        205 non-null    object 
 18  bore               205 non-null    object 
 19  stroke             205 non-null    object 
 20  compression-ratio  205 non-null    float64
 21  horsepower         205 non-null    object 
 22  peak-rpm           205 non-null    object 
 23  city-mpg           205 non-null    int64  
 24  highway-mpg        205 non-null    int64  
 25  price              205 non-null    object 
dtypes: float64(5), int64(5), object(16)
memory usage: 41.8+ KB</code></pre>
</div>
</div>
<p>There are 205 cars and 26 features. Most of the features directly describe physical characteristics of the cars. Some exceptions are “symboling” and “normalized-losses”, which are values related to car insurance and are beyond the scope of this project. Also, the “price” column provides the price of each car in USD.</p>
<p>Let us look at the first five rows.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>cars_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">symboling</th>
<th data-quarto-table-cell-role="th">normalized-losses</th>
<th data-quarto-table-cell-role="th">make</th>
<th data-quarto-table-cell-role="th">fuel-type</th>
<th data-quarto-table-cell-role="th">aspiration</th>
<th data-quarto-table-cell-role="th">num-of-doors</th>
<th data-quarto-table-cell-role="th">body-style</th>
<th data-quarto-table-cell-role="th">drive-wheels</th>
<th data-quarto-table-cell-role="th">engine-location</th>
<th data-quarto-table-cell-role="th">wheel-base</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">engine-size</th>
<th data-quarto-table-cell-role="th">fuel-system</th>
<th data-quarto-table-cell-role="th">bore</th>
<th data-quarto-table-cell-role="th">stroke</th>
<th data-quarto-table-cell-role="th">compression-ratio</th>
<th data-quarto-table-cell-role="th">horsepower</th>
<th data-quarto-table-cell-role="th">peak-rpm</th>
<th data-quarto-table-cell-role="th">city-mpg</th>
<th data-quarto-table-cell-role="th">highway-mpg</th>
<th data-quarto-table-cell-role="th">price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>3</td>
<td>?</td>
<td>alfa-romero</td>
<td>gas</td>
<td>std</td>
<td>two</td>
<td>convertible</td>
<td>rwd</td>
<td>front</td>
<td>88.6</td>
<td>...</td>
<td>130</td>
<td>mpfi</td>
<td>3.47</td>
<td>2.68</td>
<td>9.0</td>
<td>111</td>
<td>5000</td>
<td>21</td>
<td>27</td>
<td>13495</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>3</td>
<td>?</td>
<td>alfa-romero</td>
<td>gas</td>
<td>std</td>
<td>two</td>
<td>convertible</td>
<td>rwd</td>
<td>front</td>
<td>88.6</td>
<td>...</td>
<td>130</td>
<td>mpfi</td>
<td>3.47</td>
<td>2.68</td>
<td>9.0</td>
<td>111</td>
<td>5000</td>
<td>21</td>
<td>27</td>
<td>16500</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1</td>
<td>?</td>
<td>alfa-romero</td>
<td>gas</td>
<td>std</td>
<td>two</td>
<td>hatchback</td>
<td>rwd</td>
<td>front</td>
<td>94.5</td>
<td>...</td>
<td>152</td>
<td>mpfi</td>
<td>2.68</td>
<td>3.47</td>
<td>9.0</td>
<td>154</td>
<td>5000</td>
<td>19</td>
<td>26</td>
<td>16500</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>2</td>
<td>164</td>
<td>audi</td>
<td>gas</td>
<td>std</td>
<td>four</td>
<td>sedan</td>
<td>fwd</td>
<td>front</td>
<td>99.8</td>
<td>...</td>
<td>109</td>
<td>mpfi</td>
<td>3.19</td>
<td>3.40</td>
<td>10.0</td>
<td>102</td>
<td>5500</td>
<td>24</td>
<td>30</td>
<td>13950</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>2</td>
<td>164</td>
<td>audi</td>
<td>gas</td>
<td>std</td>
<td>four</td>
<td>sedan</td>
<td>4wd</td>
<td>front</td>
<td>99.4</td>
<td>...</td>
<td>136</td>
<td>mpfi</td>
<td>3.19</td>
<td>3.40</td>
<td>8.0</td>
<td>115</td>
<td>5500</td>
<td>18</td>
<td>22</td>
<td>17450</td>
</tr>
</tbody>
</table>

<p>5 rows × 26 columns</p>
</div>
</div>
</div>
<p>If we compare the data type of each column to its contents, several opportunities for data cleaning can be seen. For example, the “normalized-losses” feature is listed as an object-type column because it contains both strings and numbers. However, the strings in the column are question marks (?). Rather than being categories, these may be placeholders for missing data. This problem applies to several other columns, not just this one.</p>
<p>Furthermore, in some columns like “num-of-doors”, numbers are written as words. For example, 2 is written as “two”. Since the numbers are in string format, these cannot be used in the K Nearest Neighbors model.</p>
<p>Thus, in summary, the following cleaning steps have to be performed:</p>
<ul>
<li>Replace question mark strings (“?”) with null values (NaN). These are the proper way to indicate missing values.</li>
<li>Convert several object columns, like “normalized-losses”, into numeric columns.</li>
<li>Replace numbers written as words with their proper numeric equivalents. For example, replace “four” with 4.</li>
</ul>
<p>These were performed in the following code cell.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean the data.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace ? with NaN since these are placeholders.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>cars_df <span class="op">=</span> cars_df.replace(<span class="st">"?"</span>, np.nan)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Change this object column to float type.</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>obj_to_numeric <span class="op">=</span> [</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"normalized-losses"</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bore"</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"stroke"</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"horsepower"</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"peak-rpm"</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"price"</span>,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> col <span class="kw">in</span> obj_to_numeric:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    cars_df[col] <span class="op">=</span> pd.to_numeric(cars_df[col], errors <span class="op">=</span> <span class="st">"coerce"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace strings with numeric equivalents.</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>cars_df[<span class="st">"num-of-doors"</span>] <span class="op">=</span> cars_df[<span class="st">"num-of-doors"</span>].replace(</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"four"</span>: <span class="fl">4.0</span>,</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"two"</span>: <span class="fl">2.0</span>,</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>cars_df[<span class="st">"num-of-cylinders"</span>] <span class="op">=</span> cars_df[<span class="st">"num-of-cylinders"</span>].replace(</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="st">"four"</span>: <span class="dv">4</span>,</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="st">"six"</span>: <span class="dv">6</span>,</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">"five"</span>: <span class="dv">5</span>,</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">"eight"</span>: <span class="dv">8</span>,</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="st">"two"</span>: <span class="dv">2</span>,</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="st">"three"</span>: <span class="dv">3</span>,</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        <span class="st">"twelve"</span>: <span class="dv">12</span>,</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>cars_df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 205 entries, 0 to 204
Data columns (total 26 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   symboling          205 non-null    int64  
 1   normalized-losses  164 non-null    float64
 2   make               205 non-null    object 
 3   fuel-type          205 non-null    object 
 4   aspiration         205 non-null    object 
 5   num-of-doors       203 non-null    float64
 6   body-style         205 non-null    object 
 7   drive-wheels       205 non-null    object 
 8   engine-location    205 non-null    object 
 9   wheel-base         205 non-null    float64
 10  length             205 non-null    float64
 11  width              205 non-null    float64
 12  height             205 non-null    float64
 13  curb-weight        205 non-null    int64  
 14  engine-type        205 non-null    object 
 15  num-of-cylinders   205 non-null    int64  
 16  engine-size        205 non-null    int64  
 17  fuel-system        205 non-null    object 
 18  bore               201 non-null    float64
 19  stroke             201 non-null    float64
 20  compression-ratio  205 non-null    float64
 21  horsepower         203 non-null    float64
 22  peak-rpm           203 non-null    float64
 23  city-mpg           205 non-null    int64  
 24  highway-mpg        205 non-null    int64  
 25  price              201 non-null    float64
dtypes: float64(12), int64(6), object(8)
memory usage: 41.8+ KB</code></pre>
</div>
</div>
<p>The new summary of columns is shown above. Several columns which were once “object” columns are now numeric. Also, since we replaced “?” placeholders with null values, we can now see that some columns have missing values.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>null_percs <span class="op">=</span> (</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    cars_df</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    .isnull()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    .<span class="bu">sum</span>()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    .divide(cars_df.shape[<span class="dv">0</span>])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    .multiply(<span class="dv">100</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>null_percs.loc[null_percs <span class="op">&gt;</span> <span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>normalized-losses    20.00000
num-of-doors          0.97561
bore                  1.95122
stroke                1.95122
horsepower            0.97561
peak-rpm              0.97561
price                 1.95122
dtype: float64</code></pre>
</div>
</div>
<p>The table above shows the percentage of missing values in each column that has them. In particular, “normalized-losses” has missing values in 20% of the observations. Thus, we will have to drop this column from the dataset. This is better than the alternative, which is to delete all rows where “normalized-losses” is missing.</p>
<p>As for the other 6 columns, we will use listwise deletion. This means that we will drop all rows with missing values in any of those columns.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>cars_df <span class="op">=</span> (</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    cars_df</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    .drop(<span class="st">"normalized-losses"</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    .dropna(</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        subset <span class="op">=</span> [</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">"num-of-doors"</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">"bore"</span>,</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">"stroke"</span>,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"horsepower"</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"peak-rpm"</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">"price"</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>num_null <span class="op">=</span> cars_df.isnull().<span class="bu">sum</span>().<span class="bu">sum</span>()</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total number of missing values: </span><span class="sc">{</span>num_null<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"New shape of dataset: </span><span class="sc">{</span>cars_df<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Total number of missing values: 0
New shape of dataset: (193, 25)</code></pre>
</div>
</div>
<p>Now, there are no more missing values in the dataset. There are 193 rows and 25 columns left.</p>
</section>
<section id="the-k-nearest-neighbors-algorithm" class="level1">
<h1>The K Nearest Neighbors Algorithm</h1>
<p>Next, I will discuss the theory behind the KNN algorithm, then implement it on the dataset.</p>
<p>First, let us discuss basic terminology. For your reference, below is a small part of the dataset:</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>cars_df.loc[:<span class="dv">5</span>, [<span class="st">"make"</span>, <span class="st">"fuel-type"</span>, <span class="st">"num-of-doors"</span>, <span class="st">"body-style"</span>, <span class="st">"price"</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="7">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">make</th>
<th data-quarto-table-cell-role="th">fuel-type</th>
<th data-quarto-table-cell-role="th">num-of-doors</th>
<th data-quarto-table-cell-role="th">body-style</th>
<th data-quarto-table-cell-role="th">price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>alfa-romero</td>
<td>gas</td>
<td>2.0</td>
<td>convertible</td>
<td>13495.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>alfa-romero</td>
<td>gas</td>
<td>2.0</td>
<td>convertible</td>
<td>16500.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>alfa-romero</td>
<td>gas</td>
<td>2.0</td>
<td>hatchback</td>
<td>16500.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>audi</td>
<td>gas</td>
<td>4.0</td>
<td>sedan</td>
<td>13950.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>audi</td>
<td>gas</td>
<td>4.0</td>
<td>sedan</td>
<td>17450.0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>audi</td>
<td>gas</td>
<td>2.0</td>
<td>sedan</td>
<td>15250.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Each row of data is called an observation; in this case, each observation is a car.</p>
<p>On the other hand, each column is either a feature or a target. The target is the variable that we try to predict, and the features are information used to make the prediction. In the case of this project, the features may include the size of the car, the number of doors, etc. The target is the price of the car.</p>
<p>The set of cars whose prices we will predict is called the testing set. On the other hand, the training set is the set of cars used to train the model to make predictions. Put more simply, in order to predict the price of a car in the testing set, we must compare it to the cars in the training set.</p>
<p>In order to compare cars, KNN uses the Euclidean distance as a similarity metric between two observations. A low distance close to 0 means that the observations are very similar to each other. The following formula is used:</p>
<p><span class="math inline">\(d = \sqrt{\sum_{i=1}^n (q_i - p_i)^2}\)</span></p>
<ul>
<li><span class="math inline">\(d\)</span> is the Euclidean distance.</li>
<li><span class="math inline">\(n\)</span> is the number of features.</li>
<li><span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span> each refer to a different observation in the data. In this case, each is a different car.</li>
<li><span class="math inline">\(q_i\)</span> is the value of feature <span class="math inline">\(i\)</span> for observation <span class="math inline">\(q\)</span>. For example, if feature <span class="math inline">\(1\)</span> is the number of doors, <span class="math inline">\(q_1\)</span> is the number of doors on car <span class="math inline">\(q\)</span>.</li>
<li>The differences between the two observations’ features are squared, then summed up. Finally, the square root of the sum gives the Euclidean distance.</li>
</ul>
<p>Given that we want to predict the price of a car <span class="math inline">\(q\)</span>, KNN computes the Euclidean distance of <span class="math inline">\(q\)</span> from <em>every single car in the training set</em>. The cars most similar to <span class="math inline">\(q\)</span> are its “nearest neighbors.”</p>
<p>We then choose a number <span class="math inline">\(k\)</span>, which will determine how many of the nearest neighbors will be selected. For example, if <span class="math inline">\(k = 5\)</span>, we select the five most similar cars. Then, we take the mean price of these five cars, and we predict that this is the price of car <span class="math inline">\(q\)</span>.</p>
<p>Since we make a prediction based on an observation’s <span class="math inline">\(k\)</span> nearest neighbors, the algorithm is called K Nearest Neighbors. Note that what I have described is an example of a KNN regression model, as it predicts a numeric target. There are still several other forms of KNN. Some use a different similarity metric like Manhattan distance, and some perform classification, which means that they predict a categorical target (Miller, 2019).</p>
</section>
<section id="techniques-for-implementation" class="level1">
<h1>Techniques for Implementation</h1>
<p>Unlike with my previous <a href="../posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html">post</a> on the Naive Bayes Algorithm, I will not be programming this algorithm manually. Instead, I will use the scikit-learn workflow, which involves pre-packaged machine learning functions.</p>
<p>In this part, I will individually discuss certain important techniques used in the machine learning workflow. In the next part, I will combine these techniques in order to obtain the optimal KNN model.</p>
<section id="standardization" class="level2">
<h2 class="anchored" data-anchor-id="standardization">Standardization</h2>
<p>The first important technique is standardization. So that each feature will contribute equally to the Euclidean distance, we will standardize each numeric feature. In other words, each value will be converted into a z-score so that the mean of each feature is 0 and its standard deviation is 1. The following equation is used:</p>
<p><span class="math inline">\(z = \frac{x - \bar{x}}{s}\)</span></p>
<ul>
<li><span class="math inline">\(z\)</span> is the z-score.</li>
<li><span class="math inline">\(x\)</span> is a value in a feature.</li>
<li><span class="math inline">\(\bar{x}\)</span> is the mean of the feature.</li>
<li><span class="math inline">\(s\)</span> is the sample standard deviation.</li>
</ul>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>all_feature_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> cars_df.columns <span class="cf">if</span> col <span class="op">!=</span> <span class="st">"price"</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Series of feature:data type</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>fdt <span class="op">=</span> cars_df[all_feature_cols].dtypes</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify numeric features</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>all_numeric_features <span class="op">=</span> fdt.index[fdt <span class="op">!=</span> <span class="st">"object"</span>]</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardize</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>cars_df[all_numeric_features] <span class="op">=</span> cars_df[all_numeric_features].<span class="bu">apply</span>(zscore, axis <span class="op">=</span> <span class="dv">0</span>, ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>cars_df[all_numeric_features].head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">symboling</th>
<th data-quarto-table-cell-role="th">num-of-doors</th>
<th data-quarto-table-cell-role="th">wheel-base</th>
<th data-quarto-table-cell-role="th">length</th>
<th data-quarto-table-cell-role="th">width</th>
<th data-quarto-table-cell-role="th">height</th>
<th data-quarto-table-cell-role="th">curb-weight</th>
<th data-quarto-table-cell-role="th">num-of-cylinders</th>
<th data-quarto-table-cell-role="th">engine-size</th>
<th data-quarto-table-cell-role="th">bore</th>
<th data-quarto-table-cell-role="th">stroke</th>
<th data-quarto-table-cell-role="th">compression-ratio</th>
<th data-quarto-table-cell-role="th">horsepower</th>
<th data-quarto-table-cell-role="th">peak-rpm</th>
<th data-quarto-table-cell-role="th">city-mpg</th>
<th data-quarto-table-cell-role="th">highway-mpg</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1.782215</td>
<td>-1.172839</td>
<td>-1.678015</td>
<td>-0.442872</td>
<td>-0.839080</td>
<td>-2.117092</td>
<td>-0.025646</td>
<td>-0.410180</td>
<td>0.045098</td>
<td>0.511697</td>
<td>-1.803495</td>
<td>-0.287525</td>
<td>0.198054</td>
<td>-0.212806</td>
<td>-0.677292</td>
<td>-0.555613</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1.782215</td>
<td>-1.172839</td>
<td>-1.678015</td>
<td>-0.442872</td>
<td>-0.839080</td>
<td>-2.117092</td>
<td>-0.025646</td>
<td>-0.410180</td>
<td>0.045098</td>
<td>0.511697</td>
<td>-1.803495</td>
<td>-0.287525</td>
<td>0.198054</td>
<td>-0.212806</td>
<td>-0.677292</td>
<td>-0.555613</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.163544</td>
<td>-1.172839</td>
<td>-0.719041</td>
<td>-0.250543</td>
<td>-0.184200</td>
<td>-0.613816</td>
<td>0.496473</td>
<td>1.544506</td>
<td>0.574066</td>
<td>-2.388614</td>
<td>0.701095</td>
<td>-0.287525</td>
<td>1.330822</td>
<td>-0.212806</td>
<td>-0.990387</td>
<td>-0.702307</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>0.972880</td>
<td>0.848214</td>
<td>0.142410</td>
<td>0.182198</td>
<td>0.143240</td>
<td>0.179580</td>
<td>-0.426254</td>
<td>-0.410180</td>
<td>-0.459826</td>
<td>-0.516262</td>
<td>0.479169</td>
<td>-0.036110</td>
<td>-0.039037</td>
<td>0.853987</td>
<td>-0.207649</td>
<td>-0.115531</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.972880</td>
<td>0.848214</td>
<td>0.077395</td>
<td>0.182198</td>
<td>0.236794</td>
<td>0.179580</td>
<td>0.498371</td>
<td>0.567163</td>
<td>0.189362</td>
<td>-0.516262</td>
<td>0.479169</td>
<td>-0.538940</td>
<td>0.303427</td>
<td>0.853987</td>
<td>-1.146935</td>
<td>-1.289083</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The table above shows the first 5 rows of all of the numeric features. Notice that each feature now contains positive and negative values close to 0 because it was standardized.</p>
</section>
<section id="feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="feature-selection">Feature Selection</h2>
<p>The second technique is feature selection. We must choose features which we think are most relevant to a car’s price. We can only select numeric features since categorical ones cannot be used to calculate Euclidean distance. Thus, we must select from the following features:</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>all_numeric_features.to_list()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>['symboling',
 'num-of-doors',
 'wheel-base',
 'length',
 'width',
 'height',
 'curb-weight',
 'num-of-cylinders',
 'engine-size',
 'bore',
 'stroke',
 'compression-ratio',
 'horsepower',
 'peak-rpm',
 'city-mpg',
 'highway-mpg']</code></pre>
</div>
</div>
<p>All of these features are physical characteristics of a car, except for “symboling”. According to the dataset documentation by Schlimmer (2019), this feature is an “insurance risk rating.” It elaborates:</p>
<blockquote class="blockquote">
<p>Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process “symboling”. A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.</p>
</blockquote>
<p>Given that this feature is systematically associated with the price of a car, it may be relevant to our model. Thus, we will consider it along with the other numeric features.</p>
<p>In order to determine which combination of features is the best, we will use univariate feature selection. “Univariate” refers to the use of a single variable. We will perform a statistical test between each feature and the target. Then, we will select the features with the highest scores from the statistical test (scikit-learn developers, 2021).</p>
<p>In our case, we have a regression problem, since we want to predict a continuous variable, car price. Thus, we will use the F-statistic as our score function. According to Frost (2017), the F-statistic indicates the “overall significance” of a linear regression model. In univariate feature selection, we would do the following steps:</p>
<ul>
<li>For each feature:
<ul>
<li>Perform linear regression where the independent variable is the feature and the dependent variable is the target (in this case, price).</li>
<li>Obtain the F-statistic.</li>
</ul></li>
<li>Compile a list with the F-statistic of each feature.</li>
<li>Identify the features with the highest F-statistics.</li>
</ul>
<p>This can be implemented automatically using the scikit-learn’s <code>SelectKBest</code> class. It is called <code>SelectKBest</code> because we can set a parameter <code>k</code> which tells how many features to select. For example, if <code>k = 3</code>, the top three features with the highest F-statistic are selected. This is done below:</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>skb <span class="op">=</span> SelectKBest(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    score_func <span class="op">=</span> f_regression,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars_df[all_numeric_features]</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cars_df[<span class="st">"price"</span>]</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> skb.fit_transform(X, y)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>best_features <span class="op">=</span> <span class="bu">list</span>(skb.get_feature_names_out())</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 3 features:"</span>, best_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Top 3 features: ['curb-weight', 'engine-size', 'horsepower']</code></pre>
</div>
</div>
<p>The results show that curb weight, engine size, and horsepower are the highest-scoring features. However, we will not select these yet for the final model, since other steps still must be discussed.</p>
</section>
<section id="train-test-split-with-stratification" class="level2">
<h2 class="anchored" data-anchor-id="train-test-split-with-stratification">Train-Test Split with Stratification</h2>
<p>Train-test split is the third important technique.</p>
<p>Before model training, the dataset has to be split into training and testing sets. We will use 80% of the data in the training set and 20% in the testing set. As the names suggest, the training set is used to train the model or help it <em>learn</em> how to predict car prices. Then, we make predictions on the cars on the testing set to see whether the predictions are accurate.</p>
<p>Before we split the data, though, we have to ensure that the frequency distribution of the target is similar between the training and testing sets. Below is a histogram of the frequency distribution of car price across the entire dataset:</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>sns.histplot(cars_df[<span class="st">"price"</span>], bins <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Frequency Distribution of Car Price"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Price (USD)"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Number of Cars"</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2021-12-21-Predicting-Car-Prices-K-Nearest-Neighbors_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The graph shows a right-skewed distribution, which means that most of the car prices are low and there are outliers with high prices. When we split the data into training and testing sets, we want each set to have a similar distribution to this.</p>
<p>De Cock (2011) provides a helpful suggestion on how to do this. The article says, “Simply order the original data set by a variable of interest (such as sale price) and select every kth observation to achieve the desired sample size (k=2 for a 50/50 split or k=4 for a 75/25 split).”</p>
<p>In our case, we want an 80/20 split. One-fifth of the data will go to the testing set, so we can use k = 5. We will thus order the observations by price, then assign every 5th observation to the testing set. All other observations will go to the training set.</p>
<p>In the code below, I have written a custom function <code>stratify_continuous</code> that uses this technique. I then performed a train-test split after stratification. <code>X_train</code> and <code>y_train</code> refer to the features and target in the training set, respectively. <code>X_test</code> and <code>y_test</code> are from the testing set.</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stratify_continuous(n_folds, y):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Stratify a dataset on a continuous target."""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_folds <span class="op">&lt;</span> <span class="dv">2</span> <span class="kw">or</span> n_folds <span class="op">&gt;</span> <span class="dv">10</span>:</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Please select a number of folds from 2 to 10."</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    fold_nums <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(n_folds))</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># DataFrame where "index" column contains the original indices</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        y</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shuffle before ranking so that cars with the same price are ordered randomly.</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        .sample(frac <span class="op">=</span> <span class="dv">1</span>, random_state <span class="op">=</span> <span class="dv">1</span>, ignore_index <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This column gives a rank to each value in y. 0 is the rank of the lowest value.</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ties are broken according to order of appearance.</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"rank"</span>] <span class="op">=</span> df[y.name].rank(method <span class="op">=</span> <span class="st">"first"</span>) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"fold"</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f <span class="kw">in</span> fold_nums[<span class="dv">1</span>:]:</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># start at f, then increment by n_folds</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(f, df.shape[<span class="dv">0</span>], n_folds))</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        df.loc[df[<span class="st">"rank"</span>].isin(indices), <span class="st">"fold"</span>] <span class="op">=</span> f</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Revert df to original order of indices</span></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.reindex(index <span class="op">=</span> y.index)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># A series that indicates the fold number of each observation according to its original position in y</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    fold_series <span class="op">=</span> df[<span class="st">"fold"</span>].copy()</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fold_series</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>folds <span class="op">=</span> stratify_continuous(</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>    n_folds <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> cars_df[<span class="st">"price"</span>],</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_folds(X, y, fold_series, test_fold):</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Take a dataset whose observations have been grouped into folds,</span></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="co">    then perform a train-test split."""</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> fold_series.dtype <span class="op">!=</span> <span class="st">"int64"</span>:</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">AttributeError</span>(<span class="st">"The fold list does not purely contain integers."</span>)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>    test_mask <span class="op">=</span> (fold_series <span class="op">==</span> test_fold)</span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> X.loc[<span class="op">~</span>test_mask].copy()</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> y.loc[<span class="op">~</span>test_mask].copy()</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> X.loc[test_mask].copy()</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> y.loc[test_mask].copy()</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X_train, X_test, y_train, y_test</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> split_folds(</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> cars_df[all_numeric_features],</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> cars_df[<span class="st">"price"</span>],</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a>    fold_series <span class="op">=</span> folds,</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>    test_fold <span class="op">=</span> <span class="dv">4</span>,</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary statistics for target columns.</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>target_df <span class="op">=</span> pd.concat(</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>    [y_train, y_test],</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>    axis <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>    join <span class="op">=</span> <span class="st">"outer"</span>,</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>target_df.columns <span class="op">=</span> [<span class="st">"y_train price"</span>, <span class="st">"y_test price"</span>]</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>target_df.describe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">y_train price</th>
<th data-quarto-table-cell-role="th">y_test price</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">count</td>
<td>155.000000</td>
<td>38.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">mean</td>
<td>13308.658065</td>
<td>13188.631579</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">std</td>
<td>8197.063090</td>
<td>7737.592975</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">min</td>
<td>5118.000000</td>
<td>5389.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">25%</td>
<td>7713.500000</td>
<td>7805.750000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">50%</td>
<td>10245.000000</td>
<td>10295.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">75%</td>
<td>16530.500000</td>
<td>16385.750000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">max</td>
<td>45400.000000</td>
<td>37028.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This table shows summary statistics for the price columns of the two sets. The sets have similar means at around USD 13,200, and they also have similar medians at around USD 10,200.</p>
<p>Let us compare the price distributions using KDE plots:</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(y_train, label <span class="op">=</span> <span class="st">"Training set"</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(y_test, label <span class="op">=</span> <span class="st">"Testing set"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Comparison of Car Prices Between Sets"</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Price (USD)"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Probability Density"</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2021-12-21-Predicting-Car-Prices-K-Nearest-Neighbors_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The KDE plots both seem to follow the same shape and have the same center. This shows that the training and testing sets have roughly the same distribution of car prices. Thus, these were stratified correctly.</p>
</section>
<section id="hyperparameter-optimization" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-optimization">Hyperparameter Optimization</h2>
<p>The fourth technique is hyperparameter optimization. This involves training the KNN model using different hyperparameter values to see which one performs the best.</p>
<p>A hyperparameter is a value that influences the behavior of a model and has no relation to the data. In the case of KNN, one important hyperparameter is the <span class="math inline">\(k\)</span> value, or the number of neighbors used to make a prediction. If <span class="math inline">\(k = 5\)</span>, we take the mean price of the top five most similar cars and call this our prediction. However, if <span class="math inline">\(k = 10\)</span>, we take the top ten cars, so the mean price may be different.</p>
<p>We can optimize <span class="math inline">\(k\)</span> in this way:</p>
<ul>
<li>Decide values of <span class="math inline">\(k\)</span> to test.</li>
<li>For each <span class="math inline">\(k\)</span> value, fit and evaluate a KNN model.</li>
<li>Identify the best-performing model and use its <span class="math inline">\(k\)</span> value in the final model.</li>
</ul>
<p>In order to evaluate a model, we need an evaluation metric. In our case, we will use the Root Mean Squared Error or RMSE. This is calculated with the following equation:</p>
<p><span class="math inline">\(RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^n (\text{actual}_i - \text{predicted}_i)^2}\)</span></p>
<ul>
<li><span class="math inline">\(n\)</span> is the sample size.</li>
<li><span class="math inline">\(\text{actual}\)</span> is the actual target value, or in this case, the actual price of a car.</li>
<li><span class="math inline">\(\text{predicted}\)</span> is the predicted target value.</li>
</ul>
<p>RMSE can be interpreted as the average error of a regression model. For example, if <span class="math inline">\(RMSE = 1000\)</span>, this means that the model’s predicted car prices are USD 1000 away from the actual car prices, on average.</p>
<p>Below is an example of hyperparameter optimization using RMSE. All of the numeric features were used for this example.</p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>k_values <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>k_rmse <span class="op">=</span> pd.Series(dtype <span class="op">=</span> <span class="st">"float64"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    knn <span class="op">=</span> KNeighborsRegressor(</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        n_neighbors <span class="op">=</span> k,</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        algorithm <span class="op">=</span> <span class="st">"auto"</span>,</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    knn.fit(X_train, y_train)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> knn.predict(X_test)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    rmse <span class="op">=</span> np.sqrt(mean_squared_error(y_test, y_pred))</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    k_rmse.loc[k] <span class="op">=</span> rmse</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"k value and RMSE"</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>k_rmse</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>k value and RMSE</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>1    4086.029491
3    3146.025226
5    3251.392477
dtype: float64</code></pre>
</div>
</div>
<p>The table above shows that RMSE was lowest for <span class="math inline">\(k = 3\)</span>. The RMSE was about USD 3146, which means that on average, the predicted prices are USD 3146 away from the actual prices.</p>
</section>
<section id="k-fold-cross-validation" class="level2">
<h2 class="anchored" data-anchor-id="k-fold-cross-validation">K-Fold Cross-Validation</h2>
<p>The last technique that will be discussed is K-Fold Cross-Validation. Earlier, we split the data into one training set and one testing set. The K-Fold Cross-Validation allows us to obtain a more holistic view of model performance by rotating the observations used in the two sets. In the words of Brownlee (2018), it estimates “how the model is expected to perform in general when used to make predictions on data not used during the training of the model.”</p>
<p>Here, <span class="math inline">\(k\)</span> has a different meaning. It determines the number of splits to make in a dataset. For example, if <span class="math inline">\(k = 5\)</span>, the dataset will be split into 5 folds, each set containing 20% of the total data.</p>
<p>In summary, the following steps are performed:</p>
<ul>
<li>Split the data into 5 folds: A, B, C, D, E.</li>
<li>Use fold A as the testing set and use the others as the training set.</li>
<li>Fit and evaluate a KNN model, thus obtaining RMSE.</li>
<li>Repeat the above process for a total of 5 times, so that each fold is used as a testing set once.</li>
<li>Compile a list of the five RMSE values obtained.</li>
<li>Compute the mean RMSE value. This is the final metric of model performance.</li>
</ul>
<p>K-Fold Cross-Validation can be implemented using scikit-learn’s <code>KFold</code> and <code>cross_val_score</code> . An example of 5-fold cross-validation is shown below.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsRegressor(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    n_neighbors <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    algorithm <span class="op">=</span> <span class="st">"auto"</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(<span class="dv">5</span>, shuffle <span class="op">=</span> <span class="va">True</span>, random_state <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>mses <span class="op">=</span> cross_val_score(</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    estimator <span class="op">=</span> knn,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> cars_df[all_numeric_features],</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> cars_df[<span class="st">"price"</span>],</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    scoring <span class="op">=</span> <span class="st">"neg_mean_squared_error"</span>,</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> kf,</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>mses <span class="op">=</span> pd.Series(mses)</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>rmses <span class="op">=</span> mses.<span class="bu">abs</span>().<span class="bu">pow</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>mean_rmse <span class="op">=</span> rmses.mean()</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>sd_rmse <span class="op">=</span> rmses.std(ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"""Regular 5-fold cross-validation</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a><span class="ss">Mean RMSE: </span><span class="sc">{</span>mean_rmse<span class="sc">:.2f}</span></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a><span class="ss">Standard Deviation RMSE: </span><span class="sc">{</span>sd_rmse<span class="sc">:.2f}</span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="ss">RMSE Values: </span><span class="sc">{</span>rmses<span class="sc">.</span>to_list()<span class="sc">}</span><span class="ss">"""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Regular 5-fold cross-validation
Mean RMSE: 3722.28
Standard Deviation RMSE: 565.62
RMSE Values: [3407.8275635020186, 3902.1144860913682, 3009.7340988268425, 4521.314079941105, 3770.3892479494248]</code></pre>
</div>
</div>
<p>The mean RMSE above presents a better picture of the model’s performance because it takes into account different possible combinations of training and testing sets.</p>
<p>Note, however, that the standard deviation of the RMSE was around 566. This means that the RMSE values varied by several hundreds of dollars from model to model during the cross-validation. In simpler terms, the model performance was inconsistent. It performed much better when trained on some folds than when it was trained on other folds.</p>
<p>Thus, we can take k-fold cross-validation a step further by stratifying the folds so that they will have similar price distributions. This will ensure that each fold is representative of the full sample. Thus, I have written a custom function in the code cell below to do this.</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stratified_kfcv(X, y, fold_series, regression_model):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Conduct k-fold cross-validation on a stratified dataset."""</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    fold_nums <span class="op">=</span> fold_series.unique()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    mse_lst <span class="op">=</span> []</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f <span class="kw">in</span> fold_nums:</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        X_train, X_test, y_train, y_test <span class="op">=</span> split_folds(</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> X,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>            test_fold <span class="op">=</span> f,</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>            fold_series <span class="op">=</span> fold_series,</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        regression_model.fit(X_train, y_train)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        y_pred <span class="op">=</span> regression_model.predict(X_test)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        mse <span class="op">=</span> mean_squared_error(y_test, y_pred)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        mse_lst.append(mse)</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mse_lst</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsRegressor(</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>    n_neighbors <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>    algorithm <span class="op">=</span> <span class="st">"auto"</span>,</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>mse_lst <span class="op">=</span> stratified_kfcv(</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> cars_df[all_numeric_features],</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> cars_df[<span class="st">"price"</span>],</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    fold_series <span class="op">=</span> folds,</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    regression_model <span class="op">=</span> knn,</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>mse_series <span class="op">=</span> pd.Series(mse_lst)</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>rmse_series <span class="op">=</span> mse_series.<span class="bu">pow</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>mean_rmse <span class="op">=</span> rmse_series.mean()</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>sd_rmse <span class="op">=</span> rmse_series.std(ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"""Stratified 5-fold cross-validation</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a><span class="ss">Mean RMSE: </span><span class="sc">{</span>mean_rmse<span class="sc">:.2f}</span></span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a><span class="ss">Standard Deviation RMSE: </span><span class="sc">{</span>sd_rmse<span class="sc">:.2f}</span></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a><span class="ss">RMSE Values: </span><span class="sc">{</span>rmse_series<span class="sc">.</span>to_list()<span class="sc">}</span><span class="ss">"""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Stratified 5-fold cross-validation
Mean RMSE: 3369.44
Standard Deviation RMSE: 387.33
RMSE Values: [3193.0727214096655, 2883.515369146238, 3844.6421242541865, 3674.5947449327227, 3251.39247707809]</code></pre>
</div>
</div>
<p>The mean RMSE from stratified CV was USD 3369. This is about USD 400 lower than the result of the regular CV, USD 3722.</p>
<p>Furthermore, the SD RMSE is equal to 387, which is lower than the previous value of 566. Therefore, the five models trained during cross-validation performed more similarly to each other.</p>
<p>Thus, we can see that stratifying observations before k-fold cross-validation can be more effective at approximating the true performance of the model compared to regular k-fold cross-validation.</p>
</section>
</section>
<section id="combining-techniques" class="level1">
<h1>Combining Techniques</h1>
<p>In this part, we will combine all of the discussed techniques to optimize the KNN model.</p>
<p>The steps are as follows:</p>
<ul>
<li>Use the standardized features that were calculated earlier.</li>
<li>For each number <code>n_features</code> from 1 to 10:
<ul>
<li>Perform univariate feature selection using the F-statistic.</li>
<li>Identify the best <code>n_features</code> features.</li>
<li>For each number <code>k</code> from 1 to 20:
<ul>
<li>Evaluate the model using stratified 5-fold cross-validation.</li>
<li>For each fold, train a <code>k</code> nearest neighbors model using the best features.</li>
<li>Obtain the mean RMSE value.</li>
</ul></li>
</ul></li>
<li>Compile a list of all mean RMSE values obtained.</li>
<li>Identify the model with the lowest mean RMSE. This is the final model.</li>
</ul>
<p>This is implemented in the code below.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>n_feature_list <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">11</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>result_lst <span class="op">=</span> []</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_features <span class="kw">in</span> n_feature_list:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Univariate feature selection</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    skb <span class="op">=</span> SelectKBest(</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        score_func <span class="op">=</span> f_regression,</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> n_features,</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> cars_df[all_numeric_features]</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> cars_df[<span class="st">"price"</span>]</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    X_new <span class="op">=</span> skb.fit_transform(X, y)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># List of "best" features</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    best_features <span class="op">=</span> <span class="bu">list</span>(skb.get_feature_names_out())</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    k_values <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">21</span>))</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> k_values:</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># stratified 5-fold cross validation</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        knn <span class="op">=</span> KNeighborsRegressor(</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use a different k value each time</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>            n_neighbors <span class="op">=</span> k,</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>            algorithm <span class="op">=</span> <span class="st">"auto"</span>,</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        mse_lst <span class="op">=</span> stratified_kfcv(</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>            X <span class="op">=</span> cars_df[best_features],</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> cars_df[<span class="st">"price"</span>],</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>            fold_series <span class="op">=</span> folds,</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>            regression_model <span class="op">=</span> knn,</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        mse_series <span class="op">=</span> pd.Series(mse_lst)</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>        rmse_series <span class="op">=</span> mse_series.<span class="bu">pow</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>        mean_rmse <span class="op">=</span> rmse_series.mean()</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>        sd_rmse <span class="op">=</span> rmse_series.std(ddof <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>        new_row <span class="op">=</span> (n_features, best_features, k, mean_rmse, sd_rmse)</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>        result_lst.append(new_row)</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>result_df <span class="op">=</span> pd.DataFrame(result_lst)</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>result_df.columns <span class="op">=</span> [<span class="st">"Number of Features"</span>, <span class="st">"Best Features"</span>, <span class="st">"k Neighbors"</span>, <span class="st">"Mean RMSE"</span>, <span class="st">"SD RMSE"</span>]</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>result_df <span class="op">=</span> (</span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>    result_df</span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>    .sort_values([<span class="st">"Mean RMSE"</span>, <span class="st">"SD RMSE"</span>], ascending <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>    .reset_index(drop <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Before we discuss the top-performing models, let us look at the general trends in the results using some graphs.</p>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>sns.lineplot(</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> result_df,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="st">"k Neighbors"</span>,</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="st">"Mean RMSE"</span>,</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    hue <span class="op">=</span> <span class="st">"Number of Features"</span>,</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mean RMSE against k Neighbors"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2021-12-21-Predicting-Car-Prices-K-Nearest-Neighbors_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The graph above shows that in general, no matter the number of features, the mean RMSE increased as the number of neighbors (k) increased. Therefore, it is best to have a low k value so that the model makes predictions only using a few cars that are most similar to the car being tested.</p>
<p>Next, let us look at a graph with the same variables, except that the number of features is now on the x-axis instead of k.</p>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>sns.lineplot(</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> result_df,</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="st">"Number of Features"</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="st">"Mean RMSE"</span>,</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    hue <span class="op">=</span> <span class="st">"k Neighbors"</span>,</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Mean RMSE against Number of Features"</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2021-12-21-Predicting-Car-Prices-K-Nearest-Neighbors_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can see that for models with a high k value (represented by the darker lines), the mean RMSE increased slightly as the number of features increased.</p>
<p>However, for models with a low k value (represented by the lighter pink lines), the mean RMSE stayed the same or even decreased when the number of features increased.</p>
<p>Therefore, the best model would be one with a low k value and a medium-to-high number of features.</p>
<p>In order to determine this more precisely, let us look at the top 10 models with the lowest RMSE.</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>result_df.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Number of Features</th>
<th data-quarto-table-cell-role="th">Best Features</th>
<th data-quarto-table-cell-role="th">k Neighbors</th>
<th data-quarto-table-cell-role="th">Mean RMSE</th>
<th data-quarto-table-cell-role="th">SD RMSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>8</td>
<td>[length, width, curb-weight, num-of-cylinders,...</td>
<td>1</td>
<td>2468.363493</td>
<td>354.699226</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>4</td>
<td>[width, curb-weight, engine-size, horsepower]</td>
<td>1</td>
<td>2663.935533</td>
<td>809.240758</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>4</td>
<td>[width, curb-weight, engine-size, horsepower]</td>
<td>2</td>
<td>2740.846793</td>
<td>541.902963</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>8</td>
<td>[length, width, curb-weight, num-of-cylinders,...</td>
<td>2</td>
<td>2751.669830</td>
<td>693.467197</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>8</td>
<td>[length, width, curb-weight, num-of-cylinders,...</td>
<td>3</td>
<td>2755.824129</td>
<td>383.365351</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>3</td>
<td>[curb-weight, engine-size, horsepower]</td>
<td>2</td>
<td>2767.545024</td>
<td>543.282005</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>5</td>
<td>[width, curb-weight, num-of-cylinders, engine-...</td>
<td>3</td>
<td>2784.838645</td>
<td>450.626647</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>4</td>
<td>[width, curb-weight, engine-size, horsepower]</td>
<td>3</td>
<td>2798.859095</td>
<td>477.298250</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>5</td>
<td>[width, curb-weight, num-of-cylinders, engine-...</td>
<td>2</td>
<td>2804.311055</td>
<td>473.482127</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>7</td>
<td>[width, curb-weight, num-of-cylinders, engine-...</td>
<td>1</td>
<td>2806.166008</td>
<td>696.001235</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The table above shows the ten models with the lowest mean RMSE. Interestingly, the best-performing model had 8 features and a k-value of 1.</p>
<p>Its RMSE was 2468, so on average, the predicted prices were USD 2468 off from the actual prices. This is decent considering that the car prices mostly fall between USD 5000 and USD 20000, though it could be better.</p>
<p>The SD RMSE is around 355. This means that the RMSE values usually varied by 355 from the mean. This is relatively low compared to the SD RMSE values of the other best-performing models, which range from 350 to over 800. Therefore, the model performance was consistent.</p>
<p>However, the following concerns are worth noting:</p>
<ul>
<li>The number of features is somewhat large, and this may be a problem because it can cause overfitting. This means that the model may be too sensitive to small but meaningless variations in the training data. It may be unable to recognize <em>general trends</em> properly. However, reducing the number of features may increase the mean RMSE.</li>
<li>The k-value is concerning because only one neighbor is considered when predicting a car’s price. I would prefer to have <span class="math inline">\(k &gt; 1\)</span> so that multiple neighbors are taken into consideration.</li>
</ul>
<p>Personally, I am fine with selecting this model as the final one to use, simply because its mean RMSE is a few hundred dollars lower than that of the other good models. In a real-world scenario, after I implement this model, I would see if it continued to perform well on new data, and then reduce its number of features or increase its k-value if needed.</p>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this project, we cleaned a dataset about car features and prices, discussed the logic behind the K Nearest Neighbors algorithm for regression, explained techniques used in the machine learning workflow, then applied these techniques to determine the optimal model for predicting car prices.</p>
<p>Thanks for reading!</p>
</section>
<section id="bibliography" class="level1">
<h1>Bibliography</h1>
<section id="data-source" class="level2">
<h2 class="anchored" data-anchor-id="data-source">Data Source</h2>
<p>Schlimmer, J. C. (1987, May 19). UCI Machine Learning Repository: Automobile Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/automobile</p>
</section>
<section id="information-sources" class="level2">
<h2 class="anchored" data-anchor-id="information-sources">Information Sources</h2>
<p>Brownlee, J. (2018, May 22). A Gentle Introduction to k-fold Cross-Validation. Machine Learning Mastery. https://machinelearningmastery.com/k-fold-cross-validation/</p>
<p>Dataquest. (n.d.). Predicting Car Prices: Machine Learning Project. Dataquest. Retrieved December 21, 2021, from https://www.dataquest.io/c/36/m/155/guided-project%3A-predicting-car-prices</p>
<p>De Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education, 19(3), null. https://doi.org/10.1080/10691898.2011.11889627</p>
<p>Frost, J. (2017, April 4). How to Interpret the F-test of Overall Significance in Regression Analysis. Statistics By Jim. http://statisticsbyjim.com/regression/interpret-f-test-overall-significance-regression/</p>
<p>Miller, M. (2019, October 18). The Basics: KNN for classification and regression. Medium. https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955</p>
<p>scikit-learn developers. (2021). 1.13.2. Univariate Feature Selection. Scikit-Learn. https://scikit-learn/stable/modules/feature_selection.html</p>
</section>
<section id="image-source" class="level2">
<h2 class="anchored" data-anchor-id="image-source">Image Source</h2>
<p>Smith, C. (2018, January 20). Charger vs Challenger: All-American Muscle Car Comparison. WheelScene. https://wheelscene.com/charger-vs-challenger/</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="MiguelAHG/migs-germar-data-science-blog" data-repo-id="R_kgDOJySw9w" data-category="Announcements" data-category-id="DIC_kwDOJySw984CXXr6" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
</div> <!-- /content -->



</body></html>