[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Miguel Antonio H. Germar, but you can call me Migs. I am a second year college student from Quezon City, Philippines. Currently, I am in my second year in the BS Applied Mathematics - Master in Data Science program in Ateneo de Manila University. I’m interested in pursuing a career in data science, so I set up this website as a blog and portfolio for my projects.\nResume: PDF\nEmail: migs.germar@gmail.com."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#tafe-survey",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#tafe-survey",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "TAFE Survey",
    "text": "TAFE Survey\nThe TAFE employee exit survey data can be found here.\nUnfortunately, the CSV file itself is no longer available on the Australian government’s websites, so I used a copy that I downloaded from Dataquest.\n\nColumns\nBelow is information about the TAFE columns.\n\ntafe = pd.read_csv(\"./private/2021-06-01-EES-Files/tafe_survey.csv\")\n\ntafe.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 702 entries, 0 to 701\nData columns (total 72 columns):\n #   Column                                                                                                                                                         Non-Null Count  Dtype  \n---  ------                                                                                                                                                         --------------  -----  \n 0   Record ID                                                                                                                                                      702 non-null    float64\n 1   Institute                                                                                                                                                      702 non-null    object \n 2   WorkArea                                                                                                                                                       702 non-null    object \n 3   CESSATION YEAR                                                                                                                                                 695 non-null    float64\n 4   Reason for ceasing employment                                                                                                                                  701 non-null    object \n 5   Contributing Factors. Career Move - Public Sector                                                                                                              437 non-null    object \n 6   Contributing Factors. Career Move - Private Sector                                                                                                             437 non-null    object \n 7   Contributing Factors. Career Move - Self-employment                                                                                                            437 non-null    object \n 8   Contributing Factors. Ill Health                                                                                                                               437 non-null    object \n 9   Contributing Factors. Maternity/Family                                                                                                                         437 non-null    object \n 10  Contributing Factors. Dissatisfaction                                                                                                                          437 non-null    object \n 11  Contributing Factors. Job Dissatisfaction                                                                                                                      437 non-null    object \n 12  Contributing Factors. Interpersonal Conflict                                                                                                                   437 non-null    object \n 13  Contributing Factors. Study                                                                                                                                    437 non-null    object \n 14  Contributing Factors. Travel                                                                                                                                   437 non-null    object \n 15  Contributing Factors. Other                                                                                                                                    437 non-null    object \n 16  Contributing Factors. NONE                                                                                                                                     437 non-null    object \n 17  Main Factor. Which of these was the main factor for leaving?                                                                                                   113 non-null    object \n 18  InstituteViews. Topic:1. I feel the senior leadership had a clear vision and direction                                                                         608 non-null    object \n 19  InstituteViews. Topic:2. I was given access to skills training to help me do my job better                                                                     613 non-null    object \n 20  InstituteViews. Topic:3. I was given adequate opportunities for personal development                                                                           610 non-null    object \n 21  InstituteViews. Topic:4. I was given adequate opportunities for promotion within %Institute]Q25LBL%                                                            608 non-null    object \n 22  InstituteViews. Topic:5. I felt the salary for the job was right for the responsibilities I had                                                                615 non-null    object \n 23  InstituteViews. Topic:6. The organisation recognised when staff did good work                                                                                  607 non-null    object \n 24  InstituteViews. Topic:7. Management was generally supportive of me                                                                                             614 non-null    object \n 25  InstituteViews. Topic:8. Management was generally supportive of my team                                                                                        608 non-null    object \n 26  InstituteViews. Topic:9. I was kept informed of the changes in the organisation which would affect me                                                          610 non-null    object \n 27  InstituteViews. Topic:10. Staff morale was positive within the Institute                                                                                       602 non-null    object \n 28  InstituteViews. Topic:11. If I had a workplace issue it was dealt with quickly                                                                                 601 non-null    object \n 29  InstituteViews. Topic:12. If I had a workplace issue it was dealt with efficiently                                                                             597 non-null    object \n 30  InstituteViews. Topic:13. If I had a workplace issue it was dealt with discreetly                                                                              601 non-null    object \n 31  WorkUnitViews. Topic:14. I was satisfied with the quality of the management and supervision within my work unit                                                609 non-null    object \n 32  WorkUnitViews. Topic:15. I worked well with my colleagues                                                                                                      605 non-null    object \n 33  WorkUnitViews. Topic:16. My job was challenging and interesting                                                                                                607 non-null    object \n 34  WorkUnitViews. Topic:17. I was encouraged to use my initiative in the course of my work                                                                        610 non-null    object \n 35  WorkUnitViews. Topic:18. I had sufficient contact with other people in my job                                                                                  613 non-null    object \n 36  WorkUnitViews. Topic:19. I was given adequate support and co-operation by my peers to enable me to do my job                                                   609 non-null    object \n 37  WorkUnitViews. Topic:20. I was able to use the full range of my skills in my job                                                                               609 non-null    object \n 38  WorkUnitViews. Topic:21. I was able to use the full range of my abilities in my job. ; Category:Level of Agreement; Question:YOUR VIEWS ABOUT YOUR WORK UNIT]  608 non-null    object \n 39  WorkUnitViews. Topic:22. I was able to use the full range of my knowledge in my job                                                                            608 non-null    object \n 40  WorkUnitViews. Topic:23. My job provided sufficient variety                                                                                                    611 non-null    object \n 41  WorkUnitViews. Topic:24. I was able to cope with the level of stress and pressure in my job                                                                    610 non-null    object \n 42  WorkUnitViews. Topic:25. My job allowed me to balance the demands of work and family to my satisfaction                                                        611 non-null    object \n 43  WorkUnitViews. Topic:26. My supervisor gave me adequate personal recognition and feedback on my performance                                                    606 non-null    object \n 44  WorkUnitViews. Topic:27. My working environment was satisfactory e.g. sufficient space, good lighting, suitable seating and working area                       610 non-null    object \n 45  WorkUnitViews. Topic:28. I was given the opportunity to mentor and coach others in order for me to pass on my skills and knowledge prior to my cessation date  609 non-null    object \n 46  WorkUnitViews. Topic:29. There was adequate communication between staff in my unit                                                                             603 non-null    object \n 47  WorkUnitViews. Topic:30. Staff morale was positive within my work unit                                                                                         606 non-null    object \n 48  Induction. Did you undertake Workplace Induction?                                                                                                              619 non-null    object \n 49  InductionInfo. Topic:Did you undertake a Corporate Induction?                                                                                                  432 non-null    object \n 50  InductionInfo. Topic:Did you undertake a Institute Induction?                                                                                                  483 non-null    object \n 51  InductionInfo. Topic: Did you undertake Team Induction?                                                                                                        440 non-null    object \n 52  InductionInfo. Face to Face Topic:Did you undertake a Corporate Induction; Category:How it was conducted?                                                      555 non-null    object \n 53  InductionInfo. On-line Topic:Did you undertake a Corporate Induction; Category:How it was conducted?                                                           555 non-null    object \n 54  InductionInfo. Induction Manual Topic:Did you undertake a Corporate Induction?                                                                                 555 non-null    object \n 55  InductionInfo. Face to Face Topic:Did you undertake a Institute Induction?                                                                                     530 non-null    object \n 56  InductionInfo. On-line Topic:Did you undertake a Institute Induction?                                                                                          555 non-null    object \n 57  InductionInfo. Induction Manual Topic:Did you undertake a Institute Induction?                                                                                 553 non-null    object \n 58  InductionInfo. Face to Face Topic: Did you undertake Team Induction; Category?                                                                                 555 non-null    object \n 59  InductionInfo. On-line Topic: Did you undertake Team Induction?process you undertook and how it was conducted.]                                                555 non-null    object \n 60  InductionInfo. Induction Manual Topic: Did you undertake Team Induction?                                                                                       555 non-null    object \n 61  Workplace. Topic:Did you and your Manager develop a Performance and Professional Development Plan (PPDP)?                                                      608 non-null    object \n 62  Workplace. Topic:Does your workplace promote a work culture free from all forms of unlawful discrimination?                                                    594 non-null    object \n 63  Workplace. Topic:Does your workplace promote and practice the principles of employment equity?                                                                 587 non-null    object \n 64  Workplace. Topic:Does your workplace value the diversity of its employees?                                                                                     586 non-null    object \n 65  Workplace. Topic:Would you recommend the Institute as an employer to others?                                                                                   581 non-null    object \n 66  Gender. What is your Gender?                                                                                                                                   596 non-null    object \n 67  CurrentAge. Current Age                                                                                                                                        596 non-null    object \n 68  Employment Type. Employment Type                                                                                                                               596 non-null    object \n 69  Classification. Classification                                                                                                                                 596 non-null    object \n 70  LengthofServiceOverall. Overall Length of Service at Institute (in years)                                                                                      596 non-null    object \n 71  LengthofServiceCurrent. Length of Service at current workplace (in years)                                                                                      596 non-null    object \ndtypes: float64(2), object(70)\nmemory usage: 395.0+ KB\n\n\nThe formatting is different because some column names are apparently too long. However, we can see that:\n\nThere are 702 rows and 72 columns.\nA few columns contain decimals, and most contain text.\nMany of the columns have missing values.\n\nDataquest notes a few columns in this dataset:\n\n“Record ID: An id used to identify the participant of the survey”\n“Reason for ceasing employment: The reason why the person’s employment ended”\n“LengthofServiceOverall. Overall Length of Service at Institute (in years): The length of the person’s employment (in years)”\n\nAdditionally, there are groups of columns that all start with the same name. These group names are:\n\nContributing Factors\nInstitute Views\nWork Unit Views\nInduction Info\nWorkplace\n\nCurrently, there are too many columns for analysis. However, since the Contributing Factors columns are directly related to the employee’s resignation, we could just keep those and remove the other 4 groups of columns.\n\n\nDescriptive Statistics\nBelow are descriptive statistics for the columns.\n\ntafe.describe(\n    include = \"all\",\n    datetime_is_numeric = True,\n)\n\n\n\n\n\n\n\n\nRecord ID\nInstitute\nWorkArea\nCESSATION YEAR\nReason for ceasing employment\nContributing Factors. Career Move - Public Sector\nContributing Factors. Career Move - Private Sector\nContributing Factors. Career Move - Self-employment\nContributing Factors. Ill Health\nContributing Factors. Maternity/Family\n...\nWorkplace. Topic:Does your workplace promote a work culture free from all forms of unlawful discrimination?\nWorkplace. Topic:Does your workplace promote and practice the principles of employment equity?\nWorkplace. Topic:Does your workplace value the diversity of its employees?\nWorkplace. Topic:Would you recommend the Institute as an employer to others?\nGender. What is your Gender?\nCurrentAge. Current Age\nEmployment Type. Employment Type\nClassification. Classification\nLengthofServiceOverall. Overall Length of Service at Institute (in years)\nLengthofServiceCurrent. Length of Service at current workplace (in years)\n\n\n\n\ncount\n7.020000e+02\n702\n702\n695.000000\n701\n437\n437\n437\n437\n437\n...\n594\n587\n586\n581\n596\n596\n596\n596\n596\n596\n\n\nunique\nNaN\n12\n2\nNaN\n6\n2\n2\n2\n2\n2\n...\n2\n2\n2\n2\n2\n9\n5\n9\n7\n7\n\n\ntop\nNaN\nBrisbane North Institute of TAFE\nNon-Delivery (corporate)\nNaN\nResignation\n-\n-\n-\n-\n-\n...\nYes\nYes\nYes\nYes\nFemale\n56 or older\nPermanent Full-time\nAdministration (AO)\nLess than 1 year\nLess than 1 year\n\n\nfreq\nNaN\n161\n432\nNaN\n340\n375\n336\n420\n403\n411\n...\n536\n512\n488\n416\n389\n162\n237\n293\n147\n177\n\n\nmean\n6.346026e+17\nNaN\nNaN\n2011.423022\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nstd\n2.515071e+14\nNaN\nNaN\n0.905977\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmin\n6.341330e+17\nNaN\nNaN\n2009.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n25%\n6.343954e+17\nNaN\nNaN\n2011.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n50%\n6.345835e+17\nNaN\nNaN\n2011.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n75%\n6.348005e+17\nNaN\nNaN\n2012.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmax\n6.350730e+17\nNaN\nNaN\n2013.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n11 rows × 72 columns\n\n\n\nSome text columns appear to have values that only contain a single hyphen (-); these will have to be investigated later."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dete-survey",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dete-survey",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "DETE Survey",
    "text": "DETE Survey\nThe DETE employee exit survey can be found here. The copy used in this project is a slightly modified version downloaded from Dataquest for convenience. It is still complete in terms of the number of entries.\n\nColumns\nBelow are the column names and types.\n\ndete = pd.read_csv(\"./private/2021-06-01-EES-Files/dete_survey.csv\")\n\ndete.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 822 entries, 0 to 821\nData columns (total 56 columns):\n #   Column                               Non-Null Count  Dtype \n---  ------                               --------------  ----- \n 0   ID                                   822 non-null    int64 \n 1   SeparationType                       822 non-null    object\n 2   Cease Date                           822 non-null    object\n 3   DETE Start Date                      822 non-null    object\n 4   Role Start Date                      822 non-null    object\n 5   Position                             817 non-null    object\n 6   Classification                       455 non-null    object\n 7   Region                               822 non-null    object\n 8   Business Unit                        126 non-null    object\n 9   Employment Status                    817 non-null    object\n 10  Career move to public sector         822 non-null    bool  \n 11  Career move to private sector        822 non-null    bool  \n 12  Interpersonal conflicts              822 non-null    bool  \n 13  Job dissatisfaction                  822 non-null    bool  \n 14  Dissatisfaction with the department  822 non-null    bool  \n 15  Physical work environment            822 non-null    bool  \n 16  Lack of recognition                  822 non-null    bool  \n 17  Lack of job security                 822 non-null    bool  \n 18  Work location                        822 non-null    bool  \n 19  Employment conditions                822 non-null    bool  \n 20  Maternity/family                     822 non-null    bool  \n 21  Relocation                           822 non-null    bool  \n 22  Study/Travel                         822 non-null    bool  \n 23  Ill Health                           822 non-null    bool  \n 24  Traumatic incident                   822 non-null    bool  \n 25  Work life balance                    822 non-null    bool  \n 26  Workload                             822 non-null    bool  \n 27  None of the above                    822 non-null    bool  \n 28  Professional Development             808 non-null    object\n 29  Opportunities for promotion          735 non-null    object\n 30  Staff morale                         816 non-null    object\n 31  Workplace issue                      788 non-null    object\n 32  Physical environment                 817 non-null    object\n 33  Worklife balance                     815 non-null    object\n 34  Stress and pressure support          810 non-null    object\n 35  Performance of supervisor            813 non-null    object\n 36  Peer support                         812 non-null    object\n 37  Initiative                           813 non-null    object\n 38  Skills                               811 non-null    object\n 39  Coach                                767 non-null    object\n 40  Career Aspirations                   746 non-null    object\n 41  Feedback                             792 non-null    object\n 42  Further PD                           768 non-null    object\n 43  Communication                        814 non-null    object\n 44  My say                               812 non-null    object\n 45  Information                          816 non-null    object\n 46  Kept informed                        813 non-null    object\n 47  Wellness programs                    766 non-null    object\n 48  Health & Safety                      793 non-null    object\n 49  Gender                               798 non-null    object\n 50  Age                                  811 non-null    object\n 51  Aboriginal                           16 non-null     object\n 52  Torres Strait                        3 non-null      object\n 53  South Sea                            7 non-null      object\n 54  Disability                           23 non-null     object\n 55  NESB                                 32 non-null     object\ndtypes: bool(18), int64(1), object(37)\nmemory usage: 258.6+ KB\n\n\nID contains integers, whereas all other columns either contain text or booleans. Also, many of the columns have missing values.\nThere are 822 rows (employees) and 56 columns (survey questions). Dataquest describes a few notable columns as follows:\n\n“ID: An id used to identify the participant of the survey”\n“SeparationType: The reason why the person’s employment ended”\n“Cease Date: The year or month the person’s employment ended”\n“DETE Start Date: The year the person began employment with the DETE”\n\nAlso, if we look closely, we can see that many of the columns in the DETE dataset match columns in the TAFE dataset. For example:\n\nprint(\"TAFE Columns\")\nprint(list(tafe.columns[5:17]))\n\nprint(\"\\nDETE Columns\")\nprint(list(dete.columns[10:28]))\n\nTAFE Columns\n['Contributing Factors. Career Move - Public Sector ', 'Contributing Factors. Career Move - Private Sector ', 'Contributing Factors. Career Move - Self-employment', 'Contributing Factors. Ill Health', 'Contributing Factors. Maternity/Family', 'Contributing Factors. Dissatisfaction', 'Contributing Factors. Job Dissatisfaction', 'Contributing Factors. Interpersonal Conflict', 'Contributing Factors. Study', 'Contributing Factors. Travel', 'Contributing Factors. Other', 'Contributing Factors. NONE']\n\nDETE Columns\n['Career move to public sector', 'Career move to private sector', 'Interpersonal conflicts', 'Job dissatisfaction', 'Dissatisfaction with the department', 'Physical work environment', 'Lack of recognition', 'Lack of job security', 'Work location', 'Employment conditions', 'Maternity/family', 'Relocation', 'Study/Travel', 'Ill Health', 'Traumatic incident', 'Work life balance', 'Workload', 'None of the above']\n\n\nThese two sets of columns represent the “Contributing Factors” group in each survey. These are some of the columns that we want to keep for analysis; this will be addressed in data cleaning later.\n\n\nDescriptive Statistics\nWe can view descriptive statistics for all columns below.\n\ndete.describe(\n    include = \"all\",\n    datetime_is_numeric = True,\n)\n\n\n\n\n\n\n\n\nID\nSeparationType\nCease Date\nDETE Start Date\nRole Start Date\nPosition\nClassification\nRegion\nBusiness Unit\nEmployment Status\n...\nKept informed\nWellness programs\nHealth & Safety\nGender\nAge\nAboriginal\nTorres Strait\nSouth Sea\nDisability\nNESB\n\n\n\n\ncount\n822.000000\n822\n822\n822\n822\n817\n455\n822\n126\n817\n...\n813\n766\n793\n798\n811\n16\n3\n7\n23\n32\n\n\nunique\nNaN\n9\n25\n51\n46\n15\n8\n9\n14\n5\n...\n6\n6\n6\n2\n10\n1\n1\n1\n1\n1\n\n\ntop\nNaN\nAge Retirement\n2012\nNot Stated\nNot Stated\nTeacher\nPrimary\nMetropolitan\nEducation Queensland\nPermanent Full-time\n...\nA\nA\nA\nFemale\n61 or older\nYes\nYes\nYes\nYes\nYes\n\n\nfreq\nNaN\n285\n344\n73\n98\n324\n161\n135\n54\n434\n...\n401\n253\n386\n573\n222\n16\n3\n7\n23\n32\n\n\nmean\n411.693431\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nstd\n237.705820\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmin\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n25%\n206.250000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n50%\n411.500000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n75%\n616.750000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmax\n823.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n11 rows × 56 columns\n\n\n\nThe Cease Date, DETE Start Date, and Role Start Date columns are interesting because these are in text format and some of the values are \"Not Stated\".\nAlso, based on counts of non-null values shown earlier, some columns have many missing values. These will have to be addressed in data cleaning."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#placeholders-for-missing-values",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#placeholders-for-missing-values",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Placeholders for Missing Values",
    "text": "Placeholders for Missing Values\nEarlier, we noticed that some columns in the DETE data contain “Not Stated” values. These are likely to be placeholders for missing data.\nTherefore, we can replace all “Not Stated” values with np.nan null values.\n\ndete = dete.replace(\"Not Stated\", np.nan)"
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dropping-columns",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dropping-columns",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Dropping Columns",
    "text": "Dropping Columns\nIn the TAFE dataset, there are 4 other big groups of columns other than Contributing Factors:\n\nInstitute Views\nWork Unit Views\nInduction Info\nWorkplace\n\nWe want to remove these columns in order to limit the columns in our dataset to the most relevant ones. This is done below.\n\ntafe = tafe.drop(\n    labels = tafe.columns[17:66],\n    axis = 1,\n)\n\nlist(tafe.columns)\n\n['Record ID',\n 'Institute',\n 'WorkArea',\n 'CESSATION YEAR',\n 'Reason for ceasing employment',\n 'Contributing Factors. Career Move - Public Sector ',\n 'Contributing Factors. Career Move - Private Sector ',\n 'Contributing Factors. Career Move - Self-employment',\n 'Contributing Factors. Ill Health',\n 'Contributing Factors. Maternity/Family',\n 'Contributing Factors. Dissatisfaction',\n 'Contributing Factors. Job Dissatisfaction',\n 'Contributing Factors. Interpersonal Conflict',\n 'Contributing Factors. Study',\n 'Contributing Factors. Travel',\n 'Contributing Factors. Other',\n 'Contributing Factors. NONE',\n 'Gender. What is your Gender?',\n 'CurrentAge. Current Age',\n 'Employment Type. Employment Type',\n 'Classification. Classification',\n 'LengthofServiceOverall. Overall Length of Service at Institute (in years)',\n 'LengthofServiceCurrent. Length of Service at current workplace (in years)']\n\n\nThere are now only 23 columns in the TAFE dataset.\nEarlier, in the Data Overview, we mentioned that DETE has similar columns to the ones in TAFE. For example, look at the columns from indices 28 to 48:\n\ndete.columns[28:49]\n\nIndex(['Professional Development', 'Opportunities for promotion',\n       'Staff morale', 'Workplace issue', 'Physical environment',\n       'Worklife balance', 'Stress and pressure support',\n       'Performance of supervisor', 'Peer support', 'Initiative', 'Skills',\n       'Coach', 'Career Aspirations', 'Feedback', 'Further PD',\n       'Communication', 'My say', 'Information', 'Kept informed',\n       'Wellness programs', 'Health & Safety'],\n      dtype='object')\n\n\nThese are equivalent to the TAFE columns under the Institute Views, Work Unit Views, Induction Info, and Workspace groups. We don’t need these groups since the Contributing Factors group is directly related to the reason why the employees resigned.\nThus, we will remove the columns shown above from the DETE dataset.\n\ndete = dete.drop(\n    dete.columns[28:49],\n    axis = 1,\n)\n\nlen(dete.columns)\n\n35"
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#matching-columns-in-tafe-and-dete",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#matching-columns-in-tafe-and-dete",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Matching Columns in TAFE and DETE",
    "text": "Matching Columns in TAFE and DETE\nBelow are some important columns in TAFE and DETE which have matching information.\n\n\n\n\n\n\n\nDETE Survey\nTAFE Survey\n\n\n\n\nID\nRecord ID\n\n\nSeparationType\nReason for ceasing employment\n\n\nCease Date\nCESSATION YEAR\n\n\nDETE Start Date\nLengthofServiceOverall. Overall Length of Service at Institute (in years)\n\n\nAge\nCurrentAge. Current Age\n\n\nGender\nGender. What is your Gender?\n\n\n\nNotably, DETE Start Date and LengthofServiceOverall are matching columns because one can tell how long the employee has been working based on the date when they first started working.\nWe want to make the column names the same between the two datasets.\nBefore we do that, we will simplify the names in DETE.\n\ndete.columns = (\n    dete.columns\n    .str.lower() # All lowercase\n    .str.strip() # Remove whitespace on sides\n    .str.replace(\" \", \"_\") # Replace spaces with underscores\n)\n\ndete.columns\n\nIndex(['id', 'separationtype', 'cease_date', 'dete_start_date',\n       'role_start_date', 'position', 'classification', 'region',\n       'business_unit', 'employment_status', 'career_move_to_public_sector',\n       'career_move_to_private_sector', 'interpersonal_conflicts',\n       'job_dissatisfaction', 'dissatisfaction_with_the_department',\n       'physical_work_environment', 'lack_of_recognition',\n       'lack_of_job_security', 'work_location', 'employment_conditions',\n       'maternity/family', 'relocation', 'study/travel', 'ill_health',\n       'traumatic_incident', 'work_life_balance', 'workload',\n       'none_of_the_above', 'gender', 'age', 'aboriginal', 'torres_strait',\n       'south_sea', 'disability', 'nesb'],\n      dtype='object')\n\n\nThe DETE columns have been simplified.\nNext, we will change some of the TAFE column labels to match the ones in the DETE dataset.\n\nnew_columns = {\n    'Record ID': 'id',\n    'CESSATION YEAR': 'cease_date',\n    'Reason for ceasing employment': 'separationtype',\n    'Gender. What is your Gender?': 'gender',\n    'CurrentAge. Current Age': 'age',\n    'Employment Type. Employment Type': 'employment_status',\n    'Classification. Classification': 'position',\n    'LengthofServiceOverall. Overall Length of Service at Institute (in years)': 'institute_service',\n    'LengthofServiceCurrent. Length of Service at current workplace (in years)': 'role_service',\n}\n\ntafe = tafe.rename(\n    new_columns,\n    axis = 1,\n)\n\nlist(tafe.columns)\n\n['id',\n 'Institute',\n 'WorkArea',\n 'cease_date',\n 'separationtype',\n 'Contributing Factors. Career Move - Public Sector ',\n 'Contributing Factors. Career Move - Private Sector ',\n 'Contributing Factors. Career Move - Self-employment',\n 'Contributing Factors. Ill Health',\n 'Contributing Factors. Maternity/Family',\n 'Contributing Factors. Dissatisfaction',\n 'Contributing Factors. Job Dissatisfaction',\n 'Contributing Factors. Interpersonal Conflict',\n 'Contributing Factors. Study',\n 'Contributing Factors. Travel',\n 'Contributing Factors. Other',\n 'Contributing Factors. NONE',\n 'gender',\n 'age',\n 'employment_status',\n 'position',\n 'institute_service',\n 'role_service']\n\n\nThe Contributing Factors columns’ names haven’t been changed yet, but this will be dealt with later."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#identifying-employees-who-resigned",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#identifying-employees-who-resigned",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Identifying Employees who Resigned",
    "text": "Identifying Employees who Resigned\nThe DETE and TAFE exit surveys were given to all employees who left the institutions. Some of them were terminated from employment, some resigned, and some retired.\nThe goal of this project is to find out why employees resigned. Thus, we have to find out who resigned, and drop the data for the rest.\nLet’s do this for the DETE dataset first. The separationtype column explains the reason why the employee ceased to work at the institution. What values does this column contain?\n\ndete[\"separationtype\"].value_counts()\n\nAge Retirement                          285\nResignation-Other reasons               150\nResignation-Other employer               91\nResignation-Move overseas/interstate     70\nVoluntary Early Retirement (VER)         67\nIll Health Retirement                    61\nOther                                    49\nContract Expired                         34\nTermination                              15\nName: separationtype, dtype: int64\n\n\nIt looks like the values that are relevant to us are the ones that start with “Resignation.” We’ll keep the rows which indicate resignation and drop the rest.\n\ndete = dete.loc[\n    # Check if the string starts with \"Resignation.\"\n    dete[\"separationtype\"].str.startswith(\"Resignation\")\n]\n\ndete.shape\n\n(311, 35)\n\n\nAfter we dropped non-resignation rows, the DETE dataset was left with 311 rows.\nNext, we’ll do the same for the TAFE dataset. What are the values in its separationtype column?\n\ntafe[\"separationtype\"].value_counts()\n\nResignation                 340\nContract Expired            127\nRetrenchment/ Redundancy    104\nRetirement                   82\nTransfer                     25\nTermination                  23\nName: separationtype, dtype: int64\n\n\nThis time, there is only one value which indicates resignation. We’ll use that to identify the rows to keep.\n\ntafe = tafe.loc[\n    # Check if the value is \"Resignation.\"\n    tafe[\"separationtype\"] == \"Resignation\"\n]\n\ntafe.shape\n\n(340, 23)\n\n\nDropping rows resulted in having 340 rows left in the TAFE dataset.\nNow, both datasets have been narrowed down to data about employees who intentionally left the institutions."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#date-columns",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#date-columns",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Date Columns",
    "text": "Date Columns\nNext, we’ll clean and inspect date columns. Specifically, these are:\n\nDETE dataset\n\ndete_start_date: The date when the employee started to work at DETE.\ncease_date: The date when the employee ceased to work at DETE.\n\nTAFE dataset\n\ncease_date: The date when the employee ceased to work at TAFE.\n\n\nLet’s start with DETE’s cease_date. What are its values?\n\ndete[\"cease_date\"].value_counts()\n\n2012       126\n2013        74\n01/2014     22\n12/2013     17\n06/2013     14\n09/2013     11\n07/2013      9\n11/2013      9\n10/2013      6\n08/2013      4\n05/2012      2\n05/2013      2\n07/2012      1\n2010         1\n09/2010      1\n07/2006      1\nName: cease_date, dtype: int64\n\n\nWe can see that some values only contain years, and others state a month before the year. Since we can’t assume the month for entries without one, we will remove all of the months. Only the years will remain, and we will store them as numerical data.\n\ndete[\"cease_date\"] = (\n    dete[\"cease_date\"]\n    .str.extract(\"(20[0-1][0-9])\") # Extract year using a regular expression\n    .astype(np.float64) # Turn years into decimals\n)\n\ndete[\"cease_date\"].value_counts().sort_index()\n\n2006.0      1\n2010.0      2\n2012.0    129\n2013.0    146\n2014.0     22\nName: cease_date, dtype: int64\n\n\nThe DETE cease_date column now only contains year values. These range from 2006 to 2014.\nNext, let’s look at DETE’s dete_start_date column.\n\ndete[\"dete_start_date\"].value_counts().sort_index()\n\n1963     1\n1971     1\n1972     1\n1973     1\n1974     2\n1975     1\n1976     2\n1977     1\n1980     5\n1982     1\n1983     2\n1984     1\n1985     3\n1986     3\n1987     1\n1988     4\n1989     4\n1990     5\n1991     4\n1992     6\n1993     5\n1994     6\n1995     4\n1996     6\n1997     5\n1998     6\n1999     8\n2000     9\n2001     3\n2002     6\n2003     6\n2004    14\n2005    15\n2006    13\n2007    21\n2008    22\n2009    13\n2010    17\n2011    24\n2012    21\n2013    10\nName: dete_start_date, dtype: int64\n\n\nThe column contains only years, no months, so it is quite clean. Most of the values are from 2004 to 2013. The years in the late 1900’s don’t seem like outliers because there are many values spread throughout those years.\nIt also makes sense that there are no dete_start_date values after 2014, since the latest cease_date is 2014.\nThere’s no need to clean this column, but let’s convert it to numerical data for consistency.\n\ndete[\"dete_start_date\"] = dete[\"dete_start_date\"].astype(np.float64)\n\nLastly, let’s look at the TAFE dataset’s cease_date column.\n\ntafe[\"cease_date\"].value_counts().sort_index()\n\n2009.0      2\n2010.0     68\n2011.0    116\n2012.0     94\n2013.0     55\nName: cease_date, dtype: int64\n\n\nThe data here looks like it’s already clean. The years are expressed as decimals, and there are no outliers to clean up. This column won’t be changed.\nThe TAFE cease_date years range from 2009 to 2013. DETE’s cease_date values range from 2006 to 2014. Therefore, both datasets give information about roughly the same period in time."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#years-of-service",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#years-of-service",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Years of Service",
    "text": "Years of Service\nRemember that one of the goals of the project is to compare dissatisfaction rates between resignees who had worked for a short time and those who had worked for a longer time. Thus, we need to know how many years of service each employee has had.\nThe TAFE dataset already has a column called institute_service which gives information on this.\n\ntafe[\"institute_service\"].value_counts().sort_index()\n\n1-2                   64\n11-20                 26\n3-4                   63\n5-6                   33\n7-10                  21\nLess than 1 year      73\nMore than 20 years    10\nName: institute_service, dtype: int64\n\n\nThese values are somewhat difficult to use since these indicate ranges of years of service. This will be dealt with later, but for now, we have to make a matching column in the DETE dataset.\nIn order to do this, we will subtract dete_start_date from cease_date. This will result in the number of years that each employee has spent working at DETE. The new column will be called institute_service like in the TAFE dataset.\n\ndete[\"institute_service\"] = dete[\"cease_date\"] - dete[\"dete_start_date\"]\n\ndete[\"institute_service\"].value_counts(bins = 10)\n\n(-0.05, 4.9]    92\n(4.9, 9.8]      75\n(9.8, 14.7]     30\n(14.7, 19.6]    26\n(19.6, 24.5]    24\n(24.5, 29.4]     8\n(29.4, 34.3]     8\n(34.3, 39.2]     7\n(39.2, 44.1]     2\n(44.1, 49.0]     1\nName: institute_service, dtype: int64\n\n\nIt can be seen that DETE employees’ years of service range from under 4.9 to over 44.1. Let’s view the distribution in a histogram.\n\nsns.histplot(\n    data = dete,\n    x = \"institute_service\",\n)\n\nplt.title(\"DETE Employees' Years of Service\")\nplt.xlabel(\"Years of Service\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe distribution is right-skewed. Most employees who resigned had worked at DETE for under 10 years."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#contributing-factors-columns",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#contributing-factors-columns",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "“Contributing Factors” Columns",
    "text": "“Contributing Factors” Columns\nThe Contributing Factors columns are about factors which may have influenced the employee’s choice to resign. In the TAFE dataset, these columns have hyphen (\"-\") values. This leads us to wonder what they represent, and whether or not we have to clean them.\nLet’s look at column 5, one of the columns with hyphens.\n\ntafe.iloc[:, 5].value_counts(dropna = False)\n\n-                              284\nCareer Move - Public Sector     48\nNaN                              8\nName: Contributing Factors. Career Move - Public Sector , dtype: int64\n\n\nMost of the values are hyphens. The rest are null values or \"Career Move - Public Sector\".\nWe can infer that the Contributing Factors group of columns represent options in a checkbox item in the survey. That’s why each column only has 2 valid values:\n\n\"-\" means that the option was not selected.\n\"Career Move - Public Sector\" means that the option was selected.\n\nThus, we can change the values in column 5 into True (selected) and False (not selected) for ease of use.\n\n# Define a function to be applied to the column.\ndef identify_selection(value):\n    if pd.isnull(value):\n        return np.nan\n    else:\n        return value != \"-\"\n\n# Apply the function elementwise.\ntafe.iloc[:, 5] = tafe.iloc[:, 5].apply(identify_selection)\n\ntafe.iloc[:, 5].value_counts(dropna = False)\n\nFalse    284\nTrue      48\nNaN        8\nName: Contributing Factors. Career Move - Public Sector , dtype: int64\n\n\nNow, the column only has True, False, and NaN values.\nLet us apply this transformaton to the entire group of “Contributing Factors” columns (5 to 16).\n\ntafe.iloc[:, 6:17] = tafe.iloc[:, 6:17].applymap(identify_selection)\n\ntafe.iloc[:, 5:17].head()\n\n\n\n\n\n\n\n\nContributing Factors. Career Move - Public Sector\nContributing Factors. Career Move - Private Sector\nContributing Factors. Career Move - Self-employment\nContributing Factors. Ill Health\nContributing Factors. Maternity/Family\nContributing Factors. Dissatisfaction\nContributing Factors. Job Dissatisfaction\nContributing Factors. Interpersonal Conflict\nContributing Factors. Study\nContributing Factors. Travel\nContributing Factors. Other\nContributing Factors. NONE\n\n\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n5\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n6\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n7\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\nSince these columns are now boolean columns, these will be easier to use in analysis.\nIt is worth noting that what we did matches the format used for Contributing Factors columns in DETE:\n\ndete.iloc[:5, 10:28]\n\n\n\n\n\n\n\n\ncareer_move_to_public_sector\ncareer_move_to_private_sector\ninterpersonal_conflicts\njob_dissatisfaction\ndissatisfaction_with_the_department\nphysical_work_environment\nlack_of_recognition\nlack_of_job_security\nwork_location\nemployment_conditions\nmaternity/family\nrelocation\nstudy/travel\nill_health\ntraumatic_incident\nwork_life_balance\nworkload\nnone_of_the_above\n\n\n\n\n3\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n5\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n8\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n9\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n11\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\nThus, we will be able to use similar techniques to identify dissatisfaction in both datasets."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#identifying-dissatisfaction",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#identifying-dissatisfaction",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Identifying Dissatisfaction",
    "text": "Identifying Dissatisfaction\nThis project focuses on employees who resigned due to dissatisfaction with their work at the government institute. Thus, we have to identify which employees were dissatisfied.\nIn the TAFE dataset, the following columns indicate dissatisfaction:\n\nContributing Factors. Dissatisfaction\nContributing Factors. Job Dissatisfaction\n\nWe will create a new dissatisfaction column. It will be a boolean column that contains True if at least 1 of the above columns has a value of True.\n\ntafe[\"dissatisfaction\"] = (\n    tafe[[\n        \"Contributing Factors. Dissatisfaction\",\n        \"Contributing Factors. Job Dissatisfaction\",\n    ]]\n    .any(axis = 1, skipna = False)\n)\n\ntafe[\"dissatisfaction\"].value_counts(dropna = False)\n\nFalse    241\nTrue      99\nName: dissatisfaction, dtype: int64\n\n\nIt looks like 91 employees in the TAFE dataset resigned due to dissatisfaction.\nAs for the DETE dataset, there are many relevant columns.\n\njob_dissatisfaction\ndissatisfaction_with_the_department\nphysical_work_environment\nlack_of_recognition\nlack_of_job_security\nwork_location\nemployment_conditions\nwork_life_balance\nworkload\n\nRemember that in the survey, these phrases were options in a checkbox item asking why the employee left. A True value in any of these columns means that the employee considered it a reason why he/she resigned.\nWe will create a new dissatisfaction column in the DETE dataset in the same way as we did for the TAFE dataset. If at least 1 of the above columns is True, the corresponding value in the new column will be True.\n\ndete[\"dissatisfaction\"] = (\n    dete[[\n        'job_dissatisfaction',\n        'dissatisfaction_with_the_department',\n        'physical_work_environment', 'lack_of_recognition',\n        'lack_of_job_security',\n        'work_location',\n        'employment_conditions',\n        'work_life_balance', \n        'workload',\n    ]]\n    .any(axis = 1, skipna = False)\n)\n\ndete[\"dissatisfaction\"].value_counts(dropna = False)\n\nFalse    162\nTrue     149\nName: dissatisfaction, dtype: int64\n\n\nThe results show that 149 of DETE employees who resigned had been dissatisfied."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#combining-dete-and-tafe",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#combining-dete-and-tafe",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Combining DETE and TAFE",
    "text": "Combining DETE and TAFE\nAt this point, the data cleaning we’ve done is sufficient for us to combine the two datasets. We will stack them vertically; they will share columns with identical names.\nWe will still need to be able to differentiate between DETE and TAFE employees, so we will indicate this in a new column.\nIn the DETE dataset, the new institute column will contain the string \"DETE\".\n\ndete[\"institute\"] = \"DETE\"\n\ndete[\"institute\"].head()\n\n3     DETE\n5     DETE\n8     DETE\n9     DETE\n11    DETE\nName: institute, dtype: object\n\n\nThe value in the TAFE dataset will be \"TAFE\".\n\ntafe[\"institute\"] = \"TAFE\"\n\ntafe[\"institute\"].head()\n\n3    TAFE\n4    TAFE\n5    TAFE\n6    TAFE\n7    TAFE\nName: institute, dtype: object\n\n\nLet’s now concatenate the 2 datasets vertically.\n\ncombined = pd.concat(\n    [dete, tafe],\n    axis = 0, # Vertical concatenation\n    ignore_index = True,\n)\n\nprint(combined.shape)\ncombined.head()\n\n(651, 53)\n\n\n\n\n\n\n\n\n\nid\nseparationtype\ncease_date\ndete_start_date\nrole_start_date\nposition\nclassification\nregion\nbusiness_unit\nemployment_status\n...\nContributing Factors. Ill Health\nContributing Factors. Maternity/Family\nContributing Factors. Dissatisfaction\nContributing Factors. Job Dissatisfaction\nContributing Factors. Interpersonal Conflict\nContributing Factors. Study\nContributing Factors. Travel\nContributing Factors. Other\nContributing Factors. NONE\nrole_service\n\n\n\n\n0\n4.0\nResignation-Other reasons\n2012.0\n2005.0\n2006\nTeacher\nPrimary\nCentral Queensland\nNaN\nPermanent Full-time\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n6.0\nResignation-Other reasons\n2012.0\n1994.0\n1997\nGuidance Officer\nNaN\nCentral Office\nEducation Queensland\nPermanent Full-time\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n9.0\nResignation-Other reasons\n2012.0\n2009.0\n2009\nTeacher\nSecondary\nNorth Queensland\nNaN\nPermanent Full-time\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n10.0\nResignation-Other employer\n2012.0\n1997.0\n2008\nTeacher Aide\nNaN\nNaN\nNaN\nPermanent Part-time\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n12.0\nResignation-Move overseas/interstate\n2012.0\n2009.0\n2009\nTeacher\nSecondary\nFar North Queensland\nNaN\nPermanent Full-time\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 53 columns\n\n\n\nThe combined dataset has 651 rows (employees) and 53 columns.\nGiven our research questions, the most important columns to keep are institute_service, age, and dissatisfaction. We would only keep the other columns if we wanted to group the employees on certain characteristics.\nThus, we will remove columns that have under 500 non-null values. These columns wouldn’t have enough useful information for us to use.\n\ncombined.dropna(\n    axis = 1,\n    thresh = 500,\n    inplace = True,\n)\n\nprint(combined.shape)\ncombined.head()\n\n(651, 10)\n\n\n\n\n\n\n\n\n\nid\nseparationtype\ncease_date\nposition\nemployment_status\ngender\nage\ninstitute_service\ndissatisfaction\ninstitute\n\n\n\n\n0\n4.0\nResignation-Other reasons\n2012.0\nTeacher\nPermanent Full-time\nFemale\n36-40\n7.0\nFalse\nDETE\n\n\n1\n6.0\nResignation-Other reasons\n2012.0\nGuidance Officer\nPermanent Full-time\nFemale\n41-45\n18.0\nTrue\nDETE\n\n\n2\n9.0\nResignation-Other reasons\n2012.0\nTeacher\nPermanent Full-time\nFemale\n31-35\n3.0\nFalse\nDETE\n\n\n3\n10.0\nResignation-Other employer\n2012.0\nTeacher Aide\nPermanent Part-time\nFemale\n46-50\n15.0\nTrue\nDETE\n\n\n4\n12.0\nResignation-Move overseas/interstate\n2012.0\nTeacher\nPermanent Full-time\nMale\n31-35\n3.0\nFalse\nDETE\n\n\n\n\n\n\n\nOnly 10 columns were left in the dataset. The 3 most important columns were kept, along with a few columns about useful demographic data."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#cleaning-age-data",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#cleaning-age-data",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Cleaning Age Data",
    "text": "Cleaning Age Data\nOne of the goals of this project involves the age data. Thus, we need to ensure that this data is clean.\nLet us view the unique values in the column.\n\ncombined[\"age\"].value_counts().sort_index()\n\n20 or younger    10\n21  25           33\n21-25            29\n26  30           32\n26-30            35\n31  35           32\n31-35            29\n36  40           32\n36-40            41\n41  45           45\n41-45            48\n46  50           39\n46-50            42\n51-55            71\n56 or older      29\n56-60            26\n61 or older      23\nName: age, dtype: int64\n\n\nWe can see that:\n\nAll values represent a range of ages.\nSome have hyphens, and others have spaces.\nA few are phrases that say “or younger” or “or older.”\n\nTherefore, we will clean the data by defining the following function and applying it elementwise to the column:\n\ndef fix_age(text):\n    if pd.isnull(text):\n        result = np.nan\n    elif \"  \" in text:\n        result = text.replace(\"  \", \"-\")\n    elif text in [\"56-60\", \"61 or older\"]:\n        result = \"56 or older\"\n    else:\n        result = text\n\n    return result\n\ncombined[\"age\"] = combined[\"age\"].apply(fix_age)\n\ncombined[\"age\"].value_counts().sort_index()\n\n20 or younger    10\n21-25            62\n26-30            67\n31-35            61\n36-40            73\n41-45            93\n46-50            81\n51-55            71\n56 or older      78\nName: age, dtype: int64\n\n\nNow, the values in the age column are consistent."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#categorizing-years-of-service",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#categorizing-years-of-service",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Categorizing Years of Service",
    "text": "Categorizing Years of Service\nRemember that DETE and TAFE differed in the format of their institute_service columns:\n\nDETE had decimal numbers representing the number of years.\nTAFE had strings, each of which represented a range of years.\n\nWe could take the TAFE ranges and replace them with the middle value of each range. However, this would make the data inaccurate.\nInstead, we will transform the data to become more general. We’ll group the data into categories which represent ranges of years. This will apply to the entire institute_service column in the combined dataset.\nThe article “Age is Just a Number: Engage Employees by Career Stage, Too” states that employee engagement can be better understood in the context of career stage, i.e., the number of years working at the company. Career stage influences an employee’s work attitude and virtues that they value in the workplace.\nWe can also say that career stage influences employees’ decisions to resign. The most obvious example of this is that a new employee isn’t very invested in the company and would be more likely to leave due to initial dissatisfaction.\nThe article gives the following 4 career stages:\n\nNewbie (0 to 3 years)\nSophomore (3 to 7 years)\nTenured (7 to 11 years)\nSage (11 or more years)\n\nWe will use the above career stages as categories in the institute_service column. But first, we have to be prepared for what kinds of values we will have to transform.\n\ncombined[\"institute_service\"].unique()\n\narray([7.0, 18.0, 3.0, 15.0, 14.0, 5.0, nan, 30.0, 32.0, 39.0, 17.0, 9.0,\n       6.0, 1.0, 35.0, 38.0, 36.0, 19.0, 4.0, 26.0, 10.0, 8.0, 2.0, 0.0,\n       23.0, 13.0, 16.0, 12.0, 21.0, 20.0, 24.0, 33.0, 22.0, 28.0, 49.0,\n       11.0, 41.0, 27.0, 42.0, 25.0, 29.0, 34.0, 31.0, '3-4', '7-10',\n       '1-2', 'Less than 1 year', '11-20', '5-6', 'More than 20 years'],\n      dtype=object)\n\n\nThe values above include:\n\nDecimal numbers\nA range of years (\"x-y\")\nA phrase describing a range of years (\"Less than x years\")\n\n\nExtracting Numbers\nBased on the values present, we will extract the number of years using the regular expression ([0-9]{1,2}). This will capture the first 1 or 2 digit whole number in a string.\n\nFor decimals, the part of the number before the decimal point will be extracted.\nFor ranges of years, the minimum year will be extracted.\nFor phrases, the first number to appear will be extracted.\n\n\ncombined[\"service_num\"] = (\n    combined[\"institute_service\"]\n    .astype(str) # Convert to strings.\n    .str.extract(\"([0-9]{1,2})\") # Extract 1 or 2-digit number.\n    .astype(np.float64) # Convert to decimals.\n)\n\ncombined[\"service_num\"].unique()\n\narray([ 7., 18.,  3., 15., 14.,  5., nan, 30., 32., 39., 17.,  9.,  6.,\n        1., 35., 38., 36., 19.,  4., 26., 10.,  8.,  2.,  0., 23., 13.,\n       16., 12., 21., 20., 24., 33., 22., 28., 49., 11., 41., 27., 42.,\n       25., 29., 34., 31.])\n\n\nThis worked for the decimals, as seen below. The initial and final values are identical.\n\ncombined.loc[\n    combined[\"institute_service\"].astype(str).str.contains(\".\"),\n    [\"institute_service\", \"service_num\"]\n].head()\n\n\n\n\n\n\n\n\ninstitute_service\nservice_num\n\n\n\n\n0\n7.0\n7.0\n\n\n1\n18.0\n18.0\n\n\n2\n3.0\n3.0\n\n\n3\n15.0\n15.0\n\n\n4\n3.0\n3.0\n\n\n\n\n\n\n\nWe can tell that this worked for the ranges because the minimum value was extracted. For example, for \"3-4\", the number 3 was extracted.\n\ncombined.loc[\n    combined[\"institute_service\"].astype(str).str.contains(\"-\"),\n    [\"institute_service\", \"service_num\"]\n].head()\n\n\n\n\n\n\n\n\ninstitute_service\nservice_num\n\n\n\n\n312\n3-4\n3.0\n\n\n313\n7-10\n7.0\n\n\n314\n3-4\n3.0\n\n\n315\n3-4\n3.0\n\n\n316\n3-4\n3.0\n\n\n\n\n\n\n\nIt also worked for the phrases, as seen below. The number in the phrase was extracted.\n\ncombined.loc[\n    combined[\"institute_service\"].astype(str).str.contains(\"than\"),\n    [\"institute_service\", \"service_num\"]\n].head()\n\n\n\n\n\n\n\n\ninstitute_service\nservice_num\n\n\n\n\n318\nLess than 1 year\n1.0\n\n\n329\nLess than 1 year\n1.0\n\n\n332\nMore than 20 years\n20.0\n\n\n333\nLess than 1 year\n1.0\n\n\n334\nLess than 1 year\n1.0\n\n\n\n\n\n\n\n\n\nMapping Numbers to Categories\nNow that we have numbers, we can map them to the career stages mentioned earlier:\n\nNewbie (0 to 3 years)\nSophomore (3 to 7 years)\nTenured (7 to 11 years)\nSage (11 or more years)\n\nWe’ll do this by defining a function then applying it elementwise to the column.\n\n# Function that returns career stage based on number of years.\ndef career_stage(years):\n    if pd.isnull(years):\n        stage = np.nan\n    elif years &lt; 3.0:\n        stage = \"Newbie\"\n    elif years &lt; 7.0:\n        stage = \"Sophomore\"\n    elif years &lt; 11.0:\n        stage = \"Tenured\"\n    elif years &gt;= 11.0:\n        stage = \"Sage\"\n    \n    return stage\n\n# Apply the function elementwise and make a new column.\ncombined[\"service_cat\"] = combined[\"service_num\"].apply(career_stage)\n\ncombined[\"service_cat\"].value_counts()\n\nNewbie       193\nSophomore    172\nSage         136\nTenured       62\nName: service_cat, dtype: int64\n\n\nThe results show that most of the employees who resigned were Newbies.\nData cleaning is now complete, so we can go to data analysis."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dissatisfaction-by-age-group",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dissatisfaction-by-age-group",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Dissatisfaction by Age Group",
    "text": "Dissatisfaction by Age Group\nFirst, we can investigate the dissatisfaction rates of resignees by their age group.\n\n# Drop missing values in specific columns.\ndf = combined.dropna(subset = [\"dissatisfaction\", \"age\"]).copy()\n\n# Cast booleans to integers.\ndf[\"dissatisfaction\"] = df[\"dissatisfaction\"].astype(int)\n\ntable_3 = df.pivot_table(\n    values = [\"dissatisfaction\"],\n    index = [\"age\"],\n    aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0.\n).reset_index()\n\ntable_3\n\n\n\n\n\n\n\n\nage\ndissatisfaction\n\n\n\n\n0\n20 or younger\n0.200000\n\n\n1\n21-25\n0.306452\n\n\n2\n26-30\n0.417910\n\n\n3\n31-35\n0.377049\n\n\n4\n36-40\n0.342466\n\n\n5\n41-45\n0.376344\n\n\n6\n46-50\n0.382716\n\n\n7\n51-55\n0.422535\n\n\n8\n56 or older\n0.423077\n\n\n\n\n\n\n\nThis table is visualized in the bar graph below.\n\nsns.barplot(\n    data = table_3,\n    x = \"age\",\n    y = \"dissatisfaction\",\n    estimator = np.mean,\n    ci = None,\n)\n\nplt.title(\"Dissatisfaction Rates of Resignees by Age Group\")\nplt.xlabel(\"Age Group\")\nplt.ylabel(\"Percentage of Resignees who were Dissatisfied\")\nplt.xticks(rotation = 30, ha = \"right\")\nplt.grid(True)\nplt.show()\n\n\n\n\nNotably, the dissatisfaction rate is:\n\nLowest (20%) among resignees aged 20 or younger.\nAbove 30% among resignees aged 21 or older.\nHighest (42%) among resignees aged 56 or older, 51-55, or 26-30.\n\nLet’s group the data further using the institute (DETE or TAFE).\n\ntable_4 = df.pivot_table(\n    values = [\"dissatisfaction\"],\n    index = [\"age\", \"institute\"],\n    aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0.\n).reset_index()\n\ntable_4\n\n\n\n\n\n\n\n\nage\ninstitute\ndissatisfaction\n\n\n\n\n0\n20 or younger\nDETE\n0.000000\n\n\n1\n20 or younger\nTAFE\n0.222222\n\n\n2\n21-25\nDETE\n0.310345\n\n\n3\n21-25\nTAFE\n0.303030\n\n\n4\n26-30\nDETE\n0.571429\n\n\n5\n26-30\nTAFE\n0.250000\n\n\n6\n31-35\nDETE\n0.551724\n\n\n7\n31-35\nTAFE\n0.218750\n\n\n8\n36-40\nDETE\n0.390244\n\n\n9\n36-40\nTAFE\n0.281250\n\n\n10\n41-45\nDETE\n0.479167\n\n\n11\n41-45\nTAFE\n0.266667\n\n\n12\n46-50\nDETE\n0.452381\n\n\n13\n46-50\nTAFE\n0.307692\n\n\n14\n51-55\nDETE\n0.593750\n\n\n15\n51-55\nTAFE\n0.282051\n\n\n16\n56 or older\nDETE\n0.551020\n\n\n17\n56 or older\nTAFE\n0.206897\n\n\n\n\n\n\n\nThe table is visualized in the bar graph below.\n\nsns.barplot(\n    data = table_4,\n    x = \"age\",\n    y = \"dissatisfaction\",\n    hue = \"institute\",\n    estimator = np.mean,\n    ci = None,\n)\n\nplt.title(\"Dissatisfaction Rates of Resignees by Age Group\")\nplt.xlabel(\"Age Group\")\nplt.ylabel(\"Percentage of Resignees who were Dissatisfied\")\nplt.xticks(rotation = 30, ha = \"right\")\nplt.grid(True)\nplt.show()\n\n\n\n\nWe can see that employees of DETE generally had much higher dissatisfaction rates than employees of TAFE across all age groups.\nThe exception is the “20 or younger” age group, which has a 0% dissatisfaction rate for DETE employees. However, this is due to the fact that only one resignee from DETE was 20 years old or younger.\n\n(\n    combined\n    .loc[combined[\"institute\"] == \"DETE\", \"age\"]\n    .value_counts()\n    .sort_index()\n)\n\n20 or younger     1\n21-25            29\n26-30            35\n31-35            29\n36-40            41\n41-45            48\n46-50            42\n51-55            32\n56 or older      49\nName: age, dtype: int64\n\n\nThus, we can generally say that the dissatisfaction rate was much higher among DETE resignees compared to TAFE resignees.\nAlso, the peak dissatisfaction rates occurred in age groups around 26-30 years old and 51-55 years old for both institutes."
  },
  {
    "objectID": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dissatisfaction-by-career-stage",
    "href": "posts/2021-06-01-employee-exit-survey-data-cleaning-aggregation.html#dissatisfaction-by-career-stage",
    "title": "Employee Exit Survey Data Cleaning and Aggregation",
    "section": "Dissatisfaction by Career Stage",
    "text": "Dissatisfaction by Career Stage\nNext, we will determine what percentage of the resignees was dissatisfied with work based on career stage.\n\n# Drop missing values in specific columns.\ndf = combined.dropna(subset = [\"dissatisfaction\", \"service_cat\"]).copy()\n\n# Cast booleans to integers.\ndf[\"dissatisfaction\"] = df[\"dissatisfaction\"].astype(int)\n\ntable_1 = df.pivot_table(\n    values = [\"dissatisfaction\"],\n    index = [\"service_cat\"],\n    aggfunc = np.mean, # Mean will determine the percentage of True values. True is 1; False is 0.\n).reset_index()\n\ntable_1\n\n\n\n\n\n\n\n\nservice_cat\ndissatisfaction\n\n\n\n\n0\nNewbie\n0.295337\n\n\n1\nSage\n0.485294\n\n\n2\nSophomore\n0.343023\n\n\n3\nTenured\n0.516129\n\n\n\n\n\n\n\nThe table is visualized in the figure below.\n\nsns.barplot(\n    data = table_1,\n    x = \"service_cat\",\n    y = \"dissatisfaction\",\n    estimator = np.mean,\n    ci = None,\n)\n\nplt.title(\"Dissatisfaction Rates of Resignees by Career Stage\")\nplt.xlabel(\"Career Stage\")\nplt.ylabel(\"Percentage of Resignees who were Dissatisfied\")\nplt.xticks(rotation = 0)\nplt.grid(True)\nplt.show()\n\n\n\n\nInterestingly, the dissatisfaction rate is highest (around 50%) within the Tenured and Sage groups of resignees. This is surprising since these are the groups of employees who have been working at the institute for the longest time.\nOne explanation could be that they became dissatisfied because they spent so much time at the company without career growth or without sufficient variety in their work.\nNext, we can group the data further by the specific institute of the employees:\n\ntable_2 = df.pivot_table(\n    values = [\"dissatisfaction\"],\n    index = [\"service_cat\", \"institute\"], # Group on institute too\n    aggfunc = np.mean,\n).reset_index()\n\ntable_2\n\n\n\n\n\n\n\n\nservice_cat\ninstitute\ndissatisfaction\n\n\n\n\n0\nNewbie\nDETE\n0.375000\n\n\n1\nNewbie\nTAFE\n0.262774\n\n\n2\nSage\nDETE\n0.560000\n\n\n3\nSage\nTAFE\n0.277778\n\n\n4\nSophomore\nDETE\n0.460526\n\n\n5\nSophomore\nTAFE\n0.250000\n\n\n6\nTenured\nDETE\n0.609756\n\n\n7\nTenured\nTAFE\n0.333333\n\n\n\n\n\n\n\nThis table is visualized below.\n\nsns.barplot(\n    data = table_2,\n    x = \"service_cat\",\n    y = \"dissatisfaction\",\n    hue = \"institute\",\n    estimator = np.mean,\n    ci = None,\n)\n\nplt.title(\"Dissatisfaction Rates of Resignees by Career Stage\")\nplt.xlabel(\"Career Stage\")\nplt.ylabel(\"Percentage of Resignees who were Dissatisfied\")\nplt.xticks(rotation = 0)\nplt.grid(True)\nplt.show()\n\n\n\n\nThe chart shows that the trend is generally consistent between DETE and TAFE. Tenured and Sage resignees have a higher dissatisfaction rate than other groups.\nHowever, it looks like the dissatisfaction rates in DETE are also much higher than the rates in TAFE. It can be said that dissatisfaction influences the resignation of a greater percentage of people in DETE than it does in TAFE."
  },
  {
    "objectID": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#database-tables",
    "href": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#database-tables",
    "title": "Using SQL to Query and Analyze a CIA Factbook Database",
    "section": "Database Tables",
    "text": "Database Tables\nFirst, we inspect the tables in the database. SQL is used in a code cell via the %%sql magic.\n\n%%sql\nSELECT *\nFROM sqlite_master\nWHERE type = \"table\";\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\ntable\nsqlite_sequence\nsqlite_sequence\n3\nCREATE TABLE sqlite_sequence(name,seq)\n\n\ntable\nfacts\nfacts\n47\nCREATE TABLE \"facts\" (\"id\" INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"code\" varchar(255) NOT NULL, \"name\" varchar(255) NOT NULL, \"area\" integer, \"area_land\" integer, \"area_water\" integer, \"population\" integer, \"population_growth\" float, \"birth_rate\" float, \"death_rate\" float, \"migration_rate\" float)\n\n\n\n\n\nThere are two tables in the database: sqlite_sequence and facts. The latter contains the data that we will analyze. The sql column above lists the columns in the facts table and their data types. The column names include “area”, “population”, “population_growth”, etc. which are the kind of information that we would expect to know about a country."
  },
  {
    "objectID": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#the-facts-table",
    "href": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#the-facts-table",
    "title": "Using SQL to Query and Analyze a CIA Factbook Database",
    "section": "The facts Table",
    "text": "The facts Table\nLet us inspect the first 5 rows of the facts table.\n\n%%sql\nSELECT *\nFROM facts\nLIMIT 5 --#Limit the result to the first 5 rows.\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nid\ncode\nname\narea\narea_land\narea_water\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\nmigration_rate\n\n\n\n\n1\naf\nAfghanistan\n652230\n652230\n0\n32564342\n2.32\n38.57\n13.89\n1.51\n\n\n2\nal\nAlbania\n28748\n27398\n1350\n3029278\n0.3\n12.92\n6.58\n3.3\n\n\n3\nag\nAlgeria\n2381741\n2381741\n0\n39542166\n1.84\n23.67\n4.31\n0.92\n\n\n4\nan\nAndorra\n468\n468\n0\n85580\n0.12\n8.13\n6.96\n0.0\n\n\n5\nao\nAngola\n1246700\n1246700\n0\n19625353\n2.78\n38.78\n11.49\n0.46\n\n\n\n\n\nMost of the column names are self-explanatory. The following are some additional information about the data according to Dataquest.\n\nThe areas are given in square kilometers (\\(\\text{km}^2\\))\npopulation_growth is given as the percentage of increase of the total population per year.\nbirth_rate and death_rate are given as the number of people who are born/pass away per year per 1000 people."
  },
  {
    "objectID": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#summary-statistics-on-population-size-and-growth",
    "href": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#summary-statistics-on-population-size-and-growth",
    "title": "Using SQL to Query and Analyze a CIA Factbook Database",
    "section": "Summary Statistics on Population Size and Growth",
    "text": "Summary Statistics on Population Size and Growth\nBelow, we calculate some basic summary statistics about the countries’ populations and population growths. We want to see which countries have the highest and lowest value of each variable.\n\n%%sql\nSELECT\n    MIN(population),\n    MAX(population),\n    MIN(population_growth),\n    MAX(population_growth)\nFROM facts\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nMIN(population)\nMAX(population)\nMIN(population_growth)\nMAX(population_growth)\n\n\n\n\n0\n7256490011\n0.0\n4.02\n\n\n\n\n\nInterestingly, the minimum population size among all of the countries in the table is 0, meaning no people at all. This is surprising since one would expect a country to have many people. This may indicate an error in the data, so let’s check which countries have 0 people.\n\n%%sql\nSELECT *\nFROM facts\nWHERE population == 0\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nid\ncode\nname\narea\narea_land\narea_water\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\nmigration_rate\n\n\n\n\n250\nay\nAntarctica\nNone\n280000\nNone\n0\nNone\nNone\nNone\nNone\n\n\n\n\n\nIt turns out that it is not a country, but rather the continent of Antarctica, which is at the South Pole. It is too cold there for people to live comfortably, so of course its population size would be 0.\nThe maximum population size, on the other hand, is over 7 billion, which seems too large. Let’s check which country this is.\n\n%%sql\nSELECT *\nFROM facts\nWHERE population == (\n    SELECT MAX(population) --# Use a subquery to get the exact value of the maximum population size.\n    FROM facts\n)\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nid\ncode\nname\narea\narea_land\narea_water\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\nmigration_rate\n\n\n\n\n261\nxx\nWorld\nNone\nNone\nNone\n7256490011\n1.08\n18.6\n7.8\nNone\n\n\n\n\n\nIt turns out that 7 billion refers to the total population in the entire world, not one country, so there is no error in the data. The data seems to have been collected in the mid-2010s, since the world population reached 7.2 billion in 2013."
  },
  {
    "objectID": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#revised-summary-statistics",
    "href": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#revised-summary-statistics",
    "title": "Using SQL to Query and Analyze a CIA Factbook Database",
    "section": "Revised Summary Statistics",
    "text": "Revised Summary Statistics\nWe’ve come to the realization that this database doesn’t just include entries on individual countries but also on entire continents and the entire world. Therefore, if we want to find useful information, we should exclude the entries for Antarctica and the World. We can do this using SQL’s NOT IN operator.\nLet’s run our initial query again with this in mind.\n\n%%sql\nSELECT\n    MIN(population),\n    MAX(population),\n    MIN(population_growth),\n    MAX(population_growth)\nFROM facts\nWHERE name NOT IN (\"World\", \"Antarctica\")\n--# Exclude World and Antarctica from the entries being used in computation.\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nMIN(population)\nMAX(population)\nMIN(population_growth)\nMAX(population_growth)\n\n\n\n\n48\n1367485388\n0.0\n4.02\n\n\n\n\n\nThe population growth statistics are the same as before, but now, the population size statistics are different.\nInterestingly, the minimum population size is still very small—only 48 people. Which country is this?\n\n%%sql\nSELECT *\nFROM facts\nWHERE population == (\n    SELECT MIN(population)\n    FROM facts\n    WHERE name NOT IN (\"World\", \"Antarctica\")\n)\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nid\ncode\nname\narea\narea_land\narea_water\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\nmigration_rate\n\n\n\n\n238\npc\nPitcairn Islands\n47\n47\n0\n48\n0.0\nNone\nNone\nNone\n\n\n\n\n\nThe country with a population of 48 is the Pitcairn Islands, which, according to the World Factbook, is a colony of the UK. The islands are small (the query result above shows only 47 square kilometers of land), so there are few people.\nOn the other hand, the maximum population size was over 1.3 billion. Let’s check which country this is.\n\n%%sql\nSELECT *\nFROM facts\nWHERE population == (\n    SELECT MAX(population)\n    FROM facts\n    WHERE name NOT IN (\"World\", \"Antarctica\")\n)\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nid\ncode\nname\narea\narea_land\narea_water\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\nmigration_rate\n\n\n\n\n37\nch\nChina\n9596960\n9326410\n270550\n1367485388\n0.45\n12.49\n7.53\n0.44\n\n\n\n\n\nOf course, the country with the largest population would be China. I expected this before I ran the query; China is well-known for the one-child policy that it enacted from 1980 to 2015 in order to curb the growth of its huge population.\nNow, let’s investigate the minimum population growth, which was 0%. This means that the population wasn’t increasing at all at that time.\n\n%%sql\nSELECT *\nFROM facts\nWHERE\n    population_growth == 0\n    AND name NOT IN (\"Antarctica\", \"World\")\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nid\ncode\nname\narea\narea_land\narea_water\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\nmigration_rate\n\n\n\n\n190\nvt\nHoly See (Vatican City)\n0\n0\n0\n842\n0.0\nNone\nNone\nNone\n\n\n200\nck\nCocos (Keeling) Islands\n14\n14\n0\n596\n0.0\nNone\nNone\nNone\n\n\n207\ngl\nGreenland\n2166086\n2166086\nNone\n57733\n0.0\n14.48\n8.49\n5.98\n\n\n238\npc\nPitcairn Islands\n47\n47\n0\n48\n0.0\nNone\nNone\nNone\n\n\n\n\n\nFour countries have shown up in the results. One of these is the Pitcairn Islands which were mentioned earlier. This makes sense as they have few people to begin with, and the Factbook states that it is rare for outsiders to migrate there.\nInterestingly, Vatican City is among the countries without population growth, but this makes sense too. It is a small independent state within Italy that serves as the authority of Roman Catholicism.\nFinally, let’s check the country with the highest population growth rate.\n\n%%sql\nSELECT *\nFROM facts\nWHERE population_growth == (\n    SELECT MAX(population_growth)\n    FROM facts\n    WHERE name NOT IN (\"World\", \"Antarctica\")\n)\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nid\ncode\nname\narea\narea_land\narea_water\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\nmigration_rate\n\n\n\n\n162\nod\nSouth Sudan\n644329\nNone\nNone\n12042910\n4.02\n36.91\n8.18\n11.47\n\n\n\n\n\nThe country with the highest population growth rate is South Sudan. Its population was increasing by 4.02% annually at the time that the data was collected. According to Hicks (2017), there was widespread famine in South Sudan, and its high population growth rendered the country’s level of food production insufficient."
  },
  {
    "objectID": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#population-raw-increase",
    "href": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#population-raw-increase",
    "title": "Using SQL to Query and Analyze a CIA Factbook Database",
    "section": "Population Raw Increase",
    "text": "Population Raw Increase\nOut of curiosity, we can investigate which countries may have the highest population raw increase. This means the increase in terms of the number of people as opposed to the percentage of the population.\nWe can calculate this in an SQL query by multiplying population size by population growth rate. However, the query result needs to be stored as a DataFrame so that it can be used in a chart.\nFirst, we can store the result of an SQL query as a ResultSet object by using the &lt;&lt; operator, according to the ipython-sql README.\n\n%%sql result &lt;&lt;\nSELECT\n    name,\n    population,\n    population_growth,\n    (population * population_growth / 100.0) AS \"population_raw_increase\" \nFROM facts\nWHERE name NOT IN (\"World\", \"Antarctica\")\nORDER BY population_raw_increase DESC\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\nReturning data to local variable result\n\n\nThen, the ResultSet can be converted into a DataFrame via its .DataFrame() method.\n\npop_increase = result.DataFrame()\n\nprint(pop_increase.shape)\npop_increase.head(10)\n\n(259, 4)\n\n\n\n\n\n\n\n\n\nname\npopulation\npopulation_growth\npopulation_raw_increase\n\n\n\n\n0\nIndia\n1.251696e+09\n1.22\n1.527069e+07\n\n\n1\nChina\n1.367485e+09\n0.45\n6.153684e+06\n\n\n2\nNigeria\n1.815621e+08\n2.45\n4.448270e+06\n\n\n3\nPakistan\n1.990858e+08\n1.46\n2.906653e+06\n\n\n4\nEthiopia\n9.946582e+07\n2.89\n2.874562e+06\n\n\n5\nBangladesh\n1.689577e+08\n1.60\n2.703324e+06\n\n\n6\nUnited States\n3.213689e+08\n0.78\n2.506677e+06\n\n\n7\nIndonesia\n2.559937e+08\n0.92\n2.355142e+06\n\n\n8\nCongo, Democratic Republic of the\n7.937514e+07\n2.45\n1.944691e+06\n\n\n9\nPhilippines\n1.009984e+08\n1.61\n1.626074e+06\n\n\n\n\n\n\n\nThe DataFrame above shows the top 10 countries in terms of population raw increase, which is written in scientific notation. Notably, while most of the values have an exponent of 6, India’s has an exponent of 7. This means that its population raw increase is a whole order of magnitude higher than that of the other countries.\nWe can communicate this result better by graphing population growth against population size, with population raw increase as a continuous grouping variable.\n\n\nCode\n(\n    alt.Chart(pop_increase)\n    .mark_point()\n    .encode(\n        x = alt.X(\"population:Q\", title = \"Population Size\"),\n        y = alt.Y(\"population_growth:Q\", title = \"Population Growth Rate (% of Population)\"),\n        color = alt.Color(\"population_raw_increase:Q\", title = \"Population Raw Increase\"),\n        tooltip = [\n            \"name\",\n            \"population\",\n            \"population_growth\",\n            \"population_raw_increase\",\n        ]\n    )\n    .properties(title = \"Population Growth Rate against Population Size\")\n    .interactive()\n)\n\n\n\n\n\n\n\nOne can click, drag, and scroll on the chart above in order to explore its data points. Hover over a point in order to view extra information like the country’s name.\nIn the case of the chart above, the population raw increase variable ranges from 0 people to 15 million people; this range is expressed with a color gradient. Looking at the data points, we can see that most of them are pale blue, indicating a low increase. India’s point has the darkest blue color.\nHovering over India’s point, we can see its tooltip, which states that the population growth rate of India is 1.22%. This is not the highest global population growth rate; we saw earlier that this is around 4%. However, India’s extremely large population size (over 1 billion) leads it to have the highest raw population increase."
  },
  {
    "objectID": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#population-density",
    "href": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#population-density",
    "title": "Using SQL to Query and Analyze a CIA Factbook Database",
    "section": "Population Density",
    "text": "Population Density\nNext, in this section, we will find the average population size and average land area among all countries.\n\n%%sql\nSELECT\n    AVG(population),\n    AVG(area_land)\nFROM facts\nWHERE name NOT IN (\"World\", \"Antarctica\")\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nAVG(population)\nAVG(area_land)\n\n\n\n\n32377011.0125\n523693.2\n\n\n\n\n\nThe average population size is over 32 million people, and the average land area is over 500 thousand square kilometers.\nWe can now investigate which countries have above-average population size and below-average land area.\n\n%%sql\nSELECT\n    name,\n    population,\n    population_growth,\n    area_land\nFROM facts\nWHERE\n    population &gt; (\n        SELECT AVG(population)\n        FROM facts\n        WHERE name NOT IN (\"World\", \"Antarctica\")\n    )\n    AND\n    area_land &lt; (\n        SELECT AVG(area_land)\n        FROM facts\n        WHERE name NOT IN (\"World\", \"Antarctica\")\n    )\n    AND\n    name NOT IN (\"World\", \"Antarctica\")\nORDER BY population_growth DESC\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\n\n\n\n\n\nname\npopulation\npopulation_growth\narea_land\n\n\n\n\nUganda\n37101745\n3.24\n197100\n\n\nIraq\n37056169\n2.93\n437367\n\n\nPhilippines\n100998376\n1.61\n298170\n\n\nBangladesh\n168957745\n1.6\n130170\n\n\nMorocco\n33322699\n1.0\n446300\n\n\nVietnam\n94348835\n0.97\n310070\n\n\nSpain\n48146134\n0.89\n498980\n\n\nUnited Kingdom\n64088222\n0.54\n241930\n\n\nThailand\n67976405\n0.34\n510890\n\n\nItaly\n61855120\n0.27\n294140\n\n\nGermany\n80854408\n0.17\n348672\n\n\nJapan\n126919659\n0.16\n364485\n\n\nKorea, South\n49115196\n0.14\n96920\n\n\nPoland\n38562189\n0.09\n304255\n\n\n\n\n\nSince the countries in this table have above-average population size and below-average land area, these may be at risk of facing high population density in the future. This may be especially true for those with the highest population growth rates, such as Uganda, Iraq, and the Philippines.\nIf we want to know which countries already have a high population density, though, we can run the query below. It calculates the population density as population size over land area, and then it displays the countries with the top 10 highest population densities.\n\n%%sql result &lt;&lt;\nSELECT\n    name,\n    population,\n    population_growth,\n    area_land,\n    population / area_land AS \"population_density\"\nFROM facts\nWHERE name NOT IN (\"World\", \"Antarctica\")\nORDER BY population_density DESC\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\nReturning data to local variable result\n\n\nThe resulting DataFrame is shown below. We have added 2 additional columns which will help us in making a chart:\n\nplace: Whether the country is in the top 10 countries in terms of population density.\ntext: The text to display for each country.\n\n\npop_density = result.DataFrame()\n\n# Place variable\npop_density[\"place\"] = [\"Top 10\"] * 10 + [\"Below Top 10\"] * (len(pop_density) - 10)\n\n# Text variable\npop_density[\"text\"] = (\n    pop_density[\"name\"].copy()\n    # Append text that indicates the numeric place of the country.\n    .str.cat([\" (Top {})\".format(i) for i in range(1, len(pop_density) + 1)])\n)\n\n# Make the text blank for the countries below the top 10.\npop_density.loc[\n    pop_density[\"place\"] == \"Below Top 10\",\n    \"text\"\n] = \"\"\n\npop_density.head(11)\n\n\n\n\n\n\n\n\nname\npopulation\npopulation_growth\narea_land\npopulation_density\nplace\ntext\n\n\n\n\n0\nMacau\n592731.0\n0.80\n28.0\n21168.0\nTop 10\nMacau (Top 1)\n\n\n1\nMonaco\n30535.0\n0.12\n2.0\n15267.0\nTop 10\nMonaco (Top 2)\n\n\n2\nSingapore\n5674472.0\n1.89\n687.0\n8259.0\nTop 10\nSingapore (Top 3)\n\n\n3\nHong Kong\n7141106.0\n0.38\n1073.0\n6655.0\nTop 10\nHong Kong (Top 4)\n\n\n4\nGaza Strip\n1869055.0\n2.81\n360.0\n5191.0\nTop 10\nGaza Strip (Top 5)\n\n\n5\nGibraltar\n29258.0\n0.24\n6.0\n4876.0\nTop 10\nGibraltar (Top 6)\n\n\n6\nBahrain\n1346613.0\n2.41\n760.0\n1771.0\nTop 10\nBahrain (Top 7)\n\n\n7\nMaldives\n393253.0\n0.08\n298.0\n1319.0\nTop 10\nMaldives (Top 8)\n\n\n8\nMalta\n413965.0\n0.31\n316.0\n1310.0\nTop 10\nMalta (Top 9)\n\n\n9\nBermuda\n70196.0\n0.50\n54.0\n1299.0\nTop 10\nBermuda (Top 10)\n\n\n10\nBangladesh\n168957745.0\n1.60\n130170.0\n1297.0\nBelow Top 10\n\n\n\n\n\n\n\n\nThe results above show the top 10 countries in terms of population density in people per square kilometer of land. Notably, Monaco and Gibraltar are cases of countries with under 10 square kilometers of land but also tens of thousands of people. On the other hand, Singapore and Hong Kong are cases of countries with a somewhat larger land area but huge population size due to urbanization. Hong Kong has the highest number of over 150 m tall buildings in the world, and Singapore has the 12th highest.\nJackson (2017) states that high population density in a place can lead to noise pollution, territorial behavior, and lack of personal space for the people there. Thus, these may be significant challenges for the countries listed above.\nContinuing on, we can take our analysis a step further by investigating countries with both high population density and high population growth rate. Such countries may be at risk of further aggravation of population density issues.\n\n\nCode\n# Base layer with x and y axes\nbase = (\n    alt.Chart(pop_density)\n    .mark_point()\n    .encode(\n        x = alt.X(\n            \"area_land:Q\",\n            title = \"Land Area (Square Kilometers)\",\n            scale = alt.Scale(domain = (0, 1100)),\n            # Limit the x and y axis domains.\n            # We will zoom in on the part of the chart that shows the important points.\n        ),\n        y = alt.Y(\n            \"population:Q\",\n            title = \"Population Size\",\n            scale = alt.Scale(domain = (0, 6000000)),\n        ),\n    )\n)\n\n# Scatter plot layer\npoints = (\n    base\n    .encode(\n        # Express groupings using color and size.\n        color = alt.Color(\"place:N\", title = \"Category of Population Density\"),\n        size = alt.Size(\"population_growth:Q\", title = \"Population Growth Rate (% of Population)\"),\n        # Provide a tooltip with extra information.\n        tooltip = [\n            \"name\",\n            \"area_land\",\n            \"population\",\n            \"population_density\",\n            \"population_growth\",\n        ],\n    )\n)\n\n# Text layer\ntext = (\n    base\n    .mark_text(\n        align = \"center\",\n        baseline = \"middle\",\n        dx = 60,\n        # Nudge text to the right.\n    )\n    .encode(\n        # Display the text variable.\n        text = \"text\",\n    )\n)\n\n# Combine layers.\nchart = (\n    (points + text)\n    .properties(title = \"Top 10 Countries in Population Density\")\n    .interactive()\n)\n\n# Display chart.\nchart\n\n\n\n\n\n\n\nIn the chart above, the orange points represent the top 10 countries in terms of population density, while the blue points represent the other countries. Larger points have a higher population growth rate.\nMacau has the highest population density. However, Gaza Strip, Bahrain, and Singapore have higher population growth rates than Macau does (as indicated by the larger circles). This indicates that these countries’ population sizes may increase quickly, thus leading to increased population density and aggravating the issues that come with it."
  },
  {
    "objectID": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#death-rate-and-birth-rate",
    "href": "posts/2021-07-24-using-sql-query-analyze-cia-factbook-database.html#death-rate-and-birth-rate",
    "title": "Using SQL to Query and Analyze a CIA Factbook Database",
    "section": "Death Rate and Birth Rate",
    "text": "Death Rate and Birth Rate\nOur last query in SQL shall investigate which countries have a higher death rate than birth rate. Such countries may face a decrease in their population size.\nNote that we will calculate a new variable, death_offset_birth, by subtracting birth rate from death rate. This variable is indicative of the net decrease in population (if only births and deaths are taken into account).\n\n%%sql result &lt;&lt;\nSELECT\n    name,\n    population,\n    population_growth,\n    birth_rate,\n    death_rate,\n    death_rate - birth_rate AS \"death_offset_birth\"\nFROM facts\nWHERE name NOT IN (\"World\", \"Antarctica\")\nORDER BY death_offset_birth DESC\n;\n\n * sqlite:///private/2021-07-24-SQL-Fundamentals-Files/factbook.db\nDone.\nReturning data to local variable result\n\n\nThe result of the query is shown below. Additionally, we have created a new column, higher_category. This indicates whether the death rate or the birth rate is higher for a particular country.\n\ndb_rates = result.DataFrame()\n\ndb_rates[\"higher_category\"] = (\n    db_rates[\"death_offset_birth\"]\n    .apply(lambda x: \"Death Rate Higher\" if x &gt; 0 else \"Birth Rate Higher\")\n)\n\ndb_rates.head(10)\n\n\n\n\n\n\n\n\nname\npopulation\npopulation_growth\nbirth_rate\ndeath_rate\ndeath_offset_birth\nhigher_category\n\n\n\n\n0\nBulgaria\n7186893.0\n0.58\n8.92\n14.44\n5.52\nDeath Rate Higher\n\n\n1\nSerbia\n7176794.0\n0.46\n9.08\n13.66\n4.58\nDeath Rate Higher\n\n\n2\nLatvia\n1986705.0\n1.06\n10.00\n14.31\n4.31\nDeath Rate Higher\n\n\n3\nLithuania\n2884433.0\n1.04\n10.10\n14.27\n4.17\nDeath Rate Higher\n\n\n4\nUkraine\n44429471.0\n0.60\n10.72\n14.46\n3.74\nDeath Rate Higher\n\n\n5\nHungary\n9897541.0\n0.22\n9.16\n12.73\n3.57\nDeath Rate Higher\n\n\n6\nGermany\n80854408.0\n0.17\n8.47\n11.42\n2.95\nDeath Rate Higher\n\n\n7\nSlovenia\n1983412.0\n0.26\n8.42\n11.37\n2.95\nDeath Rate Higher\n\n\n8\nRomania\n21666350.0\n0.30\n9.14\n11.90\n2.76\nDeath Rate Higher\n\n\n9\nCroatia\n4464844.0\n0.13\n9.45\n12.18\n2.73\nDeath Rate Higher\n\n\n\n\n\n\n\nThe results above show the top 10 countries in terms of the difference between death rate and birth rate. At the top is Bulgaria; for every 1000 people each year, 14.44 die and 8.92 are born, meaning that deaths outnumber births by 5.52.\nWe can make an area chart in order to get an idea of the death rate situation for all countries.\nBefore that, though, how many countries have a death rate that is higher than the birth rate?\n\n(db_rates[\"higher_category\"] == \"Death Rate Higher\").sum()\n\n24\n\n\nThere are 24 such countries. Thus, we can mention this fact in the chart title.\nThe chart is shown below.\n\n\nCode\n(\n    alt.Chart(db_rates)\n    .mark_area(point = True) # Show a point for each observation.\n    .encode(\n        x = alt.X(\"birth_rate:Q\", title = \"Birth Rate (Births per 1000 People per Year)\"),\n        y = alt.Y(\"death_offset_birth:Q\", title = \"Death Rate offset by Birth Rate\"),\n        color = alt.Color(\"higher_category:N\", title = \"Higher Rate\"),\n        tooltip = [\n            \"name\",\n            \"birth_rate\",\n            \"death_rate\",\n            \"death_offset_birth\",\n        ],\n    )\n    .properties(title = \"Death Rate is Higher than Birth Rate for 24 Countries\")\n    .interactive()\n)\n\n\n\n\n\n\n\nIn the chart above, each point represents a country. Click, drag, and scroll in order to explore the countries and read their tooltips.\nThe horizontal line marks the spot where the birth rate and death rate are equal—they cancel each other out. The blue part represents the majority of countries, which have a birth rate higher than the death rate. On the other hand, the orange part represents the 24 countries where deaths outnumber births.\nInterestingly, there is a downward trend in the chart. Death rate offset by birth rate appears to have a strong negative correlation with birth rate.\nMore importantly, one would expect the 24 highlighted countries to have a negative population growth. However, if we look at the table, the population growth rates are still positive. This means that other factors such as migration may be involved in keeping the population growing rather than shrinking."
  },
  {
    "objectID": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#tables-and-views",
    "href": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#tables-and-views",
    "title": "Answering Business Questions for an Online Music Store using SQL",
    "section": "Tables and Views",
    "text": "Tables and Views\nFirst, we’ll inspect the tables and views available in the chinook.db database.\n\nTables contain columns of data. Each column has a different name and data type.\nViews do not contain data. Instead, these are pre-written SQL queries which show a transformation of existing data. Thus, it can be called a “virtual table.” (Sławińska 2020)\n\n\n%%sql\nSELECT\n    name,\n    type\nFROM sqlite_master\nWHERE type IN (\"table\", \"view\")\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\n\n\n\n\n\nname\ntype\n\n\n\n\nalbum\ntable\n\n\nartist\ntable\n\n\ncustomer\ntable\n\n\nemployee\ntable\n\n\ngenre\ntable\n\n\ninvoice\ntable\n\n\ninvoice_line\ntable\n\n\nmedia_type\ntable\n\n\nplaylist\ntable\n\n\nplaylist_track\ntable\n\n\ntrack\ntable\n\n\nusa_track_purchases\nview\n\n\nusa_genre_sales\nview\n\n\nagent_customer\nview\n\n\nagent_stats\nview\n\n\ncountry_invoices\nview\n\n\ncountry_customers\nview\n\n\ncountry_labels\nview\n\n\ncountry_avg_order\nview\n\n\ncountry_stats\nview\n\n\ninvoice_tracks_bought\nview\n\n\ninvoice_album\nview\n\n\ninvoice_full_album_tracks\nview\n\n\ninvoice_purchase_type\nview\n\n\npurchase_type_proportion\nview\n\n\n\n\n\nOriginally, there were only 11 tables and 0 views in the database. The views listed above are ones which I made throughout this project. I will show how I created these views in later sections.\nFor now, let’s inspect the customer table, as we will be using it to answer most of the company’s business questions.\n\n%%sql\nSELECT *\nFROM customer\nLIMIT 5\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\n\n\n\n\n\ncustomer_id\nfirst_name\nlast_name\ncompany\naddress\ncity\nstate\ncountry\npostal_code\nphone\nfax\nemail\nsupport_rep_id\n\n\n\n\n1\nLuís\nGonçalves\nEmbraer - Empresa Brasileira de Aeronáutica S.A.\nAv. Brigadeiro Faria Lima, 2170\nSão José dos Campos\nSP\nBrazil\n12227-000\n+55 (12) 3923-5555\n+55 (12) 3923-5566\nluisg@embraer.com.br\n3\n\n\n2\nLeonie\nKöhler\nNone\nTheodor-Heuss-Straße 34\nStuttgart\nNone\nGermany\n70174\n+49 0711 2842222\nNone\nleonekohler@surfeu.de\n5\n\n\n3\nFrançois\nTremblay\nNone\n1498 rue Bélanger\nMontréal\nQC\nCanada\nH2G 1A7\n+1 (514) 721-4711\nNone\nftremblay@gmail.com\n3\n\n\n4\nBjørn\nHansen\nNone\nUllevålsveien 14\nOslo\nNone\nNorway\n0171\n+47 22 44 22 22\nNone\nbjorn.hansen@yahoo.no\n4\n\n\n5\nFrantišek\nWichterlová\nJetBrains s.r.o.\nKlanova 9/506\nPrague\nNone\nCzech Republic\n14700\n+420 2 4172 5555\n+420 2 4172 5555\nfrantisekw@jetbrains.com\n4\n\n\n\n\n\nEach row in this table contains data on a different customer of Chinook. Each customer has a unique customer ID and an assigned support representative from Chinook. The support rep’s employee ID is stored in the support_rep_id column. The other columns contain information on the customer’s name, occupation, location, and contact details.\n\n\n\n\n\n\nNote\n\n\n\nAll names and personal details in the Chinook database are fictitious and randomly generated. Public use of this database is not a breach of data privacy."
  },
  {
    "objectID": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#best-selling-music-genres-in-the-usa",
    "href": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#best-selling-music-genres-in-the-usa",
    "title": "Answering Business Questions for an Online Music Store using SQL",
    "section": "Best-Selling Music Genres in the USA",
    "text": "Best-Selling Music Genres in the USA\nIn our first scenario, Chinook has signed a deal with a new record company, so its tracks can now be put up for sale on the Chinook store. The record company has 4 albums so far; below are the artist names and their genres.\n\nRegal (Hip-Hop)\nRed Tone (Punk)\nMeteor and the Girls (Pop)\nSlim Jim Bites (Blues)\n\nHowever, Chinook would like to spread its releases over time, so it will only add 3 albums to the store. Thus, we have to determine the best-selling genres on the store. Furthermore, since the record company would like to target a USA audience, we can narrow our analysis to Chinook’s USA customers.\nFirst, we create a view called usa_track_purchases. This will show the genre, track name, unit price, and quantity bought for each of the invoice lines of USA customers.\n\n%%sql\nDROP VIEW IF EXISTS usa_track_purchases;\n\nCREATE VIEW usa_track_purchases AS\n    SELECT\n        il.invoice_line_id AS invoice_line_id,\n        g.name AS genre,\n        t.name AS track_name,\n        il.unit_price AS unit_price,\n        il.quantity AS quantity\n    FROM customer AS c\n    INNER JOIN\n        invoice AS iv\n        ON iv.customer_id = c.customer_id\n    INNER JOIN\n        invoice_line AS il\n        ON il.invoice_id = iv.invoice_id\n    INNER JOIN\n        track AS t\n        ON t.track_id = il.track_id\n    INNER JOIN\n        genre AS g\n        ON g.genre_id = t.genre_id\n    WHERE c.country = \"USA\"\n;\n\nSELECT *\nFROM usa_track_purchases\nLIMIT 7\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ninvoice_line_id\ngenre\ntrack_name\nunit_price\nquantity\n\n\n\n\n1\nRock\nRight Next Door to Hell\n0.99\n1\n\n\n2\nRock\nDust N' Bones\n0.99\n1\n\n\n3\nRock\nLive and Let Die\n0.99\n1\n\n\n4\nRock\nDon't Cry (Original)\n0.99\n1\n\n\n5\nRock\nPerfect Crime\n0.99\n1\n\n\n6\nRock\nYou Ain't the First\n0.99\n1\n\n\n7\nRock\nBad Obsession\n0.99\n1\n\n\n\n\n\nBy querying the view above, we will create another view, usa_genre_sales. This will contain the following specific information about each genre:\n\nNumber of tracks sold\nPercentage of tracks sold\nTotal sales in US dollars\n\n\n%%sql \nDROP VIEW IF EXISTS usa_genre_sales;\n\nCREATE VIEW usa_genre_sales AS\n    SELECT\n        genre,\n        \n        SUM(quantity) AS number_sold,\n        \n        --Get the quantity per genre and divide it by the total quantity of USA purchases.\n        ROUND(\n            CAST(SUM(quantity) AS Float)\n            / CAST(\n                (SELECT COUNT(*) FROM usa_track_purchases)\n                AS Float\n            )\n            * 100.0,\n            2\n        ) AS percentage_sold,\n        \n        ROUND(\n            SUM(unit_price * CAST(quantity AS Float)),\n            2\n        ) AS total_sales\n        \n    FROM usa_track_purchases\n    GROUP BY genre\n    ORDER BY number_sold DESC, total_sales DESC\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\n\n\n[]\n\n\n\n%%sql result &lt;&lt;\nSELECT *\nFROM usa_genre_sales\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nReturning data to local variable result\n\n\n\nusa_genre_df = result.DataFrame()\nusa_genre_df\n\n\n\n\n\n\n\n\ngenre\nnumber_sold\npercentage_sold\ntotal_sales\n\n\n\n\n0\nRock\n561\n53.38\n555.39\n\n\n1\nAlternative & Punk\n130\n12.37\n128.70\n\n\n2\nMetal\n124\n11.80\n122.76\n\n\n3\nR&B/Soul\n53\n5.04\n52.47\n\n\n4\nBlues\n36\n3.43\n35.64\n\n\n5\nAlternative\n35\n3.33\n34.65\n\n\n6\nLatin\n22\n2.09\n21.78\n\n\n7\nPop\n22\n2.09\n21.78\n\n\n8\nHip Hop/Rap\n20\n1.90\n19.80\n\n\n9\nJazz\n14\n1.33\n13.86\n\n\n10\nEasy Listening\n13\n1.24\n12.87\n\n\n11\nReggae\n6\n0.57\n5.94\n\n\n12\nElectronica/Dance\n5\n0.48\n4.95\n\n\n13\nClassical\n4\n0.38\n3.96\n\n\n14\nHeavy Metal\n3\n0.29\n2.97\n\n\n15\nSoundtrack\n2\n0.19\n1.98\n\n\n16\nTV Shows\n1\n0.10\n0.99\n\n\n\n\n\n\n\nWe can make a bar graph from this result in order to communicate findings better.\n\n\nCode\n(\n    alt.Chart(usa_genre_df)\n    .mark_bar()\n    .encode(\n        x = alt.X(\"genre:N\", title = \"Music Genre\", sort = \"-y\"),\n        y = alt.Y(\"percentage_sold:Q\", title = \"Percentage of All Purchases in the USA\"),\n        color = alt.Color(\"total_sales:Q\", title = \"Total Sales (USD)\"),\n        tooltip = usa_genre_df.columns.tolist(),\n    )\n    .properties(\n        title = \"Popularity of Music Genres with Chinook's USA Customers\",\n        height = 300,\n        width = 600,\n    )\n    .configure_axis(\n        labelAngle = 30,\n    )\n    .interactive()\n)\n\n\n\n\n\n\n\nOne can hover over each bar in the chart above for a tooltip with more specific information.\nResults show that Rock is the best-selling music genre as it makes up 53% of total purchases in the USA. Rock is followed by the Alternative & Punk and Metal genres, which each make up over 10% of purchases.\nOn a side note, the total_sales column’s values are very close to that of the number_sold column. This can be explained by the fact that track prices range from USD 0.99 to USD 1.99.\n\n%%sql\nSELECT\n    MIN(unit_price) AS min_price,\n    MAX(unit_price) AS max_price\nFROM track\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\n\n\n\n\n\nmin_price\nmax_price\n\n\n\n\n0.99\n1.99\n\n\n\n\n\nSince there is little variation among track prices, the genre with the most units sold is usually also the genre with the highest sales.\nGoing back to the scenario at hand, we need to compare the statistics on the Hip-Hop, Punk, Pop, and Blues genres. We will run a query for this below.\n\n%%sql\nSELECT *\nFROM usa_genre_sales\nWHERE genre IN (\"Hip Hop/Rap\", \"Alternative & Punk\", \"Pop\", \"Blues\")\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\n\n\n\n\n\ngenre\nnumber_sold\npercentage_sold\ntotal_sales\n\n\n\n\nAlternative & Punk\n130\n12.37\n128.7\n\n\nBlues\n36\n3.43\n35.64\n\n\nPop\n22\n2.09\n21.78\n\n\nHip Hop/Rap\n20\n1.9\n19.8\n\n\n\n\n\nThe result above shows that Alternative & Punk, Blues, and Pop are the three best-selling genres out of the four. Notably, Alternative & Punk makes up 12.37% of all purchases in the USA.\nTherefore, we would recommend the Chinook administration to add the albums of Red Tone (Punk), Slim Jim Bites (Blues), and Meteor and the Girls (Pop) to the digital store. The album of Regal (Hip-Hop) has lower priority and can be added at a later date."
  },
  {
    "objectID": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#sales-support-agent-performance",
    "href": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#sales-support-agent-performance",
    "title": "Answering Business Questions for an Online Music Store using SQL",
    "section": "Sales Support Agent Performance",
    "text": "Sales Support Agent Performance\nNext, Chinook is requesting us to evaluate the performance of its sales support agents. Each customer is assigned to an agent after their first purchase. Since there are only 3 agents, each agent provides support to many customers. Details about the agents are shown in the query below.\n\n%%sql\nSELECT *\nFROM employee\nWHERE title = \"Sales Support Agent\"\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\n\n\n\n\n\nemployee_id\nlast_name\nfirst_name\ntitle\nreports_to\nbirthdate\nhire_date\naddress\ncity\nstate\ncountry\npostal_code\nphone\nfax\nemail\n\n\n\n\n3\nPeacock\nJane\nSales Support Agent\n2\n1973-08-29 00:00:00\n2017-04-01 00:00:00\n1111 6 Ave SW\nCalgary\nAB\nCanada\nT2P 5M5\n+1 (403) 262-3443\n+1 (403) 262-6712\njane@chinookcorp.com\n\n\n4\nPark\nMargaret\nSales Support Agent\n2\n1947-09-19 00:00:00\n2017-05-03 00:00:00\n683 10 Street SW\nCalgary\nAB\nCanada\nT2P 5G3\n+1 (403) 263-4423\n+1 (403) 263-4289\nmargaret@chinookcorp.com\n\n\n5\nJohnson\nSteve\nSales Support Agent\n2\n1965-03-03 00:00:00\n2017-10-17 00:00:00\n7727B 41 Ave\nCalgary\nAB\nCanada\nT3B 1Y7\n1 (780) 836-9987\n1 (780) 836-9543\nsteve@chinookcorp.com\n\n\n\n\n\nSince we have data on each customer’s purchases, we can calculate the total purchases associated with each support agent. We can then use this to compare the performance of the agents.\nFirst, we create a view called agent_customer by joining the employee table with the customer table based on the support representative ID. We will include some extra details about each agent, such as their birthdate and hire date. We won’t include their location since we know that they are all in Calgary, AB, Canada. As for the customers, we will include their customer ID, name, and total purchases as the sum of their invoices.\n\n%%sql\nDROP VIEW IF EXISTS agent_customer;\n\nCREATE VIEW agent_customer AS\n    SELECT\n        e.employee_id AS agent_id,\n        e.first_name || \" \" || e.last_name AS agent_name,\n        e.birthdate AS agent_bd,\n        e.hire_date AS agent_hire_date,\n        c.customer_id AS customer_id,\n        c.first_name || \" \" || c.last_name AS customer_name,\n        SUM(iv.total) AS customer_total_purchases\n    FROM employee AS e\n    LEFT JOIN\n        customer AS c\n        ON c.support_rep_id = e.employee_id\n    LEFT JOIN\n        invoice AS iv\n        ON iv.customer_id = c.customer_id\n    WHERE e.title = \"Sales Support Agent\"\n    GROUP BY c.customer_id\n    ORDER BY agent_id, customer_id\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\n\n\n[]\n\n\n\n%%sql result &lt;&lt;\nSELECT *\nFROM agent_customer\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nReturning data to local variable result\n\n\n\nagent_customer_df = result.DataFrame()\nagent_customer_df.head()\n\n\n\n\n\n\n\n\nagent_id\nagent_name\nagent_bd\nagent_hire_date\ncustomer_id\ncustomer_name\ncustomer_total_purchases\n\n\n\n\n0\n3\nJane Peacock\n1973-08-29 00:00:00\n2017-04-01 00:00:00\n1\nLuís Gonçalves\n108.90\n\n\n1\n3\nJane Peacock\n1973-08-29 00:00:00\n2017-04-01 00:00:00\n3\nFrançois Tremblay\n99.99\n\n\n2\n3\nJane Peacock\n1973-08-29 00:00:00\n2017-04-01 00:00:00\n12\nRoberto Almeida\n82.17\n\n\n3\n3\nJane Peacock\n1973-08-29 00:00:00\n2017-04-01 00:00:00\n15\nJennifer Peterson\n66.33\n\n\n4\n3\nJane Peacock\n1973-08-29 00:00:00\n2017-04-01 00:00:00\n18\nMichelle Brooks\n79.20\n\n\n\n\n\n\n\nNext, we will query this view in order to determine the following information about each agent:\n\nnumber of customers\ntotal sales from the agent’s customers\npercentage of all agents’ sales\naverage sales per customer\n\n\n%%sql\nDROP VIEW IF EXISTS agent_stats;\n\nCREATE VIEW agent_stats AS\n    SELECT\n        agent_id,\n        agent_name,\n        agent_bd,\n        agent_hire_date,\n        COUNT(customer_id) AS number_customers,\n        ROUND(\n            SUM(customer_total_purchases),\n            2\n        ) AS sales_number,\n        ROUND(\n            SUM(customer_total_purchases) \n            / (SELECT SUM(customer_total_purchases) FROM agent_customer)\n            * 100,\n            2\n        )AS sales_percentage,\n        ROUND(\n            AVG(customer_total_purchases),\n            2\n        ) AS average_per_customer\n    FROM agent_customer\n    GROUP BY agent_id\n;\n\nSELECT *\nFROM agent_stats\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\nagent_id\nagent_name\nagent_bd\nagent_hire_date\nnumber_customers\nsales_number\nsales_percentage\naverage_per_customer\n\n\n\n\n3\nJane Peacock\n1973-08-29 00:00:00\n2017-04-01 00:00:00\n21\n1731.51\n36.77\n82.45\n\n\n4\nMargaret Park\n1947-09-19 00:00:00\n2017-05-03 00:00:00\n20\n1584.0\n33.63\n79.2\n\n\n5\nSteve Johnson\n1965-03-03 00:00:00\n2017-10-17 00:00:00\n18\n1393.92\n29.6\n77.44\n\n\n\n\n\nAt first glance, Jane Peacock seems like the best-performing agent since Chinook got the highest total sales from her customers (USD 1731.51). This idea is cast into doubt when it is mentioned that she had the highest number of customers (21).\nAlso, the differences among the average sales per customer is quite small. Each of Jane’s customers spends 3 more dollars on Chinook than each of Margaret’s. Each of Margaret’s customers spends 2 more dollars on Chinook than each of Steve Johnson’s.\nThe average sales per customer may also be influenced by outliers, as shown by the boxplot below.\n\n\nCode\n(\n    alt.Chart(agent_customer_df)\n    .mark_boxplot()\n    .encode(\n        y = alt.Y(\"customer_total_purchases:Q\", title = \"Customer Total Purchases\"),\n        x = alt.X(\"agent_name:N\", title = \"Agent Name\"),\n    )\n    .properties(\n        title = \"Distribution of Customer Purchases by Sales Support Agent\",\n        height = 300,\n        width = 500,\n    )\n    .interactive()\n)\n\n\n\n\n\n\n\nIf we hover over each of the boxes above, we can get more information such as minimum, 1st quartile, median, etc. The median is less influenced by outliers, so we can look at that. The median values are 79.20 (Jane), 77.72 (Margaret), and 75.74 (Steve). These values are still very close to each other.\nTherefore, we cannot conclusively state that any agent performs better than the others."
  },
  {
    "objectID": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#sales-data-by-country",
    "href": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#sales-data-by-country",
    "title": "Answering Business Questions for an Online Music Store using SQL",
    "section": "Sales Data by Country",
    "text": "Sales Data by Country\nNext, Chinook is requesting us to analyze sales in each country where it offers its service. Specifically, they would like to know the:\n\ntotal number of customers\ntotal value of sales\naverage sales per customer\naverage order value\n\nEvery order is a batch purchase of multiple tracks.\n\n\nFurthermore, since there are some countries in the database with only one customer, we shall group these customers together under a category called “Other”, which must appear at the very bottom of our final result.\nFirst, we will create a view, country_invoices, which shows all invoices and the country of the customer.\n\n%%sql\nDROP VIEW IF EXISTS country_invoices;\n\nCREATE VIEW country_invoices AS\n    SELECT\n        c.country AS country,\n        c.customer_id AS customer_id,\n        c.first_name || \" \" || c.last_name AS customer_name,\n        iv.invoice_id AS invoice_id,\n        iv.total AS order_value\n    FROM customer AS c\n    LEFT JOIN\n        invoice AS iv\n        ON iv.customer_id = c.customer_id\n;\n\nSELECT *\nFROM country_invoices\nLIMIT 5\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ncountry\ncustomer_id\ncustomer_name\ninvoice_id\norder_value\n\n\n\n\nBrazil\n1\nLuís Gonçalves\n16\n8.91\n\n\nBrazil\n1\nLuís Gonçalves\n77\n5.9399999999999995\n\n\nBrazil\n1\nLuís Gonçalves\n149\n8.91\n\n\nBrazil\n1\nLuís Gonçalves\n153\n13.86\n\n\nBrazil\n1\nLuís Gonçalves\n182\n5.9399999999999995\n\n\n\n\n\nThen, we will create a view, country_customers, which shows the total purchases per customer.\n\n%%sql\nDROP VIEW IF EXISTS country_customers;\n\nCREATE VIEW country_customers AS\n    SELECT\n        country,\n        customer_id,\n        customer_name,\n        SUM(order_value) AS customer_total_purchase\n    FROM country_invoices\n    GROUP BY customer_id\n;\n\nSELECT *\nFROM country_customers\nLIMIT 5\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ncountry\ncustomer_id\ncustomer_name\ncustomer_total_purchase\n\n\n\n\nBrazil\n1\nLuís Gonçalves\n108.89999999999998\n\n\nGermany\n2\nLeonie Köhler\n82.17\n\n\nCanada\n3\nFrançois Tremblay\n99.99\n\n\nNorway\n4\nBjørn Hansen\n72.27000000000001\n\n\nCzech Republic\n5\nFrantišek Wichterlová\n144.54000000000002\n\n\n\n\n\nWe will then create a view called country_labels. It will show the number of customers per country. Countries with only 1 customer will be given a label of “Other”.\n\n%%sql\nDROP VIEW IF EXISTS country_labels;\n\nCREATE VIEW country_labels AS\n    SELECT\n        country,\n        COUNT(customer_id) AS number_customers,\n        CASE\n            WHEN COUNT(customer_id) &gt; 1 THEN country\n            ELSE \"Other\"\n            END AS country_label,\n        CASE\n            WHEN COUNT(customer_id) &gt; 1 THEN 0\n            ELSE 1\n            END AS is_other\n    FROM country_customers\n    GROUP BY country\n;\n\nSELECT *\nFROM country_labels\nLIMIT 10\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ncountry\nnumber_customers\ncountry_label\nis_other\n\n\n\n\nArgentina\n1\nOther\n1\n\n\nAustralia\n1\nOther\n1\n\n\nAustria\n1\nOther\n1\n\n\nBelgium\n1\nOther\n1\n\n\nBrazil\n5\nBrazil\n0\n\n\nCanada\n8\nCanada\n0\n\n\nChile\n1\nOther\n1\n\n\nCzech Republic\n2\nCzech Republic\n0\n\n\nDenmark\n1\nOther\n1\n\n\nFinland\n1\nOther\n1\n\n\n\n\n\nWe will also create a view called country_avg_order which simply shows the average order value per country. This will be done by querying the country_invoices view.\n\n%%sql\nDROP VIEW IF EXISTS country_avg_order;\n\nCREATE VIEW country_avg_order AS\n    SELECT\n        cl.country_label,\n        AVG(civ.order_value) AS avg_order_value\n    FROM country_invoices AS civ\n    INNER JOIN\n        country_labels AS cl\n        ON cl.country = civ.country\n    GROUP BY cl.country_label\n;\n\nSELECT *\nFROM country_avg_order\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ncountry_label\navg_order_value\n\n\n\n\nBrazil\n7.011147540983605\n\n\nCanada\n7.047236842105264\n\n\nCzech Republic\n9.107999999999999\n\n\nFrance\n7.781400000000001\n\n\nGermany\n8.161463414634145\n\n\nIndia\n8.72142857142857\n\n\nOther\n7.44857142857143\n\n\nPortugal\n6.3837931034482756\n\n\nUSA\n7.942671755725194\n\n\nUnited Kingdom\n8.768571428571429\n\n\n\n\n\nFinally, we will create a view called country_stats that shows all of the 4 statistics that were requested by the Chinook management. The “Other” entry will be forced to the bottom of the query result using the is_other column we created in the country_labels view.\n\n%%sql\nDROP VIEW IF EXISTS country_stats;\n\nCREATE VIEW country_stats AS\n    SELECT\n        l.country_label,\n        COUNT(c.customer_id) AS number_customers,\n        ROUND(SUM(c.customer_total_purchase), 2) AS total_sales,\n        ROUND(AVG(c.customer_total_purchase), 2) AS avg_sales_customer,\n        ROUND(a.avg_order_value, 2) AS avg_order_value\n    FROM country_customers AS c\n    INNER JOIN\n        country_labels AS l\n        ON l.country = c.country\n    INNER JOIN\n        country_avg_order AS a\n        ON a.country_label = l.country_label\n    GROUP BY l.country_label\n    ORDER BY\n        l.is_other ASC,\n        total_sales DESC\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\n\n\n[]\n\n\n\n%%sql result &lt;&lt;\nSELECT *\nFROM country_stats\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nReturning data to local variable result\n\n\n\ncountry_stats_df = result.DataFrame()\ncountry_stats_df\n\n\n\n\n\n\n\n\ncountry_label\nnumber_customers\ntotal_sales\navg_sales_customer\navg_order_value\n\n\n\n\n0\nUSA\n13\n1040.49\n80.04\n7.94\n\n\n1\nCanada\n8\n535.59\n66.95\n7.05\n\n\n2\nBrazil\n5\n427.68\n85.54\n7.01\n\n\n3\nFrance\n5\n389.07\n77.81\n7.78\n\n\n4\nGermany\n4\n334.62\n83.66\n8.16\n\n\n5\nCzech Republic\n2\n273.24\n136.62\n9.11\n\n\n6\nUnited Kingdom\n3\n245.52\n81.84\n8.77\n\n\n7\nPortugal\n2\n185.13\n92.57\n6.38\n\n\n8\nIndia\n2\n183.15\n91.58\n8.72\n\n\n9\nOther\n15\n1094.94\n73.00\n7.45\n\n\n\n\n\n\n\nWe can make a scatter plot with the data above in order to make results more clear.\n\n\nCode\n# Exclude the \"Other\" entry.\nmain_df = country_stats_df.loc[country_stats_df[\"country_label\"] != \"Other\"]\n\n# Base layer.\nbase = (\n    alt.Chart(main_df)\n    .mark_point(size = 300) # Set default size of points to 300 pixels.\n    .encode(\n        x = alt.X(\"number_customers:Q\", title = \"Number of Customers\"),\n        y = alt.Y(\"total_sales:Q\", title = \"Total Sales (USD)\"),\n    )\n)\n\n# Scatter plot layer.\npoints = base.encode(\n    color = alt.Color(\"avg_sales_customer:Q\", title = \"Average Sales per Customer (USD)\"),\n    tooltip = country_stats_df.columns.tolist(),\n)\n\n# Text layer.\ntext = (\n    base\n    .mark_text( # Move text to the top left of each point.\n        align = \"right\",\n        dy = -5,\n        dx = -20,\n    )\n    .encode(\n        text = \"country_label:N\"\n    )\n)\n\n# Combine layers.\nchart = (\n    (points + text)\n    .properties(\n        title = \"Chinook Sales by Country\",\n        height = 300,\n        width = 700,\n    )\n    .interactive()\n)\n\n# Display chart.\nchart\n\n\n\n\n\n\n\nLooking at the top right of the chart, we can see that the USA has the highest number of Chinook customers (13), as well as the highest total sales (USD 1040.49). All of the other countries have only 2 to 8 customers.\nOn the other hand, the Czech Republic has the highest average sales per customer at USD 136.62; this is indicated by its dark color in the chart. This country also had the highest average order value at USD 9.11. This means that though the country has few customers, these people are avid buyers of music.\nChinook may benefit from marketing its service more aggressively in countries other than the USA where it has gained a good foothold, such as Canada, Brazil, and France. Chinook may also target the Czech Republic since the customers there seem to buy a lot of music on a per-person basis."
  },
  {
    "objectID": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#comparing-purchase-types-full-album-vs-selected-sets",
    "href": "posts/2021-07-31-answering-business-questions-online-music-store-sql.html#comparing-purchase-types-full-album-vs-selected-sets",
    "title": "Answering Business Questions for an Online Music Store using SQL",
    "section": "Comparing Purchase Types: Full Album vs Selected Sets",
    "text": "Comparing Purchase Types: Full Album vs Selected Sets\nIn our last scenario, we have been requested to compare the popularities of Chinook’s two purchase types:\n\nFull album\n\nThe customer buys a full album.\nAlbums are pre-defined in Chinook’s library. The customer may not add other tracks on top of an album.\n\nSelected set of tracks\n\nThe customer manually selects any number of individual tracks.\n\n\nIn both cases, each track is bought at its unit price; there are no discounts.\nCurrently, Chinook’s purchasing strategy is to buy full albums from record companies. However, Chinook doesn’t know whether full album purchases are popular among its customers. If selected sets are more popular, then Chinook may switch to a new strategy in which it will only buy the most popular individual tracks from record companies.\nOur analysis will help Chinook make the final decision. For each purchase type, we will show the number of invoices and percentage of all invoices with that type. The type with the higher number will be the more popular one.\nFirst, we create a view, invoice_tracks, that shows all tracks under each invoice. The album associated with each track is also shown.\n\n%%sql\nDROP VIEW IF EXISTS invoice_tracks_bought;\n\nCREATE VIEW invoice_tracks_bought AS\n    SELECT\n        iv.invoice_id AS invoice_id,\n        il.track_id AS track_id,\n        t.name AS track_name,\n        t.album_id AS album_id,\n        a.title AS album_name\n    FROM invoice AS iv\n    INNER JOIN\n        invoice_line AS il\n        ON il.invoice_id = iv.invoice_id\n    INNER JOIN\n        track AS t\n        ON t.track_id = il.track_id\n    INNER JOIN\n        album AS a\n        ON a.album_id = t.album_id\n    ORDER BY invoice_id, track_id, album_id\n;\n\nSELECT *\nFROM invoice_tracks_bought\nLIMIT 7\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ninvoice_id\ntrack_id\ntrack_name\nalbum_id\nalbum_name\n\n\n\n\n1\n1158\nRight Next Door to Hell\n91\nUse Your Illusion I\n\n\n1\n1159\nDust N' Bones\n91\nUse Your Illusion I\n\n\n1\n1160\nLive and Let Die\n91\nUse Your Illusion I\n\n\n1\n1161\nDon't Cry (Original)\n91\nUse Your Illusion I\n\n\n1\n1162\nPerfect Crime\n91\nUse Your Illusion I\n\n\n1\n1163\nYou Ain't the First\n91\nUse Your Illusion I\n\n\n1\n1164\nBad Obsession\n91\nUse Your Illusion I\n\n\n\n\n\nThen, we create a view called invoice_album, which shows only the album associated with the first track of each invoice. This way, there is only one row per invoice. We will also include the total purchase amount of each invoice so that we can use it later.\n\n%%sql\nDROP VIEW IF EXISTS invoice_album;\n\nCREATE VIEW invoice_album AS \n    SELECT\n        itb.invoice_id AS invoice_id,\n        MIN(itb.album_id) AS album_id,\n        itb.album_name AS album_name,\n        iv.total AS total_purchase\n    FROM invoice_tracks_bought AS itb\n    INNER JOIN\n        invoice AS iv\n        ON iv.invoice_id = itb.invoice_id\n    GROUP BY itb.invoice_id\n    ORDER BY itb.invoice_id\n;\n\nSELECT *\nFROM invoice_album\nLIMIT 5\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ninvoice_id\nalbum_id\nalbum_name\ntotal_purchase\n\n\n\n\n1\n91\nUse Your Illusion I\n15.84\n\n\n2\n20\nThe Best Of Buddy Guy - The Millenium Collection\n9.9\n\n\n3\n203\nA-Sides\n1.98\n\n\n4\n58\nCome Taste The Band\n7.92\n\n\n5\n163\nFrom The Muddy Banks Of The Wishkah [live]\n16.83\n\n\n\n\n\nThen, we join invoice_album with the track table in order to list all of the tracks under the album associated with each invoice. The result will be a view called invoice_full_album_tracks.\n\n%%sql\nDROP VIEW IF EXISTS invoice_full_album_tracks;\n\nCREATE VIEW invoice_full_album_tracks AS\n    SELECT\n        ia.*,\n        t.track_id AS track_id,\n        t.name AS track_name\n    FROM invoice_album AS ia\n    INNER JOIN\n        track AS t\n        ON t.album_id = ia.album_id\n    ORDER BY invoice_id, album_id, track_id\n;\n\nSELECT *\nFROM invoice_full_album_tracks\nLIMIT 5\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ninvoice_id\nalbum_id\nalbum_name\ntotal_purchase\ntrack_id\ntrack_name\n\n\n\n\n1\n91\nUse Your Illusion I\n15.84\n1158\nRight Next Door to Hell\n\n\n1\n91\nUse Your Illusion I\n15.84\n1159\nDust N' Bones\n\n\n1\n91\nUse Your Illusion I\n15.84\n1160\nLive and Let Die\n\n\n1\n91\nUse Your Illusion I\n15.84\n1161\nDon't Cry (Original)\n\n\n1\n91\nUse Your Illusion I\n15.84\n1162\nPerfect Crime\n\n\n\n\n\nThe invoice_full_album_tracks view looks very similar to the invoice_tracks_bought view at first glance. However, there is a difference:\n\ninvoice_tracks_bought\n\nIt contains the tracks bought in each invoice.\nEach set of tracks may or may not be a full album.\n\ninvoice_full_album_tracks\n\nIt contains all of the tracks under one album associated with an invoice.\nSome of these tracks may not have been bought by the customer.\n\n\nNext, we will create a new view called invoice_purchase_type which indicates whether each invoice is a “Full Album” or “Selected Set” purchase.\nIn order to determine this, we will have a CASE statement which can be explained as follows:\n\nWHEN clause: If the set of tracks bought and the full album are exactly the same, mark the purchase type as “Full Album”.\nELSE clause: Otherwise, mark the purchase type as “Selected Set”.\n\nInside the WHEN clause, we have a rather complicated-looking set of operations. Let’s look at one part:\n(\n    SELECT itb.track_id\n    FROM invoice_tracks_bought AS itb\n    WHERE itb.invoice_id = ia.invoice_id\n\n    EXCEPT\n\n    SELECT ifa.track_id\n    FROM invoice_full_album_tracks AS ifa\n    WHERE ifa.invoice_id = ia.invoice_id\n) IS NULL\nIn order to make the explanation more simple, we can call the subqueries above “set 1” and “set 2”.\n(\n    {set 1}\n\n    EXCEPT\n\n    {set 2}\n) IS NULL\n\nSet 1 represents all tracks bought in one invoice.\nSet 2 represents the full set of tracks in an album associated with the invoice.\nVia EXCEPT and IS NULL, we check whether Set 1 is a subset of Set 2.\n\nIf it is, the result is True.\nOtherwise, False.\n\n\nWe then repeat the same process but in reverse, to check if Set 2 is a subset of Set 1. We thus end up with two boolean values, and we use the AND operator on these.\n(\n    {set 1}\n\n    EXCEPT\n\n    {set 2}\n) IS NULL\n\nAND\n\n(\n    {set 2}\n\n    EXCEPT\n\n    {set 1}\n) IS NULL\nThe purpose of AND is to determine the following.\n\nIf both conditions are True:\n\nThe two sets of tracks match exactly.\nThe invoice is a “Full Album” purchase.\n\nIf any condition is False:\n\nThe two sets of tracks do not match exactly.\nThe invoice is a “Selected Set” purchase.\n\n\nThe full query is shown below.\n\n%%sql\nDROP VIEW IF EXISTS invoice_purchase_type;\n\nCREATE VIEW invoice_purchase_type AS\n    SELECT\n        ia.invoice_id,\n        CASE\n            WHEN (\n\n                (\n                    SELECT itb.track_id\n                    FROM invoice_tracks_bought AS itb\n                    WHERE itb.invoice_id = ia.invoice_id\n\n                    EXCEPT\n\n                    SELECT ifa.track_id\n                    FROM invoice_full_album_tracks AS ifa\n                    WHERE ifa.invoice_id = ia.invoice_id\n                ) IS NULL\n\n                AND\n\n                (\n                    SELECT ifa.track_id\n                    FROM invoice_full_album_tracks AS ifa\n                    WHERE ifa.invoice_id = ia.invoice_id\n\n                    EXCEPT\n\n                    SELECT itb.track_id\n                    FROM invoice_tracks_bought AS itb\n                    WHERE itb.invoice_id = ia.invoice_id\n                ) IS NULL\n\n            ) THEN \"Full Album\"\n            ELSE \"Selected Set\"\n            END AS purchase_type,\n        ia.total_purchase AS total_purchase\n    FROM invoice_album AS ia\n;\n\nSELECT *\nFROM invoice_purchase_type\nLIMIT 10\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\nDone.\n\n\n\n\n\ninvoice_id\npurchase_type\ntotal_purchase\n\n\n\n\n1\nFull Album\n15.84\n\n\n2\nSelected Set\n9.9\n\n\n3\nSelected Set\n1.98\n\n\n4\nSelected Set\n7.92\n\n\n5\nFull Album\n16.83\n\n\n6\nSelected Set\n1.98\n\n\n7\nSelected Set\n10.89\n\n\n8\nSelected Set\n9.9\n\n\n9\nSelected Set\n8.91\n\n\n10\nSelected Set\n1.98\n\n\n\n\n\nWith the view above, we can finally answer the question being asked. A view called purchase_type_proportion will be created which shows the following information per purchase type:\n\nnumber of invoices\npercentage of invoices\naverage sales per invoice\nsales in USD\npercentage of total sales\n\n\n%%sql\nDROP VIEW IF EXISTS purchase_type_proportion;\n\nCREATE VIEW purchase_type_proportion AS\n    SELECT\n        purchase_type,\n        \n        COUNT(purchase_type) AS type_count,\n        \n        ROUND(\n            CAST(COUNT(purchase_type) AS Float)\n            / CAST(\n                (SELECT COUNT(purchase_type)\n                 FROM invoice_purchase_type)\n                AS Float\n            )\n            * 100,\n            2\n        ) AS type_percentage,\n        \n        ROUND(\n            AVG(total_purchase),\n            2\n        ) AS avg_sales_per_invoice,\n        \n        ROUND(\n            SUM(total_purchase),\n            2\n        ) AS sales_number,\n        \n        ROUND(\n            SUM(total_purchase)\n            / (SELECT SUM(total_purchase)\n               FROM invoice_purchase_type)\n            * 100,\n            2\n        ) AS sales_percentage\n        \n    FROM invoice_purchase_type\n    GROUP BY purchase_type\n    ORDER BY type_count DESC\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nDone.\n\n\n[]\n\n\n\n%%sql result &lt;&lt;\nSELECT *\nFROM purchase_type_proportion\n;\n\n * sqlite:///private/2021-07-31-Intermediate-SQL-Files/chinook.db\nDone.\nReturning data to local variable result\n\n\n\npurchase_type_df = result.DataFrame()\npurchase_type_df\n\n\n\n\n\n\n\n\npurchase_type\ntype_count\ntype_percentage\navg_sales_per_invoice\nsales_number\nsales_percentage\n\n\n\n\n0\nSelected Set\n500\n81.43\n6.50\n3248.19\n68.97\n\n\n1\nFull Album\n114\n18.57\n12.82\n1461.24\n31.03\n\n\n\n\n\n\n\nThe type_percentage column shows that the majority (81%) of all Chinook invoices are Selected Sets. Full Album purchases only make up 19% of all invoices.\nThis is shown in the pie chart below.\n\n\nCode\ncolors = sns.color_palette('pastel')[:2]\n\nplt.pie(\n    x = purchase_type_df[\"type_percentage\"],\n    labels = purchase_type_df[\"purchase_type\"],\n    colors = colors,\n    autopct = \"%0.2f%%\",\n)\n\nplt.title(\"Proportion of Chinook Store Invoices by Purchase Type\")\n\nplt.show()\n\n\n\n\n\nHowever, the average sales per Full Album purchase is almost twice that of a Selected Set purchase. This is shown in the bar chart below.\n\n\nCode\n# Base layer with bar chart\nbase = (\n    alt.Chart(purchase_type_df)\n    .mark_bar()\n    .encode(\n        x = alt.X(\"purchase_type:N\", title = \"Purchase Type\"),\n        y = alt.Y(\"avg_sales_per_invoice:Q\", title = \"Average Sales per Invoice (USD)\"),\n        tooltip = purchase_type_df.columns.tolist(),\n    )\n)\n\n# Text layer\ntext = (\n    base\n    .mark_text(\n        align = \"center\",\n        dy = 10,\n        color = \"white\",\n    )\n    .encode(\n        text = \"avg_sales_per_invoice:Q\",\n    )\n)\n\n# Combine layers into one chart\nchart = (\n    (base + text)\n    .properties(\n        title = \"Average Sales per Invoice by Purchase Type\",\n        height = 300,\n        width = 200,\n    )\n    .configure_axis(labelAngle = 30)\n    .interactive()\n)\n\n# Display chart.\nchart\n\n\n\n\n\n\n\nFull Albums cost more than Selected Sets because the former tend to have more tracks. Thus, even if Full Albums only represent 19% of all invoices, these also represent 31% of all dollar sales. This is shown in the pie chart below.\n\n\nCode\nplt.pie(\n    x = purchase_type_df[\"sales_percentage\"],\n    labels = purchase_type_df[\"purchase_type\"],\n    colors = colors,\n    autopct = \"%0.2f%%\",\n)\n\nplt.title(\"Proportion of Chinook Store Sales by Purchase Type\")\n\nplt.show()\n\n\n\n\n\nOverall, though, Selected Sets still represent the majority of Chinook’s invoices (81%) and sales (69%). Therefore, we recommend that Chinook shift to a new purchasing strategy in which it only buys the most popular tracks from record companies. Chinook should not buy full albums since it is less likely for customers to purchase these."
  },
  {
    "objectID": "posts/2022-05-10-Student-Mapping-Project.html",
    "href": "posts/2022-05-10-Student-Mapping-Project.html",
    "title": "ASHS Student Mapping Project: A Tool for Disaster Risk Reduction in the Online Set-up",
    "section": "",
    "text": "In senior high school, I was a member of the ASHS Programming Varsity (ProgVar). Sometime in Grade 12, the ProgVar officers and I were approached by the Office of the Assistant Principal for Administration with a request to develop an app that would help identify students affected by natural hazards, especially adverse weather conditions like typhoons. I rose to the challenge and proposed the Student Mapping Project. Throughout several months, I used my free time to set up a database of student locations and develop a web app. This app can take a list of affected areas and generate a list of students who live in those areas, as well as a map showing their geographic distribution.\nShown below is an example of a map generated by the Student Mapping App. Cities in Metro Manila were selected as the affected areas. Yellow areas indicate a higher number of students and purple areas indicate a lower number.\n\n\n\n\n\nToday, May 10, the app has reached its completion and it has been presented to the school principal. From here on, I’ll continue to maintain the app and turn it over to the next ProgVar officers. It will have its first official run next school year.\nThis excerpt from the documentation summarizes the important details.\n\nThe primary goal of the Student Mapping Project (SMP) was to identify students of the Ateneo de Manila Senior High School (ASHS) who live in areas affected by natural hazards, most commonly typhoons and earthquakes. This could help the school contact the affected students and determine whether they need assistance or monitoring.\nFor context, the school had shifted to an online set-up in the pandemic, so ASHS students attended classes from different parts of the country. In SY 2021-2022, out of a total of 1879 students, only 40.5% lived in Quezon City, where the school is based, whereas 31.1% lived elsewhere in Metro Manila, and 28.4% lived elsewhere in the Philippines. Therefore, there was a need for wider monitoring of hazards and automated identification of affected students.\nSeveral parties collaborated in the creation of this project. The main project proponent, Miguel Antonio H. Germar, was a member of the ASHS Programming Varsity (ProgVar). He developed the Student Mapping App (SMA) with the following objectives in mind: (1) to create a database where students’ locations are stored in a standardized format; (2) to develop a feature where the user may input the list of locations affected by a hazard; and (3) to develop a feature that automatically identifies the affected students and provides relevant statistics, maps, and tables.\nAnother involved party was Kanlaon, the student committee in charge of projects related to the environment and disaster risk reduction (DRR). The role of Kanlaon in the project was to use the SMA to identify students affected by hazards currently occurring in the country. They would then write reports to help the school conduct constituency checks, which involve contacting affected students to determine whether they require assistance.\nAlso involved in the project were the following members of the school administration: the Office of the Assistant Principal for Administration (OAPAdmin), the Student Services Coordinator (SSC), and the Information Technology (IT) department. They were regularly consulted in order to ensure that proper procedures were followed, especially with regard to the students’ data privacy.\n\nTo view the full documentation, use this link: Documentation for ASHS Student Mapping Project\nThis project used the Global Administrative Areas (GADM) database, which is free for non-commercial use.\n\nReferences\nUniversity of Berkeley, Museum of Vertebrate Zoology and the International Rice Research Institute. (2018, April). Global Administrative Areas (GADM) Version 3.4. GADM. https://gadm.org"
  },
  {
    "objectID": "posts/2022-07-10-Guide-Start-Learning-Data-Science.html",
    "href": "posts/2022-07-10-Guide-Start-Learning-Data-Science.html",
    "title": "📌 Guide to Start Learning Data Science",
    "section": "",
    "text": "This post is published via Google Docs.\nVisit this link: Guide to Start Learning Data Science - Google Docs"
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "",
    "text": "“Explanatory” visualizations are created in order to explain something to other people. These are made to inform general, non-technical audiences. Thus, we want to make a chart with the following characteristics:\n\nEye-catching\nVisually pleasing\nEasy to understand\nHas a clear main point, as opposed to being too detailed\n\nPut simply, it is similar to an infographic. However, instead of being a standalone image, it is usually put in an article with accompanying text.\nIn this project, I detail the process of designing an explanatory chart of USD-PHP exchange rates. I used the “Forex data since 2011-1-1” dataset, which was uploaded by user emrecanaltinsoy on Kaggle. By the end of the project, I was able to make the following chart.\n\n\n\nA chart about USD-PHP exchange rates over time.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe larger implications of exchange rate trends are beyond the scope of this project. For Filipinos, I suggest reading “[ANALYSIS] Why the stronger peso mirrors a weaker PH economy” (Punongbayan 2020), which is an interesting recent article.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI wrote this notebook for the Dataquest course’s Guided Project: Storytelling Data Visualization on Exchange Rates. However, all of the text and code here are written by me unless stated otherwise."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#column-labels",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#column-labels",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Column Labels",
    "text": "Column Labels\nThe column labels will first be changed so that these are easier to use.\n\nphp.columns = [\"date\", \"usd_php\"]\nphp.head()\n\n\n\n\n\n\n\n\ndate\nusd_php\n\n\n\n\n0\n2011-01-01\n43.639999\n\n\n1\n2011-01-02\n43.639999\n\n\n2\n2011-01-03\n43.799999\n\n\n3\n2011-01-04\n43.550002\n\n\n4\n2011-01-05\n43.900002"
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#date-column",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#date-column",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Date Column",
    "text": "Date Column\nThe date column contains text with the format {4 digit year}-{2 digit month}-{2 digit day}.\nBelow, I convert the text to datetime objects for ease of use.\n\nphp[\"date\"] = pd.to_datetime(php[\"date\"])\nphp.sort_values(\n    by = \"date\",\n    ascending = True,\n    inplace = True,\n)\n    \nphp[\"date\"]\n\n0      2011-01-01\n1      2011-01-02\n2      2011-01-03\n3      2011-01-04\n4      2011-01-05\n          ...    \n3793   2021-05-21\n3794   2021-05-22\n3795   2021-05-23\n3796   2021-05-24\n3797   2021-05-25\nName: date, Length: 3798, dtype: datetime64[ns]"
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#descriptive-statistics",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#descriptive-statistics",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nBefore we can clean the data, we have to view its descriptive statistics.\n\nphp.describe(datetime_is_numeric = True)\n\n\n\n\n\n\n\n\ndate\nusd_php\n\n\n\n\ncount\n3798\n3798.000000\n\n\nmean\n2016-03-13 12:00:00\n47.010148\n\n\nmin\n2011-01-01 00:00:00\n3.094050\n\n\n25%\n2013-08-07 06:00:00\n43.639330\n\n\n50%\n2016-03-13 12:00:00\n46.890740\n\n\n75%\n2018-10-18 18:00:00\n50.584238\n\n\nmax\n2021-05-25 00:00:00\n54.323583\n\n\nstd\nNaN\n3.920411\n\n\n\n\n\n\n\nAs expected, the dates range from November 1, 2011 to May 25, 2021.\nHowever, the minimum USD-PHP exchange rate in the data is 3.09. This is very low compared to the other percentiles."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#exchange-rate-outliers",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#exchange-rate-outliers",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Exchange Rate Outliers",
    "text": "Exchange Rate Outliers\nThere may be some outliers in the data. We can confirm this using a boxplot.\n\nsns.boxplot(\n    data = php,\n    y = \"usd_php\"\n)\n\nplt.title(\"USD-PHP Exchange Rate Distribution\")\nplt.ylabel(\"USD-PHP Exchange Rate\")\nplt.grid(True)\nplt.show()\n\n\n\n\nIndeed, most of the values fall between 40 and 55, whereas a few outliers exist below 10. These may be inaccurate data.\nHow many outliers are there?\n\n(php[\"usd_php\"]\n .value_counts(bins = 10)\n .sort_index()\n)\n\n(3.0420000000000003, 8.217]       2\n(8.217, 13.34]                    0\n(13.34, 18.463]                   0\n(18.463, 23.586]                  0\n(23.586, 28.709]                  0\n(28.709, 33.832]                  0\n(33.832, 38.955]                  0\n(38.955, 44.078]               1157\n(44.078, 49.201]               1289\n(49.201, 54.324]               1350\nName: usd_php, dtype: int64\n\n\nThere are only 2 values less than 10. It is highly unlikely that these values are accurate. This can be shown using a line chart.\n\nsns.lineplot(\n    data = php,\n    x = \"date\",\n    y = \"usd_php\",\n)\n\nplt.title(\"USD-PHP Exchange Rate Over Time\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"USD-PHP Exchange Rate\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe outliers exist somewhere in the 2014 data, and they create an unnatural dip in the chart.\nThus, the inaccurate datapoints will be dropped from the dataset.\n\nphp = php.loc[php[\"usd_php\"] &gt; 10]\n\nphp.describe(datetime_is_numeric = True)\n\n\n\n\n\n\n\n\ndate\nusd_php\n\n\n\n\ncount\n3796\n3796.000000\n\n\nmean\n2016-03-13 22:32:45.015806208\n47.033285\n\n\nmin\n2011-01-01 00:00:00\n40.500000\n\n\n25%\n2013-08-06 18:00:00\n43.639981\n\n\n50%\n2016-03-14 12:00:00\n46.892504\n\n\n75%\n2018-10-19 06:00:00\n50.584680\n\n\nmax\n2021-05-25 00:00:00\n54.323583\n\n\nstd\nNaN\n3.789566\n\n\n\n\n\n\n\nThe minimum USD-PHP rate is now 40.5, which makes more sense."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#basic-line-chart",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#basic-line-chart",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Basic Line Chart",
    "text": "Basic Line Chart\nFirst, we start with a basic line chart that shows the exchange rates on all days in the dataset.\n\nsns.lineplot(\n    data = php,\n    x = \"date\",\n    y = \"usd_php\",\n)\n\nplt.title(\"USD-PHP Exchange Rate Over Time\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"USD-PHP Exchange Rate\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe chart shows that the exchange rate dropped to under 41.0 in 2013. It then steadily climbed up to over 54.0 in 2018 before moving down again afterwards.\nFor Filipinos, it is ideal for the USD-PHP rate to be lower so that the peso has more power. Thus, it can be said that the exchange rate was better from 2011 to 2015 compared to how it has been in recent years. However, note that the USD-PHP exchange rate is not the only descriptor of the Philippines’ economy."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#rolling-average",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#rolling-average",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Rolling Average",
    "text": "Rolling Average\nIn order to focus more on general trends than small fluctuations, we can graph the rolling average (or moving average). The rolling average is taken by replacing each datapoint with the mean average of a certain number of the datapoints leading up to it.\nUsing a rolling average can make a graph look visually cleaner and make general trends easier to see.\nThe number of datapoints used in each average is called the rolling window. This can be specified in Pandas using pd.Series.rolling(). Below, we use a rolling window of 182 days (around half a year) in order to transform the rate data.\n\nphp[\"rolling\"] = php[\"usd_php\"].rolling(182).mean()\nphp.tail()\n\n\n\n\n\n\n\n\ndate\nusd_php\nrolling\n\n\n\n\n3793\n2021-05-21\n47.919983\n48.237840\n\n\n3794\n2021-05-22\n47.929517\n48.236031\n\n\n3795\n2021-05-23\n47.929517\n48.234226\n\n\n3796\n2021-05-24\n48.068599\n48.232923\n\n\n3797\n2021-05-25\n48.155798\n48.232809\n\n\n\n\n\n\n\nThe last 5 rows of the dataset are shown above. The rolling averages are not exactly equal to the original numbers, but these are close enough to show the same trend.\nA line chart of the rolling averages is shown below.\n\nsns.lineplot(\n    data = php,\n    x = \"date\",\n    y = \"rolling\",\n)\n\nplt.title(\"USD-PHP Exchange Rate Over Time: Rolling Average\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"USD-PHP Exchange Rate\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe same trends mentioned earlier are clearer to see in the above chart."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#key-concepts",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#key-concepts",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Key Concepts",
    "text": "Key Concepts\nThe following concepts will be used throughout the process of designing the chart. I learned these from Dataquest’s “Data Scientist in Python” course.\n\nFamiliarity\nAudiences prefer familiar charts since they can understand these easily. Therefore, it is better to use a basic chart as a template than to use something obscure or create something entirely new.\nIn the case of showing USD-PHP exchange rates over time, it is best to use a basic line chart as a template.\n\n\nData-Ink Ratio\nWhen making an explanatory chart, one must maximize the data-ink ratio.\n\nData refers to the elements that represent data and its relationships, like bars and lines.\nInk refers to the total amount of ink that the chart would use if it were printed on paper.\n\nMaximizing the data-ink ratio means focusing more on data-related elements and minimizing the use of other, less important elements.\nThis helps the audience understand the main point without being distracted by other details.\n\n\nGestalt Psychology\nGestalt psychology is founded on the idea that people tend to see patterns rather than individual objects.\nUnder Gestalt psychology, there are several Principles of Grouping. These are ways to visually group elements together.\n\nProximity: Elements are close to each other\nSimilarity: Elements are similar due to color, shape, etc.\nEnclosure: Elements are enclosed in an outer shape, like a rectangle\nConnection: Elements are connected by a form, usually a line\n\nWhen designing charts, these are helpful in implying relationships between elements instead of explicitly stating them."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#visual-style",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#visual-style",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Visual Style",
    "text": "Visual Style\nBefore we start making the chart, we have to choose a style.\nI chose Matplotlib’s built-in “fivethirtyeight” style. It’s based on the style of the charts used on the FiveThirtyEight website by Nate Silver.\nAdditionally, I used color-hex.com to get hex codes for specific kinds of blue, orange, and dark gray that I want to use in my chart.\n\n# Use the FiveThirtyEight style of charts.\nstyle.use(\"fivethirtyeight\")\n\n# Color hex codes\nc_blue = \"#14c4dd\"\nc_orange = \"#ffa500\"\nc_dark_gray = \"#d2d2d2\""
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#setting-up-subplots",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#setting-up-subplots",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Setting up Subplots",
    "text": "Setting up Subplots\nIn order to make the chart fresh and interesting, we have to make it more complex than 1 plot with a line chart. In our case, I have the following ideas:\n\nShow 2 line charts, one on top of the other.\nThe upper chart shows how the rate changed from 2011 to 2021.\nThe lower chart zooms into the pandemic portion of the line chart from 2020 to 2021.\n\nIn order to do this, I will create a Matplotlib Figure with two Axes (subplots), as described above.\n\n# Figure has 2 rows and 1 column.\n# There is an upper and lower subplot.\nfig, (ax1, ax2) = plt.subplots(\n    nrows = 2,\n    ncols = 1,\n    figsize = (10, 10), # 10 inches x 10 inches\n    dpi = 80,\n)"
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#the-upper-subplot",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#the-upper-subplot",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "The Upper Subplot",
    "text": "The Upper Subplot\nNext, we design the upper subplot. Here’s what it looks like with a basic line chart of the raw exchange rate data.\n\nax1.plot(\n    php[\"date\"],\n    php[\"usd_php\"],\n    color = c_blue,\n)\n\nfig\n\n\n\n\nThe line above looks messy. For the general audience, the overall trends are more important than the specific daily values. Thus, we will use the rolling average to make the chart cleaner.\n\nax1.clear()\n\nax1.plot(\n    php[\"date\"],\n    php[\"rolling\"],\n    color = c_blue,\n)\n\nfig\n\n\n\n\nAdditionally, we will split the line into:\n\nThe pre-pandemic portion (blue)\nThe pandemic portion (orange)\n\nThe pandemic portion will also be enclosed in a dark gray box, in order to further separate it from the pre-pandemic portion.\n\nax1.clear()\n\n# Main line chart (2011-2019)\nphp_pre = php.loc[php[\"date\"].dt.year &lt; 2020]\nax1.plot(\n    php_pre[\"date\"],\n    php_pre[\"rolling\"],\n    color = c_blue,\n)\n\n# Pandemic part of line chart (2020-2021)\nphp_pandemic = php.loc[php[\"date\"].dt.year.between(2020, 2021)]\nax1.plot(\n    php_pandemic[\"date\"],\n    php_pandemic[\"rolling\"],\n    color = c_orange,\n)\n\n# Special background for pandemic portion\nax1.axvspan(\n    \"2020-01\",\n    \"2021-05-25\",\n    facecolor = c_dark_gray,\n    alpha = 1,\n)\n\nfig\n\n\n\n\nNext, we want to maximize the data-ink ratio by removing unnecessary elements. We will do the following:\n\nOn the x-axis, show only the labels for 2012, 2016, and 2020.\nOn the y-axis, show only the labels for 48, 50, and 52.\nRemove grid lines.\n\nWe will also add 1 grid line at y = 50 so that it can guide viewers. It would be particularly helpful for Filipino viewers since they commonly think that USD 1 = PHP 50.\n\n# Set tick label color to gray.\nax1.tick_params(axis = 'both', colors = 'gray')\n\n# Specific tick labels\nax1.set_xticks([\"2012\", \"2016\", \"2020\"])\nax1.set_xticklabels([\"2012\", \"2016\", \"2020\"])\n\nax1.set_yticks([48, 50, 52])\nax1.set_yticklabels([\"48\", \"50\", \"52\"])\n\n# Remove grid\nax1.grid(False)\n\n# Horizontal line at y = 50\nax1.axhline(50, linewidth = 1, color = \"gray\")\n\nfig\n\n\n\n\nNow, the upper subplot is much cleaner; there is less visual noise.\nThe last step for the upper subplot would be to add informative text:\n\n“Pre-Pandemic” label for the blue line\nA comment about the upward trend leading up to 2018\n\n\n# \"Pre-Pandemic\" label\nax1.text(\n    x = dt.date(year = 2013, month = 7, day = 1),\n    y = 48,\n    s = \"Pre-Pandemic\",\n    color = \"gray\",\n    size = 14,\n    ha = \"center\",\n    va = \"center\",\n)\n\n# Comment on upward trend\nax1.text(\n    x = dt.date(year = 2018, month = 10, day = 1),\n    y = 46,\n    s = \"Rate climbs up to 54\\nin Oct 2018\",\n    color = \"black\",\n    size = 11,\n    ha = \"center\",\n    va = \"center\",\n)\n\nfig\n\n\n\n\nThe “Pre-Pandemic” label was set to gray because it is a structural element; it is less important.\nOn the other hand, the long comment was set to black because it states a statistic from the data and helps tell a story about the data. It is more important, so it should be darker.\nWe have finished designing the upper subplot."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#lower-subplot",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#lower-subplot",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Lower Subplot",
    "text": "Lower Subplot\nNext, the lower subplot will zoom in on the pandemic portion of the data, which is from 2020 to 2021. Since this is the more important part, we should use the raw data for more detail.\n\n# Pandemic portion, zoomed in\nax2.plot(\n    php_pandemic[\"date\"],\n    php_pandemic[\"usd_php\"],\n    color = c_orange,\n)\n\nfig\n\n\n\n\nThen, we will make changes similar to the ones done for the upper subplot:\n\nOn the x-axis, show only the labels for January 2020, July 2020, and January 2021.\nOn the y-axis, show only the labels for 48, 50, and 52.\nRemove grid lines.\nAdd 1 grid line at y = 50.\nEnclose the entire line in a dark gray box.\n\n\n# Set tick labels to gray.\nax2.tick_params(axis = 'both', colors = 'gray')\n\n# Specific tick labels.\nax2.set_xticks([\"2020-01\", \"2020-07\", \"2021-01\"])\nax2.set_xticklabels(['Jan 2020', 'Jul 2020', 'Jan 2021'])\n\nax2.set_yticks([48, 50, 52])\nax2.set_yticklabels([\"48\", \"50\", \"52\"])\n\n# Remove grid\nax2.grid(False)\n\n# Horizontal line at y = 50\nax2.axhline(50, linewidth = 1, color = \"gray\")\n\n# Special background for pandemic portion\nax2.axvspan(\n    \"2020-01\",\n    \"2021-05-25\",\n    facecolor = c_dark_gray,\n    alpha = 1,\n)\n\nfig\n\n\n\n\nWe are using the principles of enclosure and similarity to visually imply that the pandemic portion in the upper subplot is being shown more closely in the lower subplot.\n\nEnclosure: dark gray boxes\nSimilarity: orange lines, horizontal grid line, y-axis labels\n\nNext, we add another comment in black text, this time about the downward trend leading up to the present day.\n\n# Comment on downward trend\nax2.text(\n    x = dt.date(year = 2021, month = 1, day = 1),\n    y = 49.25,\n    s = \"Rate drops down to 48\\nin May 2021\",\n    color = \"black\",\n    size = 11,\n    ha = \"center\",\n    va = \"center\",\n)\n\nfig\n\n\n\n\nWe have finished designing the lower subplot. However, we still need to add some finishing touches."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#figure-level-customization",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#figure-level-customization",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Figure-Level Customization",
    "text": "Figure-Level Customization\nIn this last step, we customize the chart on the level of the Matplotlib Figure. This involves both of the subplots and the space around them.\nWhat we want to do is to use the principles of proximity and connection to make the relationship between the 2 subplots even clearer.\n\nProximity: Increase the space between the two subplots.\nConnection: Draw a line connecting the two dark gray boxes.\n\nAdd a “COVID-19 Pandemic” label next to the line.\n\n\nThese are done in the code below.\n\n# Add space between the subplots\nfig.tight_layout(pad = 5)\n\n# Line connection between gray boxes\nfig.add_artist( \n    lines.Line2D(\n        xdata = [0.82, 0.82],\n        ydata = [0.44, 0.585],\n        color = c_dark_gray,\n        alpha = 1,\n    )\n)\n\n# \"COVID-19\" label between subplots\nfig.text(\n    x = 0.8,\n    y = 0.5,\n    s = \"COVID-19 Pandemic\",\n    color = \"gray\",\n    size = 14,\n    ha = \"right\",\n    va = \"center\",\n)\n\nfig\n\n\n\n\nNow, when one first reads the chart, it is very clear that the gray boxes contain data about exchange rates in the COVID-19 pandemic.\nThe last touch would be to add a title and subtitle to the chart. Since the title is typically the first thing a viewer reads on a chart, it is best to state a statistic related to the data, like “USD-PHP Rate Drops to 48 after 1 Year in the Pandemic.”\nThis way, the title becomes a data element. The data-ink ratio is increased.\n\n# Title with a statistic\nfig.text(\n    x = 0.5,\n    y = 0.95,\n    s = \"USD-PHP Rate Drops to 48 after 1 Year in the Pandemic\",\n    size = 16,\n    weight = \"bold\",\n    ha = \"center\",\n    va = \"center\",\n)\n\n# Subtitle\nfig.text(\n    x = 0.5,\n    y = 0.92,\n    s = \"USD-PHP exchange rate over time\",\n    size = 12,\n    ha = \"center\",\n    va = \"center\",\n)\n\nfig\n\n\n\n\nThat’s it. The explanatory chart is complete."
  },
  {
    "objectID": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#full-code",
    "href": "posts/2021-05-28-making-explanatory-chart-usd-php-exchange-rates.html#full-code",
    "title": "Making an Explanatory Chart of USD-PHP Exchange Rates",
    "section": "Full Code",
    "text": "Full Code\nThe full code to make the graph is shown below. The comments explain which part does what.\n\n# Use the FiveThirtyEight style of charts.\nstyle.use(\"fivethirtyeight\")\n\n# Color hex codes\nc_blue = \"#14c4dd\"\nc_orange = \"#ffa500\"\nc_dark_gray = \"#d2d2d2\"\n\n# Figure has 2 rows and 1 column.\n# There is an upper and lower subplot.\nfig, (ax1, ax2) = plt.subplots(\n    nrows = 2,\n    ncols = 1,\n    figsize = (10, 10),\n    dpi = 80,\n)\n\n\n\n# ---UPPER subplot\n\n# Main line chart (2011-2019)\nphp_pre = php.loc[php[\"date\"].dt.year &lt; 2020]\nax1.plot(\n    php_pre[\"date\"],\n    php_pre[\"rolling\"],\n    color = c_blue,\n)\n\n# Pandemic part of line chart (2020-2021)\nphp_pandemic = php.loc[php[\"date\"].dt.year.between(2020, 2021)]\nax1.plot(\n    php_pandemic[\"date\"],\n    php_pandemic[\"rolling\"],\n    color = c_orange,\n)\n\n# Special background for pandemic portion\nax1.axvspan(\n    \"2020-01\",\n    \"2021-05-25\",\n    facecolor = c_dark_gray,\n    alpha = 1,\n)\n\n# Set tick label color to gray.\nax1.tick_params(axis = 'both', colors = 'gray')\n\n# Specific tick labels\nax1.set_xticks([\"2012\", \"2016\", \"2020\"])\nax1.set_xticklabels([\"2012\", \"2016\", \"2020\"])\n\nax1.set_yticks([48, 50, 52])\nax1.set_yticklabels([\"48\", \"50\", \"52\"])\n\n# Remove grid\nax1.grid(False)\n\n# Horizontal line at y = 50\nax1.axhline(50, linewidth = 1, color = \"gray\")\n\n# Text in upper subplot\nax1.text(\n    x = dt.date(year = 2018, month = 10, day = 1),\n    y = 46,\n    s = \"Rate climbs up to 54\\nin Oct 2018\",\n    color = \"black\",\n    size = 11,\n    ha = \"center\",\n    va = \"center\",\n)\nax1.text(\n    x = dt.date(year = 2013, month = 7, day = 1),\n    y = 48,\n    s = \"Pre-Pandemic\",\n    color = \"gray\",\n    size = 14,\n    ha = \"center\",\n    va = \"center\",\n)\n\n\n\n# ---LOWER subplot\n\n# Pandemic portion, zoomed in\nax2.plot(\n    php_pandemic[\"date\"],\n    php_pandemic[\"usd_php\"],\n    color = c_orange,\n)\n\n# Set tick labels to gray.\nax2.tick_params(axis = 'both', colors = 'gray')\n\n# Specific tick labels.\nax2.set_xticks([\"2020-01\", \"2020-07\", \"2021-01\"])\nax2.set_xticklabels(['Jan 2020', 'Jul 2020', 'Jan 2021'])\n\nax2.set_yticks([48, 50, 52])\nax2.set_yticklabels([\"48\", \"50\", \"52\"])\n\n# Remove grid\nax2.grid(False)\n\n# Horizontal line at y = 50\nax2.axhline(50, linewidth = 1, color = \"gray\")\n\n# Special background for pandemic portion\nax2.axvspan(\n    \"2020-01\",\n    \"2021-05-25\",\n    facecolor = c_dark_gray,\n    alpha = 1,\n)\n\n# Text in lower subplot\nax2.text(\n    x = dt.date(year = 2021, month = 1, day = 1),\n    y = 49.25,\n    s = \"Rate drops down to 48\\nin May 2021\",\n    color = \"black\",\n    size = 11,\n    ha = \"center\",\n    va = \"center\",\n)\n\n\n\n\n# ---FIGURE level customization\n# Add space between the subplots\nfig.tight_layout(pad = 5)\n\n# Line connection between pandemic parts\nfig.add_artist( \n    lines.Line2D(\n        xdata = [0.82, 0.82],\n        ydata = [0.44, 0.585],\n        color = c_dark_gray,\n        alpha = 1,\n    )\n)\n\n# Title with statistic\nfig.text(\n    x = 0.5,\n    y = 0.95,\n    s = \"USD-PHP Rate Drops to 48 after 1 Year in the Pandemic\",\n    size = 16,\n    weight = \"bold\",\n    ha = \"center\",\n    va = \"center\",\n)\n\n# Subtitle\nfig.text(\n    x = 0.5,\n    y = 0.92,\n    s = \"USD-PHP exchange rate over time\",\n    size = 12,\n    ha = \"center\",\n    va = \"center\",\n)\n\n# \"COVID-19\" label between subplots\nfig.text(\n    x = 0.8,\n    y = 0.5,\n    s = \"COVID-19 Pandemic\",\n    color = \"gray\",\n    size = 14,\n    ha = \"right\",\n    va = \"center\",\n)\n\n\n\n# Save chart locally.\nplt.savefig(\"./private/2021-05-28-MEC-Files/2021-05-28-explanatory-chart.png\")\n\n# Show chart\nplt.show()"
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#list-of-datasets",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#list-of-datasets",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "List of Datasets",
    "text": "List of Datasets\nThe following are the datasets that were downloaded from NYC Open Data for this project. The files were renamed to have shorter names.\n\nsat_results.csv: 2012 SAT results (link)\nhs_directory: 2010 - 2011 School Attendance and Enrollment Statistics by District (link)\nclass_size.csv: 2010-2011 Class Size - School-level detail (link)\nap_2010.csv: 2010 AP (College Board) School Level Results (link)\ngraduation.csv: 2005-2010 Graduation Outcomes - School Level (link)\ndemographics.csv: 2006 - 2012 School Demographics and Accountability Snapshot (link)\nsurvey_all.txt, survey_d75.txt: 2011 NYC School Survey (link)\n\nThe code below reads the CSVs into the program.\n\ndata_files = [\n    \"ap_2010.csv\",\n    \"class_size.csv\",\n    \"demographics.csv\",\n    \"graduation.csv\",\n    \"hs_directory.csv\",\n    \"sat_results.csv\"\n]\n\ndata = {}\n\nfor filename in data_files:\n    key = filename[:-4] # Remove \".csv\"\n    df = pd.read_csv(\"./private/2021-06-13-HSD-Files/\" + filename)\n    \n    # Rename \"dbn\" to \"DBN\".\n    df = df.rename(\n        {\"dbn\": \"DBN\"},\n        axis = 1,\n    )\n    \n    # Assign the DataFrame to a key in the dictionary.\n    data[key] = df\n\nAs for the TXT files, these are tab-delimited and encoded using Windows-1252 encoding, so we have to specify that when we read them in. We will also concatenate these two datasets vertically because these have the same column names.\n\n# Survey data on all community schools.\nall_survey = pd.read_csv(\n    \"./private/2021-06-13-HSD-Files/survey_all.txt\",\n    delimiter = \"\\t\",\n    encoding = \"windows-1252\",\n)\n\n# Survey data on all District 75 schools.\nd75_survey = pd.read_csv(\n    \"./private/2021-06-13-HSD-Files/survey_d75.txt\",\n    delimiter = \"\\t\",\n    encoding = \"windows-1252\",\n)\n\n# Concatenate the two datasets vertically into one dataset.\nsurvey = pd.concat(\n    [all_survey, d75_survey],\n    axis = 0,\n)\n\n# Rename \"dbn\" column to \"DBN\" for consistency.\nsurvey = survey.rename(\n    {\"dbn\": \"DBN\"},\n    axis = 1,\n)\n\n# Put the DataFrame into the main dictionary.\ndata[\"survey\"] = survey\n\nNow, all of the datasets can be accessed via the data Series.\n\ndata = pd.Series(data)\n\nlist(data.index)\n\n['ap_2010',\n 'class_size',\n 'demographics',\n 'graduation',\n 'hs_directory',\n 'sat_results',\n 'survey']\n\n\nTo give an idea of what these datasets look like, we show the first few rows of the SAT results dataset below.\n\nEach row contains data on one school.\nEach column provides a different detail about the schools, such as school name, number of test takers in each school, etc.\n\n\ndata.sat_results.head()\n\n\n\n\n\n\n\n\nDBN\nSCHOOL NAME\nNum of SAT Test Takers\nSAT Critical Reading Avg. Score\nSAT Math Avg. Score\nSAT Writing Avg. Score\n\n\n\n\n0\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL STUDIES\n29\n355\n404\n363\n\n\n1\n01M448\nUNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n91\n383\n423\n366\n\n\n2\n01M450\nEAST SIDE COMMUNITY SCHOOL\n70\n377\n402\n370\n\n\n3\n01M458\nFORSYTH SATELLITE ACADEMY\n7\n414\n401\n359\n\n\n4\n01M509\nMARTA VALLE HIGH SCHOOL\n44\n390\n433\n384"
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#sat-total-scores",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#sat-total-scores",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "SAT Total Scores",
    "text": "SAT Total Scores\nThe SAT results dataset contains columns showing the average score in each section of the test.\nHowever, there is no column representing the average SAT total score. This is important to know because it gauges a student’s overall mastery of high school knowledge and college preparedness.\nThus, we will generate a total score column by taking the sum of the three section score columns.\n\nscore_cols = [\n    \"SAT Math Avg. Score\",\n    \"SAT Critical Reading Avg. Score\",\n    \"SAT Writing Avg. Score\",\n]\n\n# Convert each column from string to numeric.\nfor col in score_cols:\n    data.sat_results[col] = pd.to_numeric(\n        data.sat_results[col],\n        errors = \"coerce\", # Set invalid parsing as NaN.\n    )\n\n# Get the total scores by summing the 3 section scores.\ndata.sat_results[\"sat_score\"] = (\n    data.sat_results[score_cols]\n    .sum(\n        axis = 1,\n        skipna = False, # Return NaN if the row has a NaN.\n    )\n)\n\n# Drop rows with missing values.\ndata.sat_results.dropna(inplace = True)\n\ndata.sat_results[score_cols + [\"sat_score\"]].head()\n\n\n\n\n\n\n\n\nSAT Math Avg. Score\nSAT Critical Reading Avg. Score\nSAT Writing Avg. Score\nsat_score\n\n\n\n\n0\n404.0\n355.0\n363.0\n1122.0\n\n\n1\n423.0\n383.0\n366.0\n1172.0\n\n\n2\n402.0\n377.0\n370.0\n1149.0\n\n\n3\n401.0\n414.0\n359.0\n1174.0\n\n\n4\n433.0\n390.0\n384.0\n1207.0\n\n\n\n\n\n\n\nThe sat_score column now shows each school’s average SAT total score."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#school-location",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#school-location",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "School Location",
    "text": "School Location\nAnother piece of information we want to find is the set of geographical coordinates of each NYC school. We can find this using the hs_directory dataset, which has a Location 1 column.\n\ndata.hs_directory[[\"DBN\", \"school_name\", \"Location 1\"]].head()\n\n\n\n\n\n\n\n\nDBN\nschool_name\nLocation 1\n\n\n\n\n0\n17K548\nBrooklyn School for Music & Theatre\n883 Classon Avenue\\nBrooklyn, NY 11225\\n(40.67...\n\n\n1\n09X543\nHigh School for Violin and Dance\n1110 Boston Road\\nBronx, NY 10456\\n(40.8276026...\n\n\n2\n09X327\nComprehensive Model School Project M.S. 327\n1501 Jerome Avenue\\nBronx, NY 10452\\n(40.84241...\n\n\n3\n02M280\nManhattan Early College School for Advertising\n411 Pearl Street\\nNew York, NY 10038\\n(40.7106...\n\n\n4\n28Q680\nQueens Gateway to Health Sciences Secondary Sc...\n160-20 Goethals Avenue\\nJamaica, NY 11432\\n(40...\n\n\n\n\n\n\n\nLet’s inspect the first value in this column to see its format.\n\nprint(data.hs_directory.loc[0, \"Location 1\"])\n\n883 Classon Avenue\nBrooklyn, NY 11225\n(40.67029890700047, -73.96164787599963)\n\n\nEach value is a string with three lines. The third line contains the school’s latitude and longitude in parentheses.\nWe can extract the coordinate data using the regular expression \\((.+)\\). This matches a pair of parentheses containing 1 or more characters between them.\n\ndef get_coords(text, which = 0):\n    \n    \"\"\"Take a string, extract coordinates, and return one of the values.\"\"\"\n    \n    pattern = r\"\\((.+)\\)\" # Regex\n    extracted = re.findall(pattern, text) # Returns a list of extracted strings\n    coords = extracted[0].split(\", \") # Split string into list of strings\n    \n    # Return one of the coordinates as a float64.\n    result = np.float64(coords[which])\n    return result\n\n# Make latitude and longitude columns.\ndata.hs_directory[\"lat\"] = (\n    data.hs_directory[\"Location 1\"]\n    .apply(get_coords, which = 0)\n)\n\ndata.hs_directory[\"lon\"] = (\n    data.hs_directory[\"Location 1\"]\n    .apply(get_coords, which = 1)\n)\n\ndata.hs_directory[[\"lat\", \"lon\"]].head()\n\n\n\n\n\n\n\n\nlat\nlon\n\n\n\n\n0\n40.670299\n-73.961648\n\n\n1\n40.827603\n-73.904475\n\n\n2\n40.842414\n-73.916162\n\n\n3\n40.710679\n-74.000807\n\n\n4\n40.718810\n-73.806500\n\n\n\n\n\n\n\nWe have successfully extracted the latitude and longitude from each string."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#survey-dataset-columns",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#survey-dataset-columns",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Survey Dataset Columns",
    "text": "Survey Dataset Columns\nAnother issue is that there are too many columns in the survey dataset:\n\ndata.survey.shape\n\n(1702, 2773)\n\n\nThere are 2773 columns. The reason is that each column represents a different survey item or option. It appears that the survey had many items.\nHowever, if we look at the data dictionary, we can see that there are only a few very important columns:\n\nimportant_fields = [\n    # Unique DBN of each school\n    \"DBN\",\n    \n    # Response rates\n    \"rr_s\", \n    \"rr_t\", \n    \"rr_p\", \n    \"N_s\", \n    \"N_t\", \n    \"N_p\", \n    \n    # Scores about various aspects of the school experience\n    \"saf_p_11\", \n    \"com_p_11\", \n    \"eng_p_11\", \n    \"aca_p_11\", \n    \"saf_t_11\", \n    \"com_t_11\", \n    \"eng_t_11\", \n    \"aca_t_11\", \n    \"saf_s_11\", \n    \"com_s_11\", \n    \"eng_s_11\", \n    \"aca_s_11\", \n    \"saf_tot_11\", \n    \"com_tot_11\", \n    \"eng_tot_11\", \n    \"aca_tot_11\",\n]\n\nThus, we will only keep these columns in survey and remove the rest.\n\ndata.survey = data.survey[important_fields]"
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#dbn-column",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#dbn-column",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "DBN Column",
    "text": "DBN Column\nEarlier, we saw that the first column in the sat_results dataset was DBN.\n\ndata.sat_results.head()\n\n\n\n\n\n\n\n\nDBN\nSCHOOL NAME\nNum of SAT Test Takers\nSAT Critical Reading Avg. Score\nSAT Math Avg. Score\nSAT Writing Avg. Score\nsat_score\n\n\n\n\n0\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL STUDIES\n29\n355.0\n404.0\n363.0\n1122.0\n\n\n1\n01M448\nUNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n91\n383.0\n423.0\n366.0\n1172.0\n\n\n2\n01M450\nEAST SIDE COMMUNITY SCHOOL\n70\n377.0\n402.0\n370.0\n1149.0\n\n\n3\n01M458\nFORSYTH SATELLITE ACADEMY\n7\n414.0\n401.0\n359.0\n1174.0\n\n\n4\n01M509\nMARTA VALLE HIGH SCHOOL\n44\n390.0\n433.0\n384.0\n1207.0\n\n\n\n\n\n\n\nThe DBN, or District Borough Number, is a unique code that identifies each school in NYC.\nSince it is unique to each school, we can use it to match rows across our datasets and combine them into one. However, the class_size dataset doesn’t have a DBN column.\n\ndata.class_size.head()\n\n\n\n\n\n\n\n\nCSD\nBOROUGH\nSCHOOL CODE\nSCHOOL NAME\nGRADE\nPROGRAM TYPE\nCORE SUBJECT (MS CORE and 9-12 ONLY)\nCORE COURSE (MS CORE and 9-12 ONLY)\nSERVICE CATEGORY(K-9* ONLY)\nNUMBER OF STUDENTS / SEATS FILLED\nNUMBER OF SECTIONS\nAVERAGE CLASS SIZE\nSIZE OF SMALLEST CLASS\nSIZE OF LARGEST CLASS\nDATA SOURCE\nSCHOOLWIDE PUPIL-TEACHER RATIO\n\n\n\n\n0\n1\nM\nM015\nP.S. 015 Roberto Clemente\n0K\nGEN ED\n-\n-\n-\n19.0\n1.0\n19.0\n19.0\n19.0\nATS\nNaN\n\n\n1\n1\nM\nM015\nP.S. 015 Roberto Clemente\n0K\nCTT\n-\n-\n-\n21.0\n1.0\n21.0\n21.0\n21.0\nATS\nNaN\n\n\n2\n1\nM\nM015\nP.S. 015 Roberto Clemente\n01\nGEN ED\n-\n-\n-\n17.0\n1.0\n17.0\n17.0\n17.0\nATS\nNaN\n\n\n3\n1\nM\nM015\nP.S. 015 Roberto Clemente\n01\nCTT\n-\n-\n-\n17.0\n1.0\n17.0\n17.0\n17.0\nATS\nNaN\n\n\n4\n1\nM\nM015\nP.S. 015 Roberto Clemente\n02\nGEN ED\n-\n-\n-\n15.0\n1.0\n15.0\n15.0\n15.0\nATS\nNaN\n\n\n\n\n\n\n\nBy comparing the sat_results dataset to the class_size dataset, we can see that the DBN is actually a combination of:\n\nthe CSD (Community School District), like “01”.\nthe School Code, like “M015”.\n\nThe CSD must have two digits, so single-digit CSDs need to be padded with a zero in front. This is done below.\n\ndef zero_pad(num):\n    \n    \"\"\"Zero-pad a number if it only has 1 digit.\"\"\"\n    \n    digits = len(str(num))\n    \n    if digits == 1:\n        zero_padded = str(num).zfill(2)\n        return zero_padded\n    \n    elif digits == 2:\n        return str(num)\n    \n    else:\n        raise ValueError(\"Invalid number of digits.\")\n\ndata.class_size[\"padded_csd\"] = data.class_size[\"CSD\"].apply(zero_pad)\n\ndata.class_size[[\"CSD\", \"padded_csd\", \"SCHOOL CODE\"]].head()\n\n\n\n\n\n\n\n\nCSD\npadded_csd\nSCHOOL CODE\n\n\n\n\n0\n1\n01\nM015\n\n\n1\n1\n01\nM015\n\n\n2\n1\n01\nM015\n\n\n3\n1\n01\nM015\n\n\n4\n1\n01\nM015\n\n\n\n\n\n\n\nNow, we can combine padded_csd and SCHOOL CODE to make the DBN.\n\ndata.class_size[\"DBN\"] = (\n    data.class_size[[\"padded_csd\", \"SCHOOL CODE\"]]\n    .sum(axis = 1) # Add values along rows.\n)\n\n\n# Reorder the df so that DBN is in front.\ndef reorder_columns(df, first_cols):\n    \n    \"\"\"Take a DataFrame and a list of columns. Move those columns to the left side. Return the new DataFrame.\"\"\"\n    \n    result = df[\n        first_cols\n        + [col for col in df if col not in first_cols]\n    ]\n    \n    return result\n\ndata.class_size = reorder_columns(data.class_size, [\"DBN\"])\n\n\n# Remove padded_csd column\ndata.class_size.drop(\n    \"padded_csd\",\n    axis = 1,\n    inplace = True,\n)\n\ndata.class_size.head()\n\n\n\n\n\n\n\n\nDBN\nCSD\nBOROUGH\nSCHOOL CODE\nSCHOOL NAME\nGRADE\nPROGRAM TYPE\nCORE SUBJECT (MS CORE and 9-12 ONLY)\nCORE COURSE (MS CORE and 9-12 ONLY)\nSERVICE CATEGORY(K-9* ONLY)\nNUMBER OF STUDENTS / SEATS FILLED\nNUMBER OF SECTIONS\nAVERAGE CLASS SIZE\nSIZE OF SMALLEST CLASS\nSIZE OF LARGEST CLASS\nDATA SOURCE\nSCHOOLWIDE PUPIL-TEACHER RATIO\n\n\n\n\n0\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n0K\nGEN ED\n-\n-\n-\n19.0\n1.0\n19.0\n19.0\n19.0\nATS\nNaN\n\n\n1\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n0K\nCTT\n-\n-\n-\n21.0\n1.0\n21.0\n21.0\n21.0\nATS\nNaN\n\n\n2\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n01\nGEN ED\n-\n-\n-\n17.0\n1.0\n17.0\n17.0\n17.0\nATS\nNaN\n\n\n3\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n01\nCTT\n-\n-\n-\n17.0\n1.0\n17.0\n17.0\n17.0\nATS\nNaN\n\n\n4\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n02\nGEN ED\n-\n-\n-\n15.0\n1.0\n15.0\n15.0\n15.0\nATS\nNaN\n\n\n\n\n\n\n\nWe were successful in creating a DBN column for the class size dataset."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#condensing-datasets",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#condensing-datasets",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Condensing Datasets",
    "text": "Condensing Datasets\nAnother issue that needs to be addressed is that some datasets have multiple rows with the same DBN. For example, look at the frequency table of DBN values in class_size.\n\ndata.class_size[\"DBN\"].value_counts()\n\n15K429    57\n09X505    56\n09X517    56\n21K690    52\n15K448    52\n          ..\n27Q273     3\n03M452     3\n09X090     2\n27Q465     2\n02M267     2\nName: DBN, Length: 1487, dtype: int64\n\n\nEach DBN appears at least twice in the dataset. This would be problematic when merging it with other datasets, since each row must have a unique DBN value.\nTherefore, we need to condense class_size and other such datasets so that the DBN value is unique to each row.\nThe code below identifies which columns need to be condensed based on the frequency of its DBN values.\n\n[\n    df_name\n    for df_name in data.index\n    if data[df_name][\"DBN\"].value_counts().max() &gt; 1\n    \n    # Check if any DBN values appear more than once.\n]\n\n['ap_2010', 'class_size', 'demographics', 'graduation']\n\n\nThe 4 datasets listed above will be condensed one by one.\n\nAdvanced Placement Dataset\nFirst, the ap_2010 dataset will be condensed. It contains data on AP (Advanced Placement) test results in 2010.\n\nprint(data.ap_2010.shape)\n\ndata.ap_2010.head()\n\n(258, 5)\n\n\n\n\n\n\n\n\n\nDBN\nSchoolName\nAP Test Takers\nTotal Exams Taken\nNumber of Exams with scores 3 4 or 5\n\n\n\n\n0\n01M448\nUNIVERSITY NEIGHBORHOOD H.S.\n39\n49\n10\n\n\n1\n01M450\nEAST SIDE COMMUNITY HS\n19\n21\ns\n\n\n2\n01M515\nLOWER EASTSIDE PREP\n24\n26\n24\n\n\n3\n01M539\nNEW EXPLORATIONS SCI,TECH,MATH\n255\n377\n191\n\n\n4\n02M296\nHigh School of Hospitality Management\ns\ns\ns\n\n\n\n\n\n\n\nFrom the first five rows, it is not immediately clear why DBN values repeat. The DBNs are unique in this sample.\nLet’s check the frequency table of DBN values.\n\ndata.ap_2010[\"DBN\"].value_counts()\n\n04M610    2\n01M448    1\n19K507    1\n17K528    1\n17K537    1\n         ..\n09X329    1\n09X365    1\n09X412    1\n09X414    1\n32K556    1\nName: DBN, Length: 257, dtype: int64\n\n\nThis frequency table is ordered by frequency, descending. Thus, we can tell that 04M610 is the only DBN that repeats in this dataset. Let’s inspect the 2 rows with this DBN.\n\ndata.ap_2010.loc[\n    data.ap_2010[\"DBN\"] == \"04M610\"\n]\n\n\n\n\n\n\n\n\nDBN\nSchoolName\nAP Test Takers\nTotal Exams Taken\nNumber of Exams with scores 3 4 or 5\n\n\n\n\n51\n04M610\nTHE YOUNG WOMEN'S LEADERSHIP SCHOOL OF EAST HA...\n41\n55\n29\n\n\n52\n04M610\nYOUNG WOMEN'S LEADERSHIP SCH\ns\ns\ns\n\n\n\n\n\n\n\nIt looks like the duplication was caused by a simple error in data entry. Row 51 is valid, whereas row 52 has an incomplete school name. It also has a string “s” in numerical columns; this is invalid data.\nThus, we will drop row 52 from the dataset.\n\ndata.ap_2010.drop(52, inplace = True)\n\ndata.ap_2010[\"DBN\"].value_counts().max()\n\n1\n\n\nNow that we’ve dropped the inaccurate row, none of the DBN values repeat.\nAs a side note, since there were “s” strings in the numerical columns, we realize that these are object-type columns. Thus, we have to convert these into numerical columns in order to perform analysis on them later on.\n\ncols = ['AP Test Takers ', 'Total Exams Taken', 'Number of Exams with scores 3 4 or 5']\n\nfor col in cols:\n    # Convert each column to numerical format.\n    data.ap_2010[col] = pd.to_numeric(\n        data.ap_2010[col],\n        errors = \"coerce\", # In case of errors, use NaN values.\n    )\n    \ndata.ap_2010.dtypes\n\nDBN                                      object\nSchoolName                               object\nAP Test Takers                          float64\nTotal Exams Taken                       float64\nNumber of Exams with scores 3 4 or 5    float64\ndtype: object\n\n\nThe numerical columns were successfully converted into float64 (decimal) format.\n\n\nClass Size Dataset\nNext, we will condense the class_size dataset. It contains aggregated data on the number of students in classes in NYC schools.\nWhy are there rows with duplicate DBNs? Let’s look at the first few rows.\n\nprint(data.class_size.shape)\n\ndata.class_size.head()\n\n(27611, 17)\n\n\n\n\n\n\n\n\n\nDBN\nCSD\nBOROUGH\nSCHOOL CODE\nSCHOOL NAME\nGRADE\nPROGRAM TYPE\nCORE SUBJECT (MS CORE and 9-12 ONLY)\nCORE COURSE (MS CORE and 9-12 ONLY)\nSERVICE CATEGORY(K-9* ONLY)\nNUMBER OF STUDENTS / SEATS FILLED\nNUMBER OF SECTIONS\nAVERAGE CLASS SIZE\nSIZE OF SMALLEST CLASS\nSIZE OF LARGEST CLASS\nDATA SOURCE\nSCHOOLWIDE PUPIL-TEACHER RATIO\n\n\n\n\n0\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n0K\nGEN ED\n-\n-\n-\n19.0\n1.0\n19.0\n19.0\n19.0\nATS\nNaN\n\n\n1\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n0K\nCTT\n-\n-\n-\n21.0\n1.0\n21.0\n21.0\n21.0\nATS\nNaN\n\n\n2\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n01\nGEN ED\n-\n-\n-\n17.0\n1.0\n17.0\n17.0\n17.0\nATS\nNaN\n\n\n3\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n01\nCTT\n-\n-\n-\n17.0\n1.0\n17.0\n17.0\n17.0\nATS\nNaN\n\n\n4\n01M015\n1\nM\nM015\nP.S. 015 Roberto Clemente\n02\nGEN ED\n-\n-\n-\n15.0\n1.0\n15.0\n15.0\n15.0\nATS\nNaN\n\n\n\n\n\n\n\nLook at the GRADE and PROGRAM TYPE columns. Each school’s data is split by grade level, from K to 12. The data are further split by the program type (GEN ED, CTT, etc.). That’s why each DBN has multiple rows.\nIn our case, we are only concerned with high school students’ data (grades 9 to 12) since the SAT is taken at this level of education. Let’s see which GRADE value/s correspond to high school.\n\ndata.class_size[\"GRADE \"].value_counts().sort_index()\n\n01          1185\n02          1167\n03          1143\n04          1140\n05          1086\n06           846\n07           778\n08           735\n09            20\n09-12      10644\n0K          1237\n0K-09       1384\nMS Core     4762\nName: GRADE , dtype: int64\n\n\nConveniently, there is a 09-12 value. Rows with this value would contain aggregated data on all high school grade levels. Thus, we will filter the class_size dataset to keep only these rows.\nThere is still the issue of the PROGRAM TYPE column, however. Let’s look at its frequency table.\n\ndata.class_size[\"PROGRAM TYPE\"].value_counts()\n\nGEN ED     14545\nCTT         7460\nSPEC ED     3653\nG&T          469\nName: PROGRAM TYPE, dtype: int64\n\n\nThere are hundreds or thousands of rows under each program type. According to the data dictionary, the following are the meanings of these values:\n\nGEN ED: General Education\nCTT: Collaborative Team Teaching\nSPEC ED: Special Education\nG&T: Gifted & Talented\n\nThe General Education program best represents the majority of high school students, and this is the most frequent program in the data. Thus, we will keep the GEN ED rows and drop the rest.\nTo recap, we chose to filter the dataset to keep rows where the grade level is from 9 to 12 and the program type is General Education. We will do this using the code below.\n\ndata.class_size = data.class_size.loc[\n    (data.class_size[\"GRADE \"] == \"09-12\")\n    & (data.class_size[\"PROGRAM TYPE\"] == \"GEN ED\")\n]\n\nprint(data.class_size.shape)\ndata.class_size.head()\n\n(6513, 17)\n\n\n\n\n\n\n\n\n\nDBN\nCSD\nBOROUGH\nSCHOOL CODE\nSCHOOL NAME\nGRADE\nPROGRAM TYPE\nCORE SUBJECT (MS CORE and 9-12 ONLY)\nCORE COURSE (MS CORE and 9-12 ONLY)\nSERVICE CATEGORY(K-9* ONLY)\nNUMBER OF STUDENTS / SEATS FILLED\nNUMBER OF SECTIONS\nAVERAGE CLASS SIZE\nSIZE OF SMALLEST CLASS\nSIZE OF LARGEST CLASS\nDATA SOURCE\nSCHOOLWIDE PUPIL-TEACHER RATIO\n\n\n\n\n225\n01M292\n1\nM\nM292\nHenry Street School for International Studies\n09-12\nGEN ED\nENGLISH\nEnglish 9\n-\n63.0\n3.0\n21.0\n19.0\n25.0\nSTARS\nNaN\n\n\n226\n01M292\n1\nM\nM292\nHenry Street School for International Studies\n09-12\nGEN ED\nENGLISH\nEnglish 10\n-\n79.0\n3.0\n26.3\n24.0\n31.0\nSTARS\nNaN\n\n\n227\n01M292\n1\nM\nM292\nHenry Street School for International Studies\n09-12\nGEN ED\nENGLISH\nEnglish 11\n-\n38.0\n2.0\n19.0\n16.0\n22.0\nSTARS\nNaN\n\n\n228\n01M292\n1\nM\nM292\nHenry Street School for International Studies\n09-12\nGEN ED\nENGLISH\nEnglish 12\n-\n69.0\n3.0\n23.0\n13.0\n30.0\nSTARS\nNaN\n\n\n229\n01M292\n1\nM\nM292\nHenry Street School for International Studies\n09-12\nGEN ED\nMATH\nIntegrated Algebra\n-\n53.0\n3.0\n17.7\n16.0\n21.0\nSTARS\nNaN\n\n\n\n\n\n\n\nThe first few rows of the dataset show us that we were successful in filtering rows by GRADE and PROGRAM TYPE. From 27611 rows, we have narrowed the dataset down to 6513 rows.\nHowever, there are still duplicate DBN values. The new problem is that the data are divided by the following columns:\n\nCORE SUBJECT (MS CORE and 9-12 ONLY)\nCORE COURSE (MS CORE and 9-12 ONLY)\n\nThese columns contain the specific subject or course of the class. We want to get class size data that accounts for all subjects, so we cannot simply filter the dataset by 1 subject.\nInstead, we will:\n\nGroup the data by DBN value.\nAggregate the numerical data by taking the mean of each column in each group.\n\n\ndata.class_size = (\n    data.class_size\n    .groupby(\"DBN\") # Group by the school's unique DBN\n    .agg(np.mean) # Aggregate numerical columns using mean\n    .reset_index() # Put the DBN column back in the df\n)\n\nprint(data.class_size.shape)\ndata.class_size.head()\n\n(583, 8)\n\n\n\n\n\n\n\n\n\nDBN\nCSD\nNUMBER OF STUDENTS / SEATS FILLED\nNUMBER OF SECTIONS\nAVERAGE CLASS SIZE\nSIZE OF SMALLEST CLASS\nSIZE OF LARGEST CLASS\nSCHOOLWIDE PUPIL-TEACHER RATIO\n\n\n\n\n0\n01M292\n1.0\n88.0000\n4.000000\n22.564286\n18.50\n26.571429\nNaN\n\n\n1\n01M332\n1.0\n46.0000\n2.000000\n22.000000\n21.00\n23.500000\nNaN\n\n\n2\n01M378\n1.0\n33.0000\n1.000000\n33.000000\n33.00\n33.000000\nNaN\n\n\n3\n01M448\n1.0\n105.6875\n4.750000\n22.231250\n18.25\n27.062500\nNaN\n\n\n4\n01M450\n1.0\n57.6000\n2.733333\n21.200000\n19.40\n22.866667\nNaN\n\n\n\n\n\n\n\nThe following changes have occurred:\n\nThe dataset now has only 583 rows.\nOnly the DBN, CSD, and numerical columns remain.\nThe numerical columns contain decimals since the data were aggregated.\n\nLet’s check what the maximum frequency of DBN values is.\n\ndata.class_size[\"DBN\"].value_counts().max()\n\n1\n\n\nEach DBN only appears once in the data. We have condensed the dataset successfully.\n\n\nDemographics Dataset\nNext, we condense the Demographics dataset. It contains data on NYC high school students’ grade level, gender, ethnicity, etc. from 2006 to 2012.\nLet’s try to identify why each DBN has multiple rows.\n\nprint(data.demographics.shape)\n\ndata.demographics.head()\n\n(10075, 38)\n\n\n\n\n\n\n\n\n\nDBN\nName\nschoolyear\nfl_percent\nfrl_percent\ntotal_enrollment\nprek\nk\ngrade1\ngrade2\n...\nblack_num\nblack_per\nhispanic_num\nhispanic_per\nwhite_num\nwhite_per\nmale_num\nmale_per\nfemale_num\nfemale_per\n\n\n\n\n0\n01M015\nP.S. 015 ROBERTO CLEMENTE\n20052006\n89.4\nNaN\n281\n15\n36\n40\n33\n...\n74\n26.3\n189\n67.3\n5\n1.8\n158.0\n56.2\n123.0\n43.8\n\n\n1\n01M015\nP.S. 015 ROBERTO CLEMENTE\n20062007\n89.4\nNaN\n243\n15\n29\n39\n38\n...\n68\n28.0\n153\n63.0\n4\n1.6\n140.0\n57.6\n103.0\n42.4\n\n\n2\n01M015\nP.S. 015 ROBERTO CLEMENTE\n20072008\n89.4\nNaN\n261\n18\n43\n39\n36\n...\n77\n29.5\n157\n60.2\n7\n2.7\n143.0\n54.8\n118.0\n45.2\n\n\n3\n01M015\nP.S. 015 ROBERTO CLEMENTE\n20082009\n89.4\nNaN\n252\n17\n37\n44\n32\n...\n75\n29.8\n149\n59.1\n7\n2.8\n149.0\n59.1\n103.0\n40.9\n\n\n4\n01M015\nP.S. 015 ROBERTO CLEMENTE\n20092010\n\n96.5\n208\n16\n40\n28\n32\n...\n67\n32.2\n118\n56.7\n6\n2.9\n124.0\n59.6\n84.0\n40.4\n\n\n\n\n5 rows × 38 columns\n\n\n\nBased on the schoolyear column, each school has a separate row for each academic year. Let’s inspect the years available.\n\ndata.demographics[\"schoolyear\"].value_counts().sort_index()\n\n20052006    1356\n20062007    1386\n20072008    1410\n20082009    1441\n20092010    1475\n20102011    1498\n20112012    1509\nName: schoolyear, dtype: int64\n\n\nOne of the values represents school year 2011-2012. This is relevant to our analysis because the SAT results dataset is from 2012. Thus, we will filter demographics to include only this year.\n\ndata.demographics = data.demographics.loc[\n    data.demographics[\"schoolyear\"] == 20112012\n]\n\nprint(data.demographics.shape)\ndata.demographics.head()\n\n(1509, 38)\n\n\n\n\n\n\n\n\n\nDBN\nName\nschoolyear\nfl_percent\nfrl_percent\ntotal_enrollment\nprek\nk\ngrade1\ngrade2\n...\nblack_num\nblack_per\nhispanic_num\nhispanic_per\nwhite_num\nwhite_per\nmale_num\nmale_per\nfemale_num\nfemale_per\n\n\n\n\n6\n01M015\nP.S. 015 ROBERTO CLEMENTE\n20112012\nNaN\n89.4\n189\n13\n31\n35\n28\n...\n63\n33.3\n109\n57.7\n4\n2.1\n97.0\n51.3\n92.0\n48.7\n\n\n13\n01M019\nP.S. 019 ASHER LEVY\n20112012\nNaN\n61.5\n328\n32\n46\n52\n54\n...\n81\n24.7\n158\n48.2\n28\n8.5\n147.0\n44.8\n181.0\n55.2\n\n\n20\n01M020\nPS 020 ANNA SILVER\n20112012\nNaN\n92.5\n626\n52\n102\n121\n87\n...\n55\n8.8\n357\n57.0\n16\n2.6\n330.0\n52.7\n296.0\n47.3\n\n\n27\n01M034\nPS 034 FRANKLIN D ROOSEVELT\n20112012\nNaN\n99.7\n401\n14\n34\n38\n36\n...\n90\n22.4\n275\n68.6\n8\n2.0\n204.0\n50.9\n197.0\n49.1\n\n\n35\n01M063\nPS 063 WILLIAM MCKINLEY\n20112012\nNaN\n78.9\n176\n18\n20\n30\n21\n...\n41\n23.3\n110\n62.5\n15\n8.5\n97.0\n55.1\n79.0\n44.9\n\n\n\n\n5 rows × 38 columns\n\n\n\nFiltering the dataset has reduced it to 1509 rows. Also, if we look at the DBN column, the values appear to be distinct now. Let’s check the maximum frequency of its values:\n\ndata.demographics[\"DBN\"].value_counts().max()\n\n1\n\n\nEach DBN value appears only once.\n\n\nGraduation Dataset\nLastly, we will condense the graduation dataset. It contains data on the graduation rate in the Cohorts of 2001 to 2006. The cohort of a year is the set of students that entered Grade 9 in that year. Thus, the cohorts in this dataset graduated between 2005 and 2010.\nBelow are the first five rows of the dataset.\n\nprint(data.graduation.shape)\n\ndata.graduation.head(5)\n\n(25096, 23)\n\n\n\n\n\n\n\n\n\nDemographic\nDBN\nSchool Name\nCohort\nTotal Cohort\nTotal Grads - n\nTotal Grads - % of cohort\nTotal Regents - n\nTotal Regents - % of cohort\nTotal Regents - % of grads\n...\nRegents w/o Advanced - n\nRegents w/o Advanced - % of cohort\nRegents w/o Advanced - % of grads\nLocal - n\nLocal - % of cohort\nLocal - % of grads\nStill Enrolled - n\nStill Enrolled - % of cohort\nDropped Out - n\nDropped Out - % of cohort\n\n\n\n\n0\nTotal Cohort\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL\n2003\n5\ns\ns\ns\ns\ns\n...\ns\ns\ns\ns\ns\ns\ns\ns\ns\ns\n\n\n1\nTotal Cohort\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL\n2004\n55\n37\n67.3%\n17\n30.9%\n45.9%\n...\n17\n30.9%\n45.9%\n20\n36.4%\n54.1%\n15\n27.3%\n3\n5.5%\n\n\n2\nTotal Cohort\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL\n2005\n64\n43\n67.2%\n27\n42.2%\n62.8%\n...\n27\n42.2%\n62.8%\n16\n25%\n37.200000000000003%\n9\n14.1%\n9\n14.1%\n\n\n3\nTotal Cohort\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL\n2006\n78\n43\n55.1%\n36\n46.2%\n83.7%\n...\n36\n46.2%\n83.7%\n7\n9%\n16.3%\n16\n20.5%\n11\n14.1%\n\n\n4\nTotal Cohort\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL\n2006 Aug\n78\n44\n56.4%\n37\n47.4%\n84.1%\n...\n37\n47.4%\n84.1%\n7\n9%\n15.9%\n15\n19.2%\n11\n14.1%\n\n\n\n\n5 rows × 23 columns\n\n\n\nAt first glance, the Cohort column appears to be responsible for the duplication of DBNs. Each school has a separate row for each cohort. Below are the cohort values:\n\ndata.graduation[\"Cohort\"].value_counts().sort_index()\n\n2001        2637\n2002        3095\n2003        3432\n2004        3708\n2005        3963\n2006        4130\n2006 Aug    4131\nName: Cohort, dtype: int64\n\n\nAs expected, the cohorts range from 2001 to 2006. There also appears to be an extra value representing August of 2006.\nSince we can only keep 1 row from each school, it is best to keep the 2006 rows since these are the most recent.\n\ndata.graduation = data.graduation.loc[\n    data.graduation[\"Cohort\"] == \"2006\"\n]\n\nprint(data.graduation.shape)\ndata.graduation.head()\n\n(4130, 23)\n\n\n\n\n\n\n\n\n\nDemographic\nDBN\nSchool Name\nCohort\nTotal Cohort\nTotal Grads - n\nTotal Grads - % of cohort\nTotal Regents - n\nTotal Regents - % of cohort\nTotal Regents - % of grads\n...\nRegents w/o Advanced - n\nRegents w/o Advanced - % of cohort\nRegents w/o Advanced - % of grads\nLocal - n\nLocal - % of cohort\nLocal - % of grads\nStill Enrolled - n\nStill Enrolled - % of cohort\nDropped Out - n\nDropped Out - % of cohort\n\n\n\n\n3\nTotal Cohort\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL\n2006\n78\n43\n55.1%\n36\n46.2%\n83.7%\n...\n36\n46.2%\n83.7%\n7\n9%\n16.3%\n16\n20.5%\n11\n14.1%\n\n\n10\nTotal Cohort\n01M448\nUNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n2006\n124\n53\n42.7%\n42\n33.9%\n79.2%\n...\n34\n27.4%\n64.2%\n11\n8.9%\n20.8%\n46\n37.1%\n20\n16.100000000000001%\n\n\n17\nTotal Cohort\n01M450\nEAST SIDE COMMUNITY SCHOOL\n2006\n90\n70\n77.8%\n67\n74.400000000000006%\n95.7%\n...\n67\n74.400000000000006%\n95.7%\n3\n3.3%\n4.3%\n15\n16.7%\n5\n5.6%\n\n\n24\nTotal Cohort\n01M509\nMARTA VALLE HIGH SCHOOL\n2006\n84\n47\n56%\n40\n47.6%\n85.1%\n...\n23\n27.4%\n48.9%\n7\n8.300000000000001%\n14.9%\n25\n29.8%\n5\n6%\n\n\n31\nTotal Cohort\n01M515\nLOWER EAST SIDE PREPARATORY HIGH SCHO\n2006\n193\n105\n54.4%\n91\n47.2%\n86.7%\n...\n22\n11.4%\n21%\n14\n7.3%\n13.3%\n53\n27.5%\n35\n18.100000000000001%\n\n\n\n\n5 rows × 23 columns\n\n\n\nThe dataset is left with 4130 rows. Let’s check if the DBNs are unique.\n\ndata.graduation[\"DBN\"].value_counts().max()\n\n13\n\n\nIt seems that the DBNs still have up to 13 duplicates each.\nThis may be due to the Demographic column. What if it contains values other than “Total Cohort”?\n\ndata.graduation[\"Demographic\"].value_counts()\n\nSpecial Education Students     411\nTotal Cohort                   405\nGeneral Education Students     405\nEnglish Proficient Students    401\nFemale                         396\nHispanic                       395\nMale                           395\nBlack                          394\nEnglish Language Learners      340\nAsian                          296\nWhite                          292\nName: Demographic, dtype: int64\n\n\nIndeed, each school has a separate row for each demographic. Since we want to include all or most of the students, we will take the “Total Cohort” rows.\n\ndata.graduation = data.graduation.loc[\n    data.graduation[\"Demographic\"] == \"Total Cohort\"\n]\n\nprint(data.graduation.shape)\ndata.graduation.head()\n\n(405, 23)\n\n\n\n\n\n\n\n\n\nDemographic\nDBN\nSchool Name\nCohort\nTotal Cohort\nTotal Grads - n\nTotal Grads - % of cohort\nTotal Regents - n\nTotal Regents - % of cohort\nTotal Regents - % of grads\n...\nRegents w/o Advanced - n\nRegents w/o Advanced - % of cohort\nRegents w/o Advanced - % of grads\nLocal - n\nLocal - % of cohort\nLocal - % of grads\nStill Enrolled - n\nStill Enrolled - % of cohort\nDropped Out - n\nDropped Out - % of cohort\n\n\n\n\n3\nTotal Cohort\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL\n2006\n78\n43\n55.1%\n36\n46.2%\n83.7%\n...\n36\n46.2%\n83.7%\n7\n9%\n16.3%\n16\n20.5%\n11\n14.1%\n\n\n10\nTotal Cohort\n01M448\nUNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n2006\n124\n53\n42.7%\n42\n33.9%\n79.2%\n...\n34\n27.4%\n64.2%\n11\n8.9%\n20.8%\n46\n37.1%\n20\n16.100000000000001%\n\n\n17\nTotal Cohort\n01M450\nEAST SIDE COMMUNITY SCHOOL\n2006\n90\n70\n77.8%\n67\n74.400000000000006%\n95.7%\n...\n67\n74.400000000000006%\n95.7%\n3\n3.3%\n4.3%\n15\n16.7%\n5\n5.6%\n\n\n24\nTotal Cohort\n01M509\nMARTA VALLE HIGH SCHOOL\n2006\n84\n47\n56%\n40\n47.6%\n85.1%\n...\n23\n27.4%\n48.9%\n7\n8.300000000000001%\n14.9%\n25\n29.8%\n5\n6%\n\n\n31\nTotal Cohort\n01M515\nLOWER EAST SIDE PREPARATORY HIGH SCHO\n2006\n193\n105\n54.4%\n91\n47.2%\n86.7%\n...\n22\n11.4%\n21%\n14\n7.3%\n13.3%\n53\n27.5%\n35\n18.100000000000001%\n\n\n\n\n5 rows × 23 columns\n\n\n\nAfter the dataset was filtered for the second time, it was left with 405 rows.\nLet’s check the maximum frequency of DBNs again.\n\ndata.graduation[\"DBN\"].value_counts().max()\n\n1\n\n\nThe DBNs are now unique to each row in the dataset."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#column-names",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#column-names",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Column Names",
    "text": "Column Names\nWe’re almost at the point where we can combine the datasets. However, the final dataset would have many columns. It would be good to be able to know which dataset each column came from. Thus, we will use the following prefix system:\n\nA_: The ap_2010 dataset\nC_: The class_size dataset\nD_: The demographics dataset\nG_: The graduation dataset\nH_: The hs_directory dataset\nR_: The sat_results dataset\nS_: The survey dataset\n\nI add these prefixes using the code below.\n\nfor df_name in data.index:\n    # Take the first letter, capitalize it, and append an underscore.\n    if df_name == \"sat_results\":\n        prefix = \"R_\"\n    else:\n        prefix = df_name[0].upper() + \"_\"\n    \n    # Add the prefix to each column label.\n    df = data[df_name]\n    non_dbn = [col for col in df if col != \"DBN\"]\n    \n    dbn_col = df[[\"DBN\"]]\n    other_cols = df[non_dbn].add_prefix(prefix)\n    \n    # Put the df back into the data Series.\n    data[df_name] = pd.concat(\n        [dbn_col, other_cols],\n        axis = 1,\n    )\n\nFor example, let’s see what ap_2010 looks like now:\n\ndata.ap_2010.head()\n\n\n\n\n\n\n\n\nDBN\nA_SchoolName\nA_AP Test Takers\nA_Total Exams Taken\nA_Number of Exams with scores 3 4 or 5\n\n\n\n\n0\n01M448\nUNIVERSITY NEIGHBORHOOD H.S.\n39.0\n49.0\n10.0\n\n\n1\n01M450\nEAST SIDE COMMUNITY HS\n19.0\n21.0\nNaN\n\n\n2\n01M515\nLOWER EASTSIDE PREP\n24.0\n26.0\n24.0\n\n\n3\n01M539\nNEW EXPLORATIONS SCI,TECH,MATH\n255.0\n377.0\n191.0\n\n\n4\n02M296\nHigh School of Hospitality Management\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nThe prefixes were added successfully."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#combining-datasets",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#combining-datasets",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Combining Datasets",
    "text": "Combining Datasets\nAt last, we are ready to combine all of the datasets into one. Datasets be merged on the DBN column since this is unique to each row.\nWe will start with the sat_results dataset as the first “left” dataset in the merging process.\nWe will consider two ways to merge datasets:\n\nLeft join: Keep all rows in the “left” dataset, but remove rows in the “right” dataset if their DBNs had no match.\nInner join: Only keep rows where the DBN exists in both datasets.\n\nBefore we choose, though, we will have to find out whether the DBNs in sat_results match the ones present in other datasets.\n\nHeatmap of DBNs\nIn previous Dataquest guided projects, heatmaps were sometimes used to visualize correlation tables and missing values. I realized that this could be extended to the situation of the DBNs.\nWhat if we created a DataFrame where:\n\nThe columns represent the 7 datasets.\nThe indices represent the unique DBNs.\nEach value is True if the DBN appears in the specific dataset, else False.\n\nThen, we would be able to create a heatmap to visualize which DBNs were present and absent in each dataset.\nI wrote the code below in order to make the DataFrame.\n\ncol_list = []\n\nfor df_name in data.index:\n    dbn = data[df_name][[\"DBN\"]] # Get a 1-column DataFrame of DBN values.\n    new_col = dbn.set_index(\"DBN\") # Set the DBNs as the indices. There are no columns now.\n    new_col[df_name] = 1 # Create a new column of ones.\n    \n    col_list.append(new_col) # Append the column to the list.\n\n# Combine the columns into one DataFrame.\ndbn_grid = pd.concat(\n    col_list,\n    axis = 1,\n).notnull() # Replace values with False if they're null, else True.\n\ndbn_grid.head()\n\n\n\n\n\n\n\n\nap_2010\nclass_size\ndemographics\ngraduation\nhs_directory\nsat_results\nsurvey\n\n\nDBN\n\n\n\n\n\n\n\n\n\n\n\n01M448\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n01M450\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n01M515\nTrue\nTrue\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\n01M539\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n02M296\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n\n\n\n\n\nLet’s take the top left cell as an example. Its value is True. This means that the DBN 01M448 appears in the ap_2010 dataset.\nSince SAT performance is our focus, we can filter the rows in the DataFrame to only include the ones where the DBN is present in sat_results.\n\ndbn_grid = dbn_grid.loc[\n    dbn_grid[\"sat_results\"] # Keep rows where this column contains True values.\n]\n\nNow, we can plot our heatmap. Above it, we will also print a table of the exact number of relevant DBNs present in each dataset.\n\nprint(\"Counts of relevant DBNs in each dataset\")\nprint(dbn_grid.sum())\n\n# Initiate the figure\nplt.figure(figsize = (15, 5))\n\n# Create a heatmap of the boolean dataframe\nsns.heatmap(\n    dbn_grid,\n    cbar = True, # Include a color bar.\n    yticklabels = False, # Do not show y-tick labels.\n)\n\nplt.title(\"Heatmap of Relevant DBN Values in each Dataset\")\nplt.xlabel(\"Dataset\")\nplt.xticks(rotation = 0)\nplt.show()\n\nCounts of relevant DBNs in each dataset\nap_2010         251\nclass_size      401\ndemographics    412\ngraduation      381\nhs_directory    339\nsat_results     421\nsurvey          418\ndtype: int64\n\n\n\n\n\nEach column in the heatmap corresponds to one of our 7 datasets. The y-axis represents the unique DBNs.\nThe DBNs present in a dataset are represented by a light brown color. The missing DBNs are represented by black.\nThe following can be observed:\n\nThe sat_results dataset is complete since we are using it as our basis for which DBNs are “relevant.” There are 421 relevant DBNs in total.\nThe ap_2010 dataset has the least DBNs, only 251.\nhs_directory and graduation have missing DBNs scattered around. They have less than 400 DBNs each.\nclass_size, demographics, and survey are the most complete datasets, with over 400 DBNs.\n\nThis information will help us in choosing how to merge the datasets.\n\n\nLeft join\nThe advantage of performing a left join is apparent when the “right” dataset has many missing DBN values.\n\nWith a left join, all of the sat_results rows would be kept.\nWith an inner join, the number of rows would be significantly reduced.\n\nWe saw that ap_2010, hs_directory, and graduation have the most missing DBN values, so these are good candidates for a left join. These 3 datasets provide information that is supplementary but not very crucial to our analysis, so it is fine for some rows to contain missing values.\n\ncombined = data.sat_results\n\nfor df_name in [\"ap_2010\", \"hs_directory\", \"graduation\"]:\n    \n    # Merge horizontally.\n    combined = combined.merge(\n        data[df_name],\n        on = \"DBN\", # Merge on DBN column\n        how = \"left\", # Left join\n    )\n    \nprint(combined.shape)\ncombined.head()\n\n(421, 92)\n\n\n\n\n\n\n\n\n\nDBN\nR_SCHOOL NAME\nR_Num of SAT Test Takers\nR_SAT Critical Reading Avg. Score\nR_SAT Math Avg. Score\nR_SAT Writing Avg. Score\nR_sat_score\nA_SchoolName\nA_AP Test Takers\nA_Total Exams Taken\n...\nG_Regents w/o Advanced - n\nG_Regents w/o Advanced - % of cohort\nG_Regents w/o Advanced - % of grads\nG_Local - n\nG_Local - % of cohort\nG_Local - % of grads\nG_Still Enrolled - n\nG_Still Enrolled - % of cohort\nG_Dropped Out - n\nG_Dropped Out - % of cohort\n\n\n\n\n0\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL STUDIES\n29\n355.0\n404.0\n363.0\n1122.0\nNaN\nNaN\nNaN\n...\n36\n46.2%\n83.7%\n7\n9%\n16.3%\n16\n20.5%\n11\n14.1%\n\n\n1\n01M448\nUNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n91\n383.0\n423.0\n366.0\n1172.0\nUNIVERSITY NEIGHBORHOOD H.S.\n39.0\n49.0\n...\n34\n27.4%\n64.2%\n11\n8.9%\n20.8%\n46\n37.1%\n20\n16.100000000000001%\n\n\n2\n01M450\nEAST SIDE COMMUNITY SCHOOL\n70\n377.0\n402.0\n370.0\n1149.0\nEAST SIDE COMMUNITY HS\n19.0\n21.0\n...\n67\n74.400000000000006%\n95.7%\n3\n3.3%\n4.3%\n15\n16.7%\n5\n5.6%\n\n\n3\n01M458\nFORSYTH SATELLITE ACADEMY\n7\n414.0\n401.0\n359.0\n1174.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n01M509\nMARTA VALLE HIGH SCHOOL\n44\n390.0\n433.0\n384.0\n1207.0\nNaN\nNaN\nNaN\n...\n23\n27.4%\n48.9%\n7\n8.300000000000001%\n14.9%\n25\n29.8%\n5\n6%\n\n\n\n\n5 rows × 92 columns\n\n\n\nThe dataset still has all of the data from sat_results due to the left join. There are some missing values where the other datasets didn’t find a matching DBN.\n\n\nInner join\nInner joins are advantageous when the “right” dataset is important to analysis. We would want each row to have complete data from this dataset.\nIn our case, the remaining datasets to merge are class_size, demographics, and survey. We need the latter two datasets to be complete, since we aim to explore the relationships between student demographics and SAT performance. Thus, we will use an inner join.\n\nfor df_name in [\"class_size\", \"demographics\", \"survey\"]:\n    \n    # Merge horizontally.\n    combined = combined.merge(\n        data[df_name],\n        on = \"DBN\", # Merge on DBN column\n        how = \"inner\", # Inner join\n    )\n    \nprint(combined.shape)\ncombined.head()\n\n(401, 158)\n\n\n\n\n\n\n\n\n\nDBN\nR_SCHOOL NAME\nR_Num of SAT Test Takers\nR_SAT Critical Reading Avg. Score\nR_SAT Math Avg. Score\nR_SAT Writing Avg. Score\nR_sat_score\nA_SchoolName\nA_AP Test Takers\nA_Total Exams Taken\n...\nS_eng_t_11\nS_aca_t_11\nS_saf_s_11\nS_com_s_11\nS_eng_s_11\nS_aca_s_11\nS_saf_tot_11\nS_com_tot_11\nS_eng_tot_11\nS_aca_tot_11\n\n\n\n\n0\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL STUDIES\n29\n355.0\n404.0\n363.0\n1122.0\nNaN\nNaN\nNaN\n...\n6.1\n6.5\n6.0\n5.6\n6.1\n6.7\n6.7\n6.2\n6.6\n7.0\n\n\n1\n01M448\nUNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n91\n383.0\n423.0\n366.0\n1172.0\nUNIVERSITY NEIGHBORHOOD H.S.\n39.0\n49.0\n...\n6.6\n7.3\n6.0\n5.7\n6.3\n7.0\n6.8\n6.3\n6.7\n7.2\n\n\n2\n01M450\nEAST SIDE COMMUNITY SCHOOL\n70\n377.0\n402.0\n370.0\n1149.0\nEAST SIDE COMMUNITY HS\n19.0\n21.0\n...\n8.0\n8.8\nNaN\nNaN\nNaN\nNaN\n7.9\n7.9\n7.9\n8.4\n\n\n3\n01M458\nFORSYTH SATELLITE ACADEMY\n7\n414.0\n401.0\n359.0\n1174.0\nNaN\nNaN\nNaN\n...\n8.9\n8.9\n6.8\n6.1\n6.1\n6.8\n7.8\n7.1\n7.2\n7.8\n\n\n4\n01M509\nMARTA VALLE HIGH SCHOOL\n44\n390.0\n433.0\n384.0\n1207.0\nNaN\nNaN\nNaN\n...\n6.1\n6.8\n6.4\n5.9\n6.4\n7.0\n6.9\n6.2\n6.6\n7.0\n\n\n\n\n5 rows × 158 columns\n\n\n\nThe final combined DataFrame has 401 rows (schools) and 158 columns. That’s only 20 rows less than what we started with."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#city-school-districts",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#city-school-districts",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "City School Districts",
    "text": "City School Districts\nAdditionally, we will create a new column containing the city school district (CSD) of each school. The CSD is made up of the first 2 digits of a school’s DBN.\n\n# Make the new column.\ncombined[\"school_dist\"] = combined[\"DBN\"].apply(\n    lambda text: text[:2] # Get first two characters.\n)\n\n# Reorder the columns.\ncombined = reorder_columns(combined, [\"school_dist\"])\n\ncombined.head()\n\n\n\n\n\n\n\n\nschool_dist\nDBN\nR_SCHOOL NAME\nR_Num of SAT Test Takers\nR_SAT Critical Reading Avg. Score\nR_SAT Math Avg. Score\nR_SAT Writing Avg. Score\nR_sat_score\nA_SchoolName\nA_AP Test Takers\n...\nS_eng_t_11\nS_aca_t_11\nS_saf_s_11\nS_com_s_11\nS_eng_s_11\nS_aca_s_11\nS_saf_tot_11\nS_com_tot_11\nS_eng_tot_11\nS_aca_tot_11\n\n\n\n\n0\n01\n01M292\nHENRY STREET SCHOOL FOR INTERNATIONAL STUDIES\n29\n355.0\n404.0\n363.0\n1122.0\nNaN\nNaN\n...\n6.1\n6.5\n6.0\n5.6\n6.1\n6.7\n6.7\n6.2\n6.6\n7.0\n\n\n1\n01\n01M448\nUNIVERSITY NEIGHBORHOOD HIGH SCHOOL\n91\n383.0\n423.0\n366.0\n1172.0\nUNIVERSITY NEIGHBORHOOD H.S.\n39.0\n...\n6.6\n7.3\n6.0\n5.7\n6.3\n7.0\n6.8\n6.3\n6.7\n7.2\n\n\n2\n01\n01M450\nEAST SIDE COMMUNITY SCHOOL\n70\n377.0\n402.0\n370.0\n1149.0\nEAST SIDE COMMUNITY HS\n19.0\n...\n8.0\n8.8\nNaN\nNaN\nNaN\nNaN\n7.9\n7.9\n7.9\n8.4\n\n\n3\n01\n01M458\nFORSYTH SATELLITE ACADEMY\n7\n414.0\n401.0\n359.0\n1174.0\nNaN\nNaN\n...\n8.9\n8.9\n6.8\n6.1\n6.1\n6.8\n7.8\n7.1\n7.2\n7.8\n\n\n4\n01\n01M509\nMARTA VALLE HIGH SCHOOL\n44\n390.0\n433.0\n384.0\n1207.0\nNaN\nNaN\n...\n6.1\n6.8\n6.4\n5.9\n6.4\n7.0\n6.9\n6.2\n6.6\n7.0\n\n\n\n\n5 rows × 159 columns\n\n\n\nThe data is now cleaned and ready for analysis."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#correlation-coefficients",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#correlation-coefficients",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Correlation Coefficients",
    "text": "Correlation Coefficients\nThe goal of this project is to determine how demographic factors affect SAT scores. Thus, we can start our analysis by investigating the Pearson’s correlation coefficient between SAT score and each of the other columns.\n\nsat_corr = (\n    combined\n    .corr()[\"R_sat_score\"] # Take sat_score correlations\n    .drop([\"R_sat_score\"]) # Drop unnecessary variables\n    .dropna() # Drop missing values\n)\n\nThe correlation table has one value for each of the numerical columns in combined.\n\nsat_corr.head()\n\nR_SAT Critical Reading Avg. Score    0.976819\nR_SAT Math Avg. Score                0.956406\nR_SAT Writing Avg. Score             0.981751\nA_AP Test Takers                     0.625184\nA_Total Exams Taken                  0.614112\nName: R_sat_score, dtype: float64\n\n\nWe’ll analyze the data by going through each set of variables based on their original dataset. We’ll only use the most important datasets.\nBefore we do this, we’ll define a function that will take a Series and return values whose labels have certain prefixes. This will help us select parts of the correlation table more easily.\n\ndef filter_prefix(series, regex):\n    \n    \"\"\"Takes a Series and a regex. Returns items in the Series whose labels match the regex at the start.\"\"\"\n    \n    result = series[[\n        label for label in series.index\n        if re.match(regex, label)\n    ]]\n    return result\n\nFor example, we could select all correlations for ap_2010 and class_size variables by filtering prefixes A and C.\n\nfilter_prefix(sat_corr, r\"[AC]\")\n\nA_AP Test Takers                          0.625184\nA_Total Exams Taken                       0.614112\nA_Number of Exams with scores 3 4 or 5    0.614752\nC_CSD                                     0.054011\nC_NUMBER OF STUDENTS / SEATS FILLED       0.400095\nC_NUMBER OF SECTIONS                      0.364537\nC_AVERAGE CLASS SIZE                      0.395964\nC_SIZE OF SMALLEST CLASS                  0.282388\nC_SIZE OF LARGEST CLASS                   0.327099\nName: R_sat_score, dtype: float64\n\n\nLet’s proceed to the first set of variables."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#ap-exam-results",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#ap-exam-results",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "AP Exam Results",
    "text": "AP Exam Results\nFirst, we’ll look at the correlations for ap_2010 variables.\n\nfilter_prefix(sat_corr, r\"[A]\")\n\nA_AP Test Takers                          0.625184\nA_Total Exams Taken                       0.614112\nA_Number of Exams with scores 3 4 or 5    0.614752\nName: R_sat_score, dtype: float64\n\n\nThe number of AP test takers has a positive correlation of 0.63 with SAT score.\nRemember that the AP exams are “Advanced Placement” exams taken by USA high schoolers in order to get college credit. Thus, the students who take AP exams tend to have higher academic performance.\nLet’s make a scatter plot of SAT score against percentage of AP test takers in each school.\n\n# Make a new column for AP test taker percentage.\ncombined[\"A_ap_perc\"] = combined[\"A_AP Test Takers \"] / combined[\"D_total_enrollment\"]\n\n# Replace inaccurate data (above 100%) with nulls.\ncombined[\"A_ap_perc\"] = combined[\"A_ap_perc\"].mask(\n    combined[\"A_ap_perc\"] &gt; 1.0,\n    np.nan,\n)\n\n# Scatter plot\ncombined.plot.scatter(\n    x = \"A_ap_perc\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs AP Test Taker Percentage\")\nplt.xlabel(\"AP Test Taker Percentage\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe relationship is not linear. We can only observe that:\n\nMost schools have less than 40% AP takers and less than 1400 SAT points.\nThe schools with high SAT scores (above 1600) have less than 60% AP test takers.\nThe schools with low SAT scores (under 1400) can have any percentage of AP test takers.\n\nTherefore, it doesn’t seem that the percentage of AP test takers in a school reliably influences its average SAT score."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#survey-data",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#survey-data",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Survey Data",
    "text": "Survey Data\nLet’s move on to the survey variables.\n\nfilter_prefix(sat_corr, r\"[S]\")\n\nS_rr_s          0.286470\nS_rr_t          0.022365\nS_rr_p          0.087970\nS_N_s           0.438427\nS_N_t           0.306520\nS_N_p           0.439547\nS_saf_p_11      0.111892\nS_com_p_11     -0.106899\nS_eng_p_11      0.016640\nS_aca_p_11      0.026691\nS_saf_t_11      0.315796\nS_com_t_11      0.097797\nS_eng_t_11      0.045746\nS_aca_t_11      0.135490\nS_saf_s_11      0.286153\nS_com_s_11      0.157417\nS_eng_s_11      0.174425\nS_aca_s_11      0.292592\nS_saf_tot_11    0.292468\nS_com_tot_11    0.083068\nS_eng_tot_11    0.093489\nS_aca_tot_11    0.178388\nName: R_sat_score, dtype: float64\n\n\nThe variable names are abbreviated, so it is difficult to understand them. However, the data dictionary explains their meanings.\nThe most important ones are from S_saf_p_11 downwards. These variables represent scores of 4 aspects of each school:\n\nSafety and Respect\nCommunication\nEngagement\nAcademic Expectations\n\nEach aspect has 4 scores based on the respondent group: parents, teachers, students, and total (all groups).\nFor example, S_saf_tot_11 refers to the total Safety and Respect score of a school. It had a coefficient of 0.29. Let’s see a scatterplot of this.\n\ncombined.plot.scatter(\n    x = \"S_saf_tot_11\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs Safety and Respect Score (Total)\")\nplt.xlabel(\"Safety and Respect Score (Total)\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe correlation is not strong, as many schools have SAT scores under 1400 regardless of Safety and Respect score.\nThe schools which did have high scores (above 1500) had safety scores ranging from 7.25 to 8.75. This suggests that having sufficient safety and respect in a school may support the learning experience, though it does not always increase SAT scores.\nAnother interesting variable would be S_aca_s_11, the Academic Expectations score based on student responses. It had a coefficient of 0.29.\n\ncombined.plot.scatter(\n    x = \"S_aca_s_11\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs Academic Expectations Score (Students)\")\nplt.xlabel(\"Academic Expectations Score (Students)\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nAgain, the correlation is not strong.\nIt’s interesting to note, however, that none of the schools with an Academic Expectations score under 7.5 had SAT scores above 1600. We could say that a school must first have proper academic expectations in order to open the opportunity to excel.\nIn terms of strong conclusions, though, we can’t make any yet."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#demographics",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#demographics",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Demographics",
    "text": "Demographics\nLastly, we go to the demographics dataset variables. These are the most important variables to investigate for this project.\nHere are the correlation coefficients:\n\nfilter_prefix(sat_corr, r\"[D]\")\n\nD_frl_percent        -0.718163\nD_total_enrollment    0.385741\nD_ell_num            -0.130355\nD_ell_percent        -0.379510\nD_sped_num            0.058782\nD_sped_percent       -0.420646\nD_asian_num           0.483939\nD_asian_per           0.552204\nD_black_num           0.043408\nD_black_per          -0.302794\nD_hispanic_num        0.052672\nD_hispanic_per       -0.363696\nD_white_num           0.460505\nD_white_per           0.646568\nD_male_num            0.345808\nD_male_per           -0.107624\nD_female_num          0.403581\nD_female_per          0.107676\nName: R_sat_score, dtype: float64\n\n\nThe results can be divided into general topics:\n\nSchool size: The total_enrollment has a coefficient of 0.39. This means that bigger schools may have higher SAT scores.\nEnglish proficiency: The coefficient for ELL percentage is -0.38.\n\nELL means English Language Learner. It refers to students who are learning as a second or third language because they did not grow up with it.\n\nSpEd percentage: The coefficient for SpEd percentage is -0.42.\n\nSpEd means Special Education. It refers to educational plans with special accommodations for students who are disabled or have developmental disorders.\n\nSex: The following are the coefficients of the percentage of each sex:\n\nMale: -0.11\nFemale: 0.11\n\nRace: The following are the coefficients related to the percentage of students belonging to specific races.\n\nAsian: 0.55\nBlack: -0.30\nHispanic: -0.36\nWhite: 0.65\n\nSocio-economic status: The coefficient for FRL percentage -0.72.\n\nFRL means Free or Reduced-Price Lunch. This is a program that supports students below a certain income threshold who do not have the means to bring their own lunch to school.\n\n\nWe will go through each topic one by one.\n\nSchool Size\nBelow, we plot a scatter plot of SAT scores against the total number of enrollees in each school.\n\ncombined.plot.scatter(\n    x = \"D_total_enrollment\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs Total Enrollment\")\nplt.xlabel(\"Total Enrollment\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe scatter plot doesn’t have a clear linear trend. Instead, the majority of points are clustered in one area, and the rest of the points are scattered.\nLet’s inspect the names of the schools at the very bottom left, which have SAT scores under 1000 and total enrollment under 1000.\n\ncombined.loc[\n    (combined[\"R_sat_score\"] &lt; 1000) & (combined[\"D_total_enrollment\"] &lt; 1000),\n    \"R_SCHOOL NAME\"\n]\n\n102                 INTERNATIONAL COMMUNITY HIGH SCHOOL\n138                 ACADEMY FOR LANGUAGE AND TECHNOLOGY\n139                     BRONX INTERNATIONAL HIGH SCHOOL\n153               KINGSBRIDGE INTERNATIONAL HIGH SCHOOL\n155               INTERNATIONAL SCHOOL FOR LIBERAL ARTS\n197    PAN AMERICAN INTERNATIONAL HIGH SCHOOL AT MONROE\n199                       HIGH SCHOOL OF WORLD CULTURES\n210                  BROOKLYN INTERNATIONAL HIGH SCHOOL\n241                                 PACIFIC HIGH SCHOOL\n251       INTERNATIONAL HIGH SCHOOL AT PROSPECT HEIGHTS\n266                          IT TAKES A VILLAGE ACADEMY\n285                           MULTICULTURAL HIGH SCHOOL\n314                ASPIRATIONS DIPLOMA PLUS HIGH SCHOOL\n320              PAN AMERICAN INTERNATIONAL HIGH SCHOOL\nName: R_SCHOOL NAME, dtype: object\n\n\nInterestingly, the schools with the lowest average SAT scores are mostly small international schools. These schools tend to have more students from different racial and ethnic backgrounds. These students may speak English as a second or third language.\nTherefore, the factor that affected SAT performance may have been the percentage of English language learners (ELL) in each school. This makes sense since the SAT is offered only in English and includes sections on Reading and Writing.\n\n\nEnglish Proficiency\nLet’s plot SAT scores against the percentage of ELLs (English Language Learners) in each school.\n\ncombined.plot.scatter(\n    x = \"D_ell_percent\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs ELL Percentage\")\nplt.xlabel(\"ELL Percentage\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nOnce again, there isn’t a linear trend even though the correlation coefficient between the variables was -0.38. However, we can note that:\n\nMost schools have less than 50% ELLs and less than 1400 SAT points.\nThe schools with the highest SAT scores (over 1500) have almost 0% ELLs.\nAll of the schools with over 50% ELLs have very low scores (under 1300).\n\nTherefore, it appears that ELL students are more challenged in reaching high SAT scores.\n\n\nSpEd Percentage\nThe coefficient for SpEd percentage was -0.42, which indicates that schools with more SpEd students may have lower SAT scores. Let’s view a scatter plot:\n\ncombined.plot.scatter(\n    x = \"D_sped_percent\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs SpEd Percentage\")\nplt.xlabel(\"SpEd Percentage\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe correlation is somewhat linear, but it is not very steep. The points are very scattered, especially on the left side. Interestingly, the schools with an SAT score over 1600 had 15% or less SpEd students.\nHowever, the variable doesn’t predict SAT score well.\n\n\nSex\nAnother factor that’s important to investigate is sex. Do schools with more male students or more female students have higher SAT scores?\nEarlier, we saw that the male and female percentages had weak correlations with SAT score.\n\nMale: -0.11; lower SAT scores.\nFemale: 0.11; higher SAT scores.\n\nHowever, these correlations are weak, so the difference may not be that great.\nLet’s use the female_per variable in a scatter plot.\n\ncombined.plot.scatter(\n    x = \"D_female_per\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs Female Percentage\")\nplt.xlabel(\"Female Percentage\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nAs expected, the correlation is weak. We can see that:\n\nIn most schools, 40% to 60% of students are female.\nSAT score is typically below 1400 regardless of female percentage.\nThe schools with the highest SAT scores (above 1500) have a 30% to 75% female population.\n\nThis variable doesn’t seem to be very important to SAT score.\n\n\nRace\nEarlier, we saw that the following were the coefficients for race percentage variables. Positive values imply that a higher percentage of the given race may lead to higher average SAT score. Negative values: lower score.\n\nAsian: 0.55\nBlack: -0.30\nHispanic: -0.36\nWhite: 0.65\n\nOut of all 4, the white percentage value has the strongest correlation, 0.65. This means that having more white students in a school can lead to a higher average SAT score. Let’s see a scatterplot of this.\n\ncombined.plot.scatter(\n    x = \"D_white_per\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs White Percentage\")\nplt.xlabel(\"White Percentage\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThough the correlation coefficient was stronger than the others, the trend is loose; the points are scattered far away from each other. We can see that:\n\nMost of the schools in the dataset have SAT scores under 1400 and white percentage under 20%.\nThe schools with the highest scores have white percentages ranging from under 10% to over 60%.\n\nThus, we can’t say that a higher white percentage definitely raises the average SAT score.\nSince the other race-related variables are weaker than this one, we won’t investigate them further. Let’s move on to the topic of socio-economic status.\n\n\nSocio-economic status\nAs mentioned earlier, there was a variable representing the percentage of students availing of FRL (Free or Reduced-Price Lunch). Since FRL is for students below an income threshold, we can consider it similar to a poverty rate.\nThe correlation coefficient for this variable is the strongest at -0.72. The FRL students may have lower SAT scores due to external difficulties associated with low socio-economic status which hinder their education.\nBelow is the scatterplot.\n\ncombined.plot.scatter(\n    x = \"D_frl_percent\",\n    y = \"R_sat_score\",\n)\n\nplt.title(\"SAT Score vs FRL Percentage\")\nplt.xlabel(\"FRL Percentage\")\nplt.ylabel(\"Average SAT Score\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThis graph shows a tighter, more linear relationship than was seen in the other graphs.\nAt the top left of the graph are schools with low FRL percentage and high average SAT score. There are still a few schools with an SAT score over 1600 despite an FRL percentage of over 50%.\nHowever, for the majority of the schools, the FRL percentage is over 60%. All of these schools have an SAT score less than 1500.\nGenerally, the results suggest that socio-economic status is the strongest predictor of average SAT score among all variables in the dataset. High poverty rate is associated with a lower average SAT score."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#multicollinearity",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#multicollinearity",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nMulticollinearity occurs when multiple predictor variables are highly correlated. Variables with high multicollinearity have unreliable significance values. Thus, this is a problem that must be avoided.\nThe first way to check for multicollinearity is with a Pearson’s correlation coefficient table. The code below plots this table using a heatmap for easier interpretation.\n\n# Create a correlation matrix from X.\ncorr = X.corr()\n\n# Create a mask that selects the bottom left triangle of the table.\ncorr = corr.iloc[1:, :-1]\nmask = np.triu(         # Replace the lower triangle with 0's.\n    np.ones_like(corr), # Make a DataFrame of 1's with the same shape as corr.\n    k = 1,              # Raise the diagonal by 1.\n)\n\n# Plot a heatmap of the values.\nplt.figure(figsize = (12, 7))\n\nax = sns.heatmap(\n    corr,\n    vmin = -1, # Anchor colors on -1 and 1.\n    vmax = 1,\n    cbar = True, # Draw a color bar.\n    cmap = \"RdBu\", # Use Red for negatives and Blue for positives.\n    mask = mask, # Use a mask to remove the upper right triangle of the matrix.\n    annot = True, # Write the data value in each cell.\n)\n\n# Format the text in the plot to make it easier to read.\nfor text in ax.texts:\n    coeff = float(text.get_text())\n    \n    # Remove data values with low correlation.\n    if np.abs(coeff) &lt; 0.3: \n        text.set_text('')\n     # Round all visible data values to the hundredths place.\n    else:\n        text.set_text(round(coeff, 2))\n\n    text.set_fontsize(\"large\")\n    \nplt.title(\"Correlation Heatmap of Independent Variables\")\n    \n# Rotate x-ticks to read from top to bottom.\nplt.xticks(rotation = -30, ha = \"left\", size = \"large\") \nplt.yticks(rotation = 0, size = \"large\")\n\nplt.show()\n\n\n\n\nIn the heatmap above, blue colors represent positive correlation; red represents negative correlation. Colors with more saturation indicate that the correlation is stronger.\nI removed most of the coefficient labels except for the ones greater than 0.3, which show somewhat strong correlations. Interestingly, we can see that:\n\nMale percentage has a perfect negative correlation with female percentage. This is to be expected since both must sum up to 100.\nHispanic percentage is correlated with black percentage and FRL percentage.\nWhite percentage is strongly negatively correlated with FRL percentage, indicating that schools with more white students also have fewer poor students.\n\nAnother way to assess multicollinearity is to use variance inflation factors or VIFs. These are numbers greater than or equal to 1.\nA VIF less than 5 suggests low to moderate collinearity. If a predictor’s VIF is at least 5, this indicates high multicollinearity, so the significance value is not reliable.\nThe code below produces a VIF for each predictor.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as sm_vif\n\ndef get_vif(X):\n\n    \"\"\"Get a VIF value for each column in a DataFrame.\"\"\"\n    \n    vif_df = pd.DataFrame() \n    vif_df[\"label\"] = X.columns\n    vif_df[\"VIF\"] = [\n        sm_vif(X.values, i)\n        for i in range(len(X.columns))\n    ]\n    vif_df.set_index(\"label\", inplace = True)\n\n    return vif_df\n\nget_vif(X)\n\n\n\n\n\n\n\n\nVIF\n\n\nlabel\n\n\n\n\n\nD_ell_percent\n2.874752\n\n\nD_sped_percent\n8.509117\n\n\nD_female_per\n2578.305578\n\n\nD_male_per\n2569.217298\n\n\nD_asian_per\n304.520086\n\n\nD_black_per\n2196.055978\n\n\nD_hispanic_per\n2413.721821\n\n\nD_white_per\n231.186584\n\n\nD_frl_percent\n60.692762\n\n\n\n\n\n\n\nThese values are problematic. Almost all of the VIFs indicate strong multicollinearity except for that of D_ell_percent. This is likely due to the strong correlations we saw earlier in the heatmap.\nTherefore, we must remove a few variables with high multicollinearity.\n\nI choose to remove male_per since it has -1.0 correlation with female_per, which makes it redundant.\nI choose to remove hispanic_per because it has over 0.5 correlation with black_per and frl_percent.\nI choose to remove frl_percent since it has over 0.3 correlation with 4 other variables, including 0.7 correlation with white_per.\n\nThus, below is the new list of predictors.\n\niv_list = [\n    \"D_ell_percent\",\n    \"D_sped_percent\",\n    \"D_female_per\",\n    \"D_asian_per\",\n    \"D_black_per\",\n    \"D_white_per\",\n]\n\nregression_data = (\n    combined[[\"DBN\", \"R_sat_score\"] + iv_list]\n    .set_index(\"DBN\")\n)\n\nX = regression_data[iv_list].copy()\ny = regression_data[\"R_sat_score\"].copy()\n\nvif_df = get_vif(X)\nvif_df\n\n\n\n\n\n\n\n\nVIF\n\n\nlabel\n\n\n\n\n\nD_ell_percent\n1.600755\n\n\nD_sped_percent\n3.824473\n\n\nD_female_per\n7.289758\n\n\nD_asian_per\n1.788134\n\n\nD_black_per\n3.926733\n\n\nD_white_per\n1.777499\n\n\n\n\n\n\n\nWe can see that most of the VIFs are less than 5, so the p-values in the model will be reliable. Only the female_per variable’s p-value will be unreliable. However, I will still include this in the model since the percentages of sexes in a school may have an effect on SAT performance."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#iid-errors-and-normality-of-residuals",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#iid-errors-and-normality-of-residuals",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "IID Errors and Normality of Residuals",
    "text": "IID Errors and Normality of Residuals\nNext, we will be testing two more assumptions:\n\nIID (identically and independently distributed) residuals\n\nThis is tested using the Durbin-Watson test statistic, which ranges from 0 to 4.\nIdeally, the statistic should be close to 2.\nAs a rule of thumb, values from 1.5 to 2.5 are considered acceptable.\n\nNormality of residuals\n\nThis is tested using the Jarque-Bera test.\nThe null hypothesis is that the residuals are normally distributed.\nA statistically significant value (p &lt; 0.05) supports the claim that the residuals are not normal.\n\n\nIn order to test these, we must fit the model and look at its results. statsmodels automatically calculates both the Durbin-Watson and the Jarque-Bera tests for us.\n\n# OLS model\n\n# Add a constant to the X matrix.\n# This is done so that the y-intercept is not locked at 0.\nX = sm.add_constant(X)\n\n# Make and fit the OLS model.\nmodel = sm.OLS(y, X)\nresults = model.fit()\nsummary = results.summary()\n\n# Extract summary tables from OLS results summary.\ndef extract_summary(sm_summary):\n\n    \"\"\"Take a statsmodels summary instance and return a list of DataFrames.\"\"\"\n\n    tables = []\n    t_inds = list(range(3))\n\n    for i in t_inds:\n        table_as_html = sm_summary.tables[i].as_html()\n\n        table_df = pd.read_html(\n            table_as_html,\n            header = (0 if i == 1 else None), # Only set a header for table 2.\n            index_col = 0,\n        )[0]\n\n        tables.append(table_df)\n\n        if i == 1:\n            # Combine summary table 1 with the VIF column.\n            table_df = pd.concat([table_df, vif_df], axis = 1)\n            table_df.rename_axis(\"label\", inplace = True)\n\n        else:\n            # For tables 0 and 2, turn the index back into a column.\n            table_df = tables[i].reset_index()\n\n        tables[i] = table_df\n\n    return tables\n\ntables = extract_summary(summary)\n\n# Display the third table.\ntables[2]\n\nC:\\Users\\migs\\anaconda3\\envs\\new_streamlit_env2\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nOmnibus:\n28.251\nDurbin-Watson:\n1.660000e+00\n\n\n1\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n6.075100e+01\n\n\n2\nSkew:\n0.378\nProb(JB):\n6.430000e-14\n\n\n3\nKurtosis:\n4.751\nCond. No.\n5.040000e+02\n\n\n\n\n\n\n\nThe Durbin-Watson test statistic is 1.66. This is less than 2, so it indicates positive autocorrelation among residuals. (That is, each residual has a positive correlation with the residual before it). However, it falls within the acceptable range (1.5 to 2.5), so it is not a cause for concern.\nThe Jarque-Bera test significance, on the other hand, is \\(6.43 \\cdot 10^{-14}\\), which is very close to 0. It is statistically significant, so the residuals are not normally distributed. However, since the sample size (401) is greater than 30, OLS regression is robust against deviations from normality. Thus, this is not a cause for concern either.\nLet’s plot a histogram of residuals to inspect the deviation from normality, just to be sure.\n\n#%% Make a histogram of residuals.\n\nresiduals = results.resid\n#bin_edges = get_bin_edges(residuals)\n\nplt.hist(residuals, )#bins = bin_edges)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residual Error\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\n\nplt.show()\n\n\n\n\nThough this isn’t normal, it is still somewhat bell-shaped. Most of the data converges around a residual error of 0, with fewer values going farther away. Thus, the distribution isn’t problematic.\nWe can also visualize normality of residuals using a Q-Q (quantile-quantile) plot.\n\n# Plot a Q-Q plot.\n\nsm.qqplot(\n    data = residuals,\n    line = \"r\",\n    # Regression line\n)\nplt.title(\"Q-Q Plot of Residuals\")\nplt.legend(\n    labels = [\"Actual data\", \"Theoretical quantiles line\"],\n    loc = \"best\",\n)\nplt.grid(True)\n\nplt.show()\n\nC:\\Users\\migs\\anaconda3\\envs\\new_streamlit_env2\\lib\\site-packages\\statsmodels\\graphics\\gofplots.py:993: UserWarning: marker is redundantly defined by the 'marker' keyword argument and the fmt string \"bo\" (-&gt; marker='o'). The keyword argument will take precedence.\n  ax.plot(x, y, fmt, **plot_style)\n\n\n\n\n\nThe red line is our reference for normality. The blue dots should ideally follow this line. We can see that they do, though not perfectly, and the data are less normal at the higher quantiles. Generally, though, the data is close enough to being normal.\nLastly, for IID residuals, we can plot the residuals against the fitted response variable.\n\n# Plot residuals against fitted DV values.\n\nfitted = results.fittedvalues\nresiduals = results.resid\n\n# Horizontal line on the x axis.\nplt.axhline(color = \"r\", label = \"Fitted SAT Score\")\n\n# Scatter plot.\nplt.scatter(fitted, residuals, label = \"Residual Error\")\n\nplt.title(\"Residuals against Fitted SAT Score\")\nplt.xlabel(\"Fitted SAT Score\")\nplt.ylabel(\"Residual Error\")\nplt.legend(loc = \"best\")\nplt.grid(True)\n\nplt.show()\n\n\n\n\nIn this graph, we ideally want to see the points cluster uniformly around the red line, without any trends.\nHowever, if we follow the outline of the general mass of points, these seem to follow a curve. However, the points on the right side are loosely scattered, so we cannot be sure whether there is truly a curve or it is merely due to outliers.\nAnyway, we saw earlier that the Durbin-Watson test statistic was within the acceptable range, so the distribution of residuals is alright. We can move on to the next step."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#linearity",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#linearity",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Linearity",
    "text": "Linearity\nThe next assumption that we will test is linearity. The predictors must be linearly correlated with the response variable; the relationship should not be too curved or abnormal.\nWe can test this visually by plotting the actual average SAT scores against the fitted scores which were predicted by our model.\n\n# Plot actual SAT scores against fitted SAT scores.\n\nfitted = results.fittedvalues\n\n# Scatter plot\nplt.scatter(fitted, y, label = \"Actual data\")\n\n# Use OLS model results to get trend line.\n\nx_fitting = sm.add_constant(fitted)\ntrend_model = sm.OLS(y, x_fitting)\ntrend_results = trend_model.fit()\n\ntrend_params = trend_results.params\ntrend_const = trend_params[\"const\"]\ntrend_coeff = trend_params[0]\ny_plot = trend_coeff * fitted + trend_const\n\nplt.plot(fitted, y_plot, color = \"r\", label = \"Trend line\")\n\n\n# Extra plot info\n\nplt.title(\"\"\"Actual SAT scores against Fitted SAT scores\"\"\")\nplt.xlabel(\"Fitted SAT Score\")\nplt.ylabel(\"Actual SAT Score\")\nplt.legend(loc = \"best\")\nplt.grid(True)\n\nplt.show()\n\nC:\\Users\\migs\\anaconda3\\envs\\new_streamlit_env2\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\n\n\nThe trend line above fits the actual data quite well. Moreover, the actual data seems to follow a mostly linear shape. Thus, there is no cause for concern here."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#homoscedasticity",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#homoscedasticity",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Homoscedasticity",
    "text": "Homoscedasticity\nThe last assumption that we will test is homoscedasticity. This means that the variance is consistent across all values of residuals. If this is not the case, the model has heteroscedasticity, which is problematic.\nThis can be tested using the Breusch-Pagan test. The null hypothesis is that heteroscedasticity is not present (which is ideal). A statistically significant p-value supports the claim that there is heteroscedasticity, in which case we would have to adjust the model.\n\n# Breusch-Pagan test.\n\nb_labels = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\nb_test = sms.het_breuschpagan(\n    residuals,\n    model.exog,\n)\n\nb_results = pd.DataFrame(zip(b_labels, b_test))\nb_results.columns = [\"Information\", \"Value\"]\n\nb_results\n\n\n\n\n\n\n\n\nInformation\nValue\n\n\n\n\n0\nLagrange multiplier statistic\n1.028960e+02\n\n\n1\np-value\n6.237518e-20\n\n\n2\nf-value\n2.266603e+01\n\n\n3\nf p-value\n5.711239e-23\n\n\n\n\n\n\n\nBoth p-values have a factor of around \\(10^{-20}\\). In other words, these are very close to 0, which means that these are significant. Thus, the model is heteroscedastic.\nWe can make the results robust against heteroscedasticity by specifying an HCCM (Heteroscedasticity Consistent Covariance Matrix). We will specifically choose HC0. It doesn’t perform well for sample sizes under 250 (Python for Data Science), but our sample size is 401, so we can use it.\n\n# Make and fit the OLS model.\nmodel = sm.OLS(y, X)\nresults = model.fit(\n    cov_type = 'HC0', # Specify an HCCM.\n)\nsummary = results.summary()\ntables = extract_summary(summary)\n\nNow that we’ve tested all of the assumptions and adjusted for heteroscedasticity, we can move on to partial regression plots."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#partial-regression-plots",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#partial-regression-plots",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Partial regression plots",
    "text": "Partial regression plots\nPartial regression plots are used to visualize the relationship between 1 predictor and the response variable while holding all other predictors constant.\nThe plots are made below.\n\n#%% Partial regression plots.\n    \ndef par_reg_plot(data, label, show_dbn = False):\n    \n    \"\"\"Take a predictor label and plot its partial regression plot, which takes the other predictors into account.\n    Note: This function references objects from the global namespace.\"\"\"\n    \n    title = \"Partial Regression Plot\\nSAT Score against `{}`\".format(label)\n    x_label = \"`{}`\\nPartial Residuals\".format(label)\n    \n    # List of all IVs other than the current one.\n    others = [col for col in iv_list if col != label]\n    \n    fig = sm.graphics.plot_partregress(\n        endog = \"R_sat_score\",\n        exog_i = label,\n        exog_others = others,\n        data = data,\n        obs_labels = show_dbn,\n    )\n    \n    plt.title(title)\n    plt.xlabel(x_label)\n    plt.ylabel(\"SAT Score\\nPartial Residuals\")\n    plt.grid(True)\n    plt.legend(\n        labels = [\"Actual partial residuals\", \"Trend line\"],\n        loc = \"best\",\n    )\n    plt.tight_layout(\n        h_pad = 2,\n    )\n    \n    plt.show()\n    \nfor label in iv_list:\n    par_reg_plot(regression_data, label)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can observe that:\n\nEnglish Language Learner percentage and Special Education percentage fit the linear trend best.\nFemale percentage residuals all cluster around one spot and don’t follow a trend.\nThe other variables, which involve race percentages, seem to be affected by outliers.\n\nWe should keep these plots in mind as we interpret the model results since these don’t show whether outliers may have led to an overestimation of the significance of a predictor."
  },
  {
    "objectID": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#interpretation-of-results",
    "href": "posts/2021-06-13-how-student-demographic-affects-sat-performance-nyc.html#interpretation-of-results",
    "title": "How Student Demographic Affects SAT Performance in NYC",
    "section": "Interpretation of Results",
    "text": "Interpretation of Results\nBelow is the first table of model results.\n\ntables[0]\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nDep. Variable:\nR_sat_score\nR-squared:\n7.860000e-01\n\n\n1\nModel:\nOLS\nAdj. R-squared:\n7.820000e-01\n\n\n2\nMethod:\nLeast Squares\nF-statistic:\n9.644000e+01\n\n\n3\nDate:\nMon, 13 Dec 2021\nProb (F-statistic):\n3.410000e-74\n\n\n4\nTime:\n10:27:39\nLog-Likelihood:\n-2.330700e+03\n\n\n5\nNo. Observations:\n401\nAIC:\n4.675000e+03\n\n\n6\nDf Residuals:\n394\nBIC:\n4.703000e+03\n\n\n7\nDf Model:\n6\nNaN\nNaN\n\n\n8\nCovariance Type:\nHC0\nNaN\nNaN\n\n\n\n\n\n\n\nMultiple regression analysis was used to test if a school’s percentages of English Language Learners, Special Education students, female students Asian students, black students, and white students significantly predicted the school’s average total SAT score.\nThe six predictors explained 78.6% of the variance (R2= 0.786, F(6, 394) = 96.44, p &lt; 0.01).\nNext is the second table of results, which shows the beta coefficient, p-value, etc. for each predictor variable.\n\ntables[1]\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\nVIF\n\n\nlabel\n\n\n\n\n\n\n\n\n\n\n\nconst\n1430.1978\n32.665\n43.784\n0.00\n1366.175\n1494.220\nNaN\n\n\nD_ell_percent\n-5.2206\n0.320\n-16.290\n0.00\n-5.849\n-4.592\n1.600755\n\n\nD_sped_percent\n-8.6750\n0.861\n-10.076\n0.00\n-10.362\n-6.988\n3.824473\n\n\nD_female_per\n-0.8097\n0.316\n-2.564\n0.01\n-1.429\n-0.191\n7.289758\n\n\nD_asian_per\n3.5967\n0.524\n6.861\n0.00\n2.569\n4.624\n1.788134\n\n\nD_black_per\n-1.3553\n0.182\n-7.446\n0.00\n-1.712\n-0.999\n3.926733\n\n\nD_white_per\n3.8196\n0.604\n6.329\n0.00\n2.637\n5.002\n1.777499\n\n\n\n\n\n\n\nThe predicted average SAT score in a school is approximately equal to 1430 - 5.22(ELL percentage) - 8.68(SpEd percentage) - 0.81(female percentage) + 3.60(Asian percentage) - 1.36(black percentage) + 3.82(white percentage).\nAll of the independent variables used in the model were significant predictors of the school’s average SAT score, with p &lt; 0.05. However, since the female percentage variable’s VIF value was greater than 5, strong multicollinearity was present, so its significance value may not be reliably interpreted.\nThe coefficients for ELL percentage, SpEd percentage, and black student percentage were negative. This means that these factors are associated with lower average SAT score in an NYC high school.\nOn the other hand, the coefficients for Asian percentage and white percentage were positive. This means that these factors are associated with higher average SAT score in an NYC high school. However, the linear relationship in the partial regression plots appeared to be affected by outliers, so the effect of these variables may not be strong for the majority of high schools.\nIn general, however, these results present initial evidence that race percentages in an NYC high school may significantly affect its average SAT score."
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#feature-selection",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#feature-selection",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "Feature Selection",
    "text": "Feature Selection\nI must select features that have a useful linear relationship with the number of bike rentals (cnt). The code cell below lists the initial list of features that I will consider. I will not use casual and registered since these are systematically related to cnt.\n\nconsidered_features = pd.Series([\n    \"season\",\n    \"yr\",\n    \"mnth\",\n    \"hr\",\n    \"time_label\",\n    \"holiday\",\n    \"weekday\",\n    \"workingday\",\n    \"weathersit\",\n    \"temp\",\n    \"atemp\",\n    \"hum\",\n    \"windspeed\",\n])\n\nFirst, I can find the Pearson’s correlation coefficient between each variable and the target.\n\n\nCode\ntarget_col = \"cnt\"\n\n# Correlation coefficient of each variable with cnt\ncnt_corr = (\n    data\n    .loc[:, considered_features.tolist() + [target_col]]\n    .corr()\n    .loc[:, \"cnt\"]\n    .drop(index = \"cnt\")\n    # Sort by distance from 0\n    .sort_values(key = np.abs, ascending = False)\n)\n\nprint(\"Correlation Coefficient of each Variable with `cnt`\")\ncnt_corr\n\n\nCorrelation Coefficient of each Variable with `cnt`\n\n\ntemp          0.404772\natemp         0.400929\nhr            0.394071\ntime_label   -0.378318\nhum          -0.322911\nyr            0.250495\nseason        0.178056\nweathersit   -0.142426\nmnth          0.120638\nwindspeed     0.093234\nholiday      -0.030927\nworkingday    0.030284\nweekday       0.026900\nName: cnt, dtype: float64\n\n\nThe coefficients have been ordered by distance from 0, descending. Coefficients farther from 0 indicate stronger correlation.\nIt appears that the temperature (temp) and feeling temperature (atemp) have the strongest correlation with bike rentals, so it is likely that these will be significant predictors.\nAs for the other variables, I will not drop any of them based solely on their correlation coefficient. This decision may seem strange since some variables have very weak correlations, even less than 0.10. However, one must note that these coefficients are the results of univariate tests. It is still possible for a variable to become significant when the effects of other variables are taken into consideration in multiple linear regression. Furthermore, the sample size is large (n = 17379), so there is little risk of overfitting from having too many predictors.\nHowever, I do have to drop predictors if they have multicollinearity issues. Thus, I have generated a correlation heatmap to inspect correlations among predictors.\n\n\nCode\nlrt.correlation_heatmap(data[considered_features.tolist() + [target_col]].corr())\n\n\n\n\n\nIn the heatmap above, blue represents positive correlations, red represents negative correlations, and darker shades represent stronger correlations.\nIt appears that temp and atemp are highly correlated (0.99). Thus, I will keep atemp since it is the “feeling” temperature (temperature perceived by humans).\nFurthermore, mnth and season have a coefficient of 0.83. Thus, I will drop season because it is less specific compared to mnth.\nLastly, weathersit and hum (humidity) have a coefficient of 0.42. Thus, I will drop weathersit because hum has a higher correlation with the target, cnt.\nBy dropping the mentioned variables, I will avoid multicollinearity in the model.\n\n\nCode\nconsidered_features = considered_features.loc[~considered_features.isin([\"temp\", \"season\", \"weathersit\"])]\n\nconsidered_features\n\n\n1             yr\n2           mnth\n3             hr\n4     time_label\n5        holiday\n6        weekday\n7     workingday\n10         atemp\n11           hum\n12     windspeed\ndtype: object\n\n\nAbove is the final list of variables that I will use in linear regression."
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#statistical-inference",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#statistical-inference",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nNow, I will perform statistical inference with the statsmodels package to check for significance values and other assumptions of linear regression.\n\n\nCode\nX = sms.add_constant(data[considered_features])\ny = data[target_col]\n\nvif_df = lrt.get_vif(X)\n\nmodel = sms.OLS(y, X)\nresults = model.fit()\nsummary = results.summary()\ntables = lrt.extract_summary(summary, vif_df)\n\ntables[0]\n\n\nC:\\Users\\migs\\anaconda3\\envs\\new_streamlit_env2\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nDep. Variable:\ncnt\nR-squared:\n0.463\n\n\n1\nModel:\nOLS\nAdj. R-squared:\n0.463\n\n\n2\nMethod:\nLeast Squares\nF-statistic:\n1497.000\n\n\n3\nDate:\nSun, 16 Jan 2022\nProb (F-statistic):\n0.000\n\n\n4\nTime:\n13:13:59\nLog-Likelihood:\n-109640.000\n\n\n5\nNo. Observations:\n17379\nAIC:\n219300.000\n\n\n6\nDf Residuals:\n17368\nBIC:\n219400.000\n\n\n7\nDf Model:\n10\nNaN\nNaN\n\n\n8\nCovariance Type:\nnonrobust\nNaN\nNaN\n\n\n\n\n\n\n\nAbove is the first table of results provided by the statsmodels package. Unfortunately, though the model’s F-statistic is significant (p &lt; 0.05), the R-squared value is 0.463. Therefore, the model only explains 46.3% of the variance in the data. This ideally should be close to 100%.\n\n\nCode\ntables[1]\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\nVIF\n\n\nfeature\n\n\n\n\n\n\n\n\n\n\n\nconst\n104.2364\n6.885\n15.139\n0.000\n90.741\n117.732\n46.593076\n\n\nyr\n81.9409\n2.028\n40.405\n0.000\n77.966\n85.916\n1.010528\n\n\nmnth\n4.9372\n0.306\n16.130\n0.000\n4.337\n5.537\n1.088734\n\n\nhr\n6.4545\n0.155\n41.600\n0.000\n6.150\n6.759\n1.131082\n\n\ntime_label\n-46.7594\n0.928\n-50.394\n0.000\n-48.578\n-44.941\n1.052451\n\n\nholiday\n-24.0548\n6.269\n-3.837\n0.000\n-36.343\n-11.767\n1.079365\n\n\nweekday\n1.9334\n0.506\n3.819\n0.000\n0.941\n2.926\n1.013161\n\n\nworkingday\n3.8622\n2.243\n1.722\n0.085\n-0.535\n8.259\n1.071211\n\n\natemp\n329.9434\n6.103\n54.058\n0.000\n317.980\n341.907\n1.081206\n\n\nhum\n-179.8373\n5.760\n-31.224\n0.000\n-191.127\n-168.548\n1.213499\n\n\nwindspeed\n9.7759\n8.717\n1.121\n0.262\n-7.310\n26.862\n1.117727\n\n\n\n\n\n\n\nNext, above is the second table of model results, showing each predictor and its coefficients and other statistics. Notably, all of the variables were significant (p &lt; 0.05), except for workingday and windspeed.\nIt is also good that all of the predictors’ variance inflation factors (VIF) were close to 1 and lower than 5. (Do not mind the VIF of const, since this represents the constant term, not a predictor.) These values indicate low multicollinearity among the predictors.\n\n\nCode\ntables[2]\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nOmnibus:\n3471.487\nDurbin-Watson:\n0.663\n\n\n1\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n7064.939\n\n\n2\nSkew:\n1.192\nProb(JB):\n0.000\n\n\n3\nKurtosis:\n5.018\nCond. No.\n152.000\n\n\n\n\n\n\n\nLastly, the Jarque-Bera test statistic is significant, so the model violates the assumption of normality of residuals. Furthermore, the Durbin-Watson test statistic (0.663) is below the ideal range (1.5-2.5), indicating strong positive autocorrelation among residuals.\nOverall, due to the low R-squared value mentioned earlier and the violation of certain assumptions of linear regression, this model may not work very well for prediction. This will be tested in the next part."
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#predictive-modelling",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#predictive-modelling",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "Predictive Modelling",
    "text": "Predictive Modelling\nBefore making a predictive model, I must make training and testing sets. Given that the data is time-series data, it would be appropriate to ensure that all test set observations occur after the training set observations. This will allow me to test the models in a similar way to how they would be used in the real world: to predict the number of bike rentals at a given time in the future.\nThus, before I perform a train-test split, I must order the observations by dteday. This has been done below.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\ndata = data.sort_values(\"dteday\", ascending = True)\n\nX = sms.add_constant(data[considered_features])\ny = data[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    # When shuffle = False, the second part of the dataset is used as the test set.\n    shuffle = False,\n    test_size = 0.2,\n)\n\n# KDE plots\nsns.kdeplot(y_train)\nsns.kdeplot(y_test)\nplt.title(\"Comparison of Target Distributions\")\nplt.xlabel(\"Hourly Count of Bike Rentals\")\nplt.ylabel(\"Probability Density\")\nplt.legend([\"y_train\", \"y_test\"])\nplt.show()\n\n\nC:\\Users\\migs\\anaconda3\\envs\\new_streamlit_env2\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\n\n\nAfter the train-test split, it seems that the two sets have roughly similar distributions. Both are right-skewed, as the low values appear frequently and the outliers are high values.\nNow, we can use RMSE (Root Mean Squared Error) to evaluate the predictive model.\n\n\nCode\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\ndef compare_train_test_rmse(model, X_train, X_test, y_train, y_test):\n    y_pred = model.predict(X_train)\n    mse = mean_squared_error(y_train, y_pred)\n    rmse = np.sqrt(mse)\n\n    print(f\"Train set: RMSE = {rmse}\")\n\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n\n    print(f\"Test set: RMSE = {rmse}\")\n\nprint(\"Linear Regression\")\ncompare_train_test_rmse(lr, X_train, X_test, y_train, y_test)\n\n\nLinear Regression\nTrain set: RMSE = 121.87152363617122\nTest set: RMSE = 171.70884445091582\n\n\nBefore I interpret these results, please note the following terminology:\n\nRMSE: Root Mean Squared Error. It can be interpreted as the average distance of the predicted values from the real values. A lower value indicates better performance.\nTest set RMSE: The RMSE resulting from evaluating the model on the testing set. We use this to assess how well the model performs on previously unseen data.\nTrain set RMSE: The RMSE resulting from evaluating the model on the training set. Since the model already saw the training set while it was being trained, this value is not a useful metric of performance. However, it is helpful for determining whether overfitting may have occurred.\nOverfitting: The event in which the model has become too sensitive to the small changes in the training data. It is unable to identify the general patterns that help make accurate predictions. If the model performs much worse (higher RMSE) on the test set compared to the training set, this may indicate overfitting.\n\nThe test set RMSE is roughly 172, which means that on average, the model’s predicted counts of bike rentals were 172 off from the true counts. Let’s put that into perspective by looking at the distribution of the target variable.\n\nsns.histplot(data = data, x = \"cnt\")\nplt.title(\"Distribution of Target Variable in the Whole Dataset\")\nplt.xlabel(\"Hourly Number of Bike Rentals\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\nThe chart shows that the most common values in the distribution range between 0 and 400, and there are some outlying high values.\nLet’s say that the true value of one observation is equal to 200 bike rentals. Since the RMSE is 172, the predicted value may usually end up being around 28 or 372. Each of these predictions is very low or high given the range of the distribution. Therefore, an RMSE of 172 indicates very poor predictive ability.\nAlso, the test set RMSE is higher than the train set RMSE by around 50. Thus, the model has somewhat overfitted. It would be better if the test set RMSE is almost as low as the train set RMSE. This would indicate that the model is able to recognize general patterns.\nLater on, we’ll see how this model compares to the other two models."
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#a-high-level-explanation",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#a-high-level-explanation",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "A High-Level Explanation",
    "text": "A High-Level Explanation\nHere, I will briefly explain how decision trees work for regression problems, based on the Dataquest course and some articles by Sriram (2020) and Sayad (2022). Take note of the following terms:\n\nfeature, predictor: A variable used to make a prediction. Multiple features may be used in one model.\ntarget, response variable: The variable that the model attempts to predict.\n\nBefore a DT can make predictions, the tree must first be built. We start with one node, and each node has a conditional statement about a feature. For example, if the feature is temperature, a node may check whether the temperature is lower than 25 degrees Celsius. The “False” case splits off to the left branch, whereas the “True” case splits off to the right branch.\nTo determine the best feature to use in a split, the model uses an error metric. For example, one may use Mean Squared Error, Mean Absolute Error, Standard Deviation, etc. At each split, the model determines the split that will maximize error reduction. Reducing the error means reducing the spread of the target values, so that the mean value is close to the real values.\nThe tree keeps splitting nodes and branching off. Eventually, the tree reaches nodes where the error is very low. Thus, these nodes become leaf nodes. The mean target value of each node is used as the predicted value.\nWhen predictions are made on a new observation, the model starts at the root node, checks the conditional statement, moves to the appropriate branch, and repeats this process until a leaf node is reached. Then, the leaf node’s predicted value is the output.\nThe implication is that, unlike linear regression, the model does not assume that there are linear relationships between the predictors and the target. Rather, it narrows down the possible target values through process-of-elimination. Thus, it is able to find non-linear relationships. This makes DTs potentially more accurate than linear regression in some scenarios."
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#evaluating-a-dt-regressor",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#evaluating-a-dt-regressor",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "Evaluating a DT Regressor",
    "text": "Evaluating a DT Regressor\nLet’s now create and evaluate a DT regressor. For the features of the model, I decided to use the ones listed below.\n\nconsidered_features = pd.Series([\n    \"season\",\n    \"yr\",\n    \"mnth\",\n    \"hr\",\n    \"time_label\",\n    \"holiday\",\n    \"weekday\",\n    \"workingday\",\n    \"weathersit\",\n    \"temp\",\n    \"atemp\",\n    \"hum\",\n    \"windspeed\",\n])\n\nEarlier, I removed some features due to collinearity issues. However, collinearity is not an issue for Decision Trees because these are not like linear regression. That is why I have not removed the collinear features before performing Decision Tree regression.\nContinuing on, let us fit and evaluate the model.\n\n\nCode\ndata = data.sort_values(\"dteday\", ascending = True)\n\nX = data[considered_features]\ny = data[target_col]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    # When shuffle = False, the second part of the dataset is used as the test set.\n    shuffle = False,\n    test_size = 0.2,\n)\n\ntree = DecisionTreeRegressor(random_state = 0)\n\ntree.fit(X_train, y_train)\n\nprint(\"Decision Tree\")\ncompare_train_test_rmse(tree, X_train, X_test, y_train, y_test)\n\n\nDecision Tree\nTrain set: RMSE = 0.514411456857934\nTest set: RMSE = 86.61283689808975\n\n\nInterestingly, the test set RMSE is now roughly 87. This means that the predicted counts of bike rentals are around 87 away from the true counts. This error value is much lower (and therefore better) than that of the linear regression model shown earlier.\nHowever, the DT model also seems to suffer from overfitting. The test set RMSE is much higher than the train set RMSE (around 0.5).\nThus, we may need to adjust the parameters of the tree to keep it from growing too large. In the code cell below, I have built the tree again, but with constraints on:\n\nThe maximum depth (number of splits from root to leaf nodes)\nThe minimum number of samples required to split a node\n\n\n\nCode\ntree = DecisionTreeRegressor(\n    max_depth = 20,\n    min_samples_split = 20,\n    random_state = 0,\n)\n\ntree.fit(X_train, y_train)\n\nprint(\"Decision Tree\")\ncompare_train_test_rmse(tree, X_train, X_test, y_train, y_test)\n\n\nDecision Tree\nTrain set: RMSE = 33.94259741687692\nTest set: RMSE = 78.39525816583571\n\n\nThese results are slightly better than before. The test set RMSE is around 79, which is better than the previous value of 87. Furthermore, The train set RMSE and test set RMSE are closer to each other, so the model suffers less overfitting.\n(Note that the train set RMSE increased. This is not a bad thing; it only means that the model is less sensitive to small variations in the training data.)\nThough these results are decent, they can be improved through the use of a Random Forest."
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#data-source",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#data-source",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "Data Source",
    "text": "Data Source\nFanaee-T, H. (2013, December 20). Bike Sharing Dataset. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset"
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#information-sources",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#information-sources",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "Information Sources",
    "text": "Information Sources\nDataquest. (n.d.). Guided Project: Predicting Bike Rentals. Dataquest. https://app.dataquest.io/c/22/m/213/guided-project%3A-predicting-bike-rentals/1/introduction-to-the-dataset\nSayad, S. (2022). Decision Tree—Regression. SaedSayad.Com. https://saedsayad.com/decision_tree_reg.htm\nscikit-learn developers. (2021). Sklearn.tree.DecisionTreeRegressor. Scikit-Learn. https://scikit-learn/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\nSriram, A. (2020, June 5). Decision Tree for Regression—The Recipe. Analytics Vidhya. https://medium.com/analytics-vidhya/decision-tree-for-regression-the-recipe-74f7628b8a0"
  },
  {
    "objectID": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#image-source",
    "href": "posts/2022-01-16-comparison-regression-models-predicting-bike-rentals.html#image-source",
    "title": "Comparison of Regression Models for Predicting Bike Rentals",
    "section": "Image Source",
    "text": "Image Source\nMingot, S. (2020, January 14). Photo by Stéphane Mingot on Unsplash. Unsplash. https://unsplash.com/photos/e8msPzLTXxU"
  },
  {
    "objectID": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html",
    "href": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html",
    "title": "agriHanda: an Agricultural Disaster Risk Web App",
    "section": "",
    "text": "In June 2021, I read that there was an upcoming local competition in data science and analytics. This was the Project SPARTA Open Data Challenge, which was organized by the Development Academy of the Philippines (DAP) and DOST-PCIEERD. The Sparta Portal provided open data about Butuan City, which is the “commercial, industrial and administrative center” of the Caraga region (“Butuan City”, n.d.). Participants could then use this data to create cleaned datasets, datablogs, data journalism pieces, research papers, dashboards, predictive models, software applications, or visual storytelling pieces (“Approach to innovations”, n.d.).\nThus, I formed a team with three of my schoolmates (Fiona Jao, Lorenzo Layug, and Yuri Dolorfino) and we entered the competition as the “Datos Puti” team. We cleaned open data about agricultural disaster risk together, then I programmed agriHanda, an app which would visualize this data.\nIn the end, we were awarded as “Second Placer” and “Best in Ingenious Solution.” I was proud of this because this was the first data science competition that I had ever joined. Furthermore, despite being a senior high school student at the time, I was able to perform well against the other eleven competing teams, most of whom were from the college and professional levels.\nTo see our certificates, use this link: Certificates Folder\nTo read the DAP’s article about the competition and recognition ceremony, use this link: Winners of First-ever Hackathon and Open Data Challenge Recognized\nIn this post, I will talk about the web app, its strengths and points for improvement, and the data science skills that I honed along the way."
  },
  {
    "objectID": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#pandas-multiindex",
    "href": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#pandas-multiindex",
    "title": "agriHanda: an Agricultural Disaster Risk Web App",
    "section": "pandas MultiIndex",
    "text": "pandas MultiIndex\nRecall that during the data cleaning phase, I had to organize the variables into a hierarchy. Thus, I had to learn how to use the pandas MultiIndex.\nFor context, pandas is a Python package for manipulating tables of data, which are called DataFrames. A regular Index object is used to store the row labels or column labels of a DataFrame. In my case, each of the original data files included columns such as “Exposure”, “Sensitivity”, etc. Each row represented a unique barangay. Each file only gave data about a specific element and hazard (for example, fisheries and storm surge).\nBelow is a simplified example of what one file would look like.\n\n\n\nBarangay\nAdaptive Capacity Score\nVulnerability Score\n\n\n\n\nA\n2\n1.2\n\n\nB\n5\n0.8\n\n\n\nWhen I combined the files, I used MultiIndex to create a hierarchy which looked like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSector\nAgriculture\n\n\n\n…\n\n\n\n\n\n\nElement\nCrops\n\n\n\n\nFisheries\n\n\n\nHazard\nDrought\n\nFlood\n\n\nDrought\n\n\n\nDisaster Risk Aspect\nAdaptive Capacity\nOverall Risk\nAdaptive Capacity\nOverall Risk\n\nAdaptive Capacity\nOverall Risk\n\n\nDetail\nAdaptive Capacity Score\nVulnerability Score\nAdaptive Capacity Score\nVulnerability Score\n\nAdaptive Capacity Score\nVulnerability Score\n\n\nBarangay\n\n\n\n\n\n\n\n\n\nA\n2\n1.2\n5\n1\n\n4\n1.5\n\n\nB\n5\n0.8\n4\n1.1\n\n1\n0.6\n\n\n\nThe first 5 rows of the table above are part of the hierarchy of labels. The levels are Sector, Element, Hazard, Disaster Risk Aspect, and Detail. With this hierarchy, the variables can be navigated easily even if there are many of them.\nFor more information on MultiIndex, visit the pandas documentation."
  },
  {
    "objectID": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#streamlit-web-app-framework",
    "href": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#streamlit-web-app-framework",
    "title": "agriHanda: an Agricultural Disaster Risk Web App",
    "section": "Streamlit web app framework",
    "text": "Streamlit web app framework\nAnother thing that I practiced in this project was the Streamlit package. This provides a framework for building simple web apps in Python. One would first write a script which uses the Streamlit API to represent text, widgets, charts, and other visual elements.\nimport streamlit as st\n\n# App title\nst.title('My App')\n\n# Let the user select a variable from a drop-down list.\ninput_var = st.selectbox(\n    'Select a variable',\n    options = ['Exposure', 'Sensitivity', 'Adaptive Capacity', 'Vulnerability', 'Risk'],\n)\n\n# Make a chart using this variable.\n# chart = ...\n\n# Display the chart.\nst.altair_chart(chart)\nThen, when the app is run, the script is run from top to bottom. When the user changes something, the entire script runs again in order to update what is shown. In the case of the script above, the app would show a title, then a drop-down list, then a chart. If the user changes their selection in the drop-down list, the chart changes as well.\nWith this simple framework, one can quickly create an interface through which other people can interact with a dashboard or even a predictive model, so it will most likely be useful for future projects. For more information, visit the Streamlit website."
  },
  {
    "objectID": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#altair-package-for-data-visualization",
    "href": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#altair-package-for-data-visualization",
    "title": "agriHanda: an Agricultural Disaster Risk Web App",
    "section": "Altair package for data visualization",
    "text": "Altair package for data visualization\nAnother important skill I used for this project was making charts using Altair. This package allows me to create charts using a simple grammar which minimizes the amount of code needed. This usually involves the following steps:\n\nMake a Chart object and pass a DataFrame to it\nSpecify a mark type, which determines the shapes used to represent data. The options include bar, boxplot, line, area, point, etc.\nSpecify encodings. This means deciding which variables are used in the chart, and how so.\nSpecify properties. These may include the chart title, font size, text rotation, etc.\n\nYou can experience a simplified version of these steps by using the Graphing Tool in agriHanda.\nIn terms of code, an example would look like this:\nimport altair as alt\n\nchart = (\n    alt.Chart(df)\n    .mark_boxplot() # Make a boxplot\n    .encode(y = input_variable, type = 'quantitative') # Use the input variable on the y-axis\n    .properties(title = 'Boxplot') # Set a title\n    .interactive() # Make the chart interactive\n)\nApart from the basic steps, I also learned how to use bindings, selections, and conditions. These are advanced techniques which make charts more interactive. In my case, I used these to add a slider to the Map of Butuan City feature so that only barangays near a certain value would be highlighted. This allows the user to, for example, only highlight places with high exposure to a hazard.\n\n\n\n\n\n\nThe slider is set to only highlight barangays with the highest vulnerability score.\n\n\nFor more information about Altair, visit their documentation."
  },
  {
    "objectID": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#git-and-github-workflow",
    "href": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#git-and-github-workflow",
    "title": "agriHanda: an Agricultural Disaster Risk Web App",
    "section": "Git and GitHub workflow",
    "text": "Git and GitHub workflow\nAnother very important thing I learned was how to use basic git commands to manage my repository.\nOriginally, I created a blank repository, then dragged the files to GitHub and pressed “commit.” I committed files in this way every time that I made a change. I later learned, however, that this was not the most efficient way to do things.\nAt some point, I learned that I could clone my repository to my laptop. This means that the files would be downloaded to a folder in my laptop, which would serve as my workspace. After I edit something in my workspace, I can commit the changes to my local repository. Thus, a snapshot of my files is saved, and I can go back to old versions if I need to. I can then push the changes to my remote repository, which is usually on GitHub. Thus, the latest changes to the repository are stored online.\nIn the case of agriHanda, since the Streamlit app is deployed from the GitHub repository, I can easily change the app just by pushing to the repository.\nAdmittedly, I am not familiar with using git commands in the command line. I rely on GitHub Desktop and Visual Studio Code, which provide graphical user interfaces for git commands. However, I hope to learn how to use Git Bash eventually.\nThe next step I can take would be to study articles like this article by Fachat (2021), so that I can better understand how git works and how to write commands."
  },
  {
    "objectID": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#writing-a-readme-and-documentation",
    "href": "posts/2021-12-17-agriHanda-Agricultural-Disaster-Risk-Web-App.html#writing-a-readme-and-documentation",
    "title": "agriHanda: an Agricultural Disaster Risk Web App",
    "section": "Writing a README and Documentation",
    "text": "Writing a README and Documentation\nLastly, the agriHanda project was my first time writing a serious README and documentation for my repository. I mainly followed the GitHub guide for READMEs. I made sure to put only the most essential information in the README, including a brief description, a link tot he documentation, the sources of the open data used in the project, the names and roles of our team members, and a statement on the terms of use of the project.\nThen, in the documentation, I wrote more thoroughly about the project. It included the background of the project, purpose and objectives, main access links, explanations of the cleaned dataset and the web app, the procedure from data cleaning to app development, a local development guide for editing the project on one’s local device, recommendations for how to improve the project, the credits to our team members, and a bibiography. You can read the agriHanda documentation here.\n\nThat’s all for this post. Thanks for reading!"
  },
  {
    "objectID": "posts/2021-12-28-linear-regression-house-prices.html#data-source",
    "href": "posts/2021-12-28-linear-regression-house-prices.html#data-source",
    "title": "Using Linear Regression to Predict House Sale Prices",
    "section": "Data Source",
    "text": "Data Source\nDe Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education, 19(3), null. https://doi.org/10.1080/10691898.2011.11889627"
  },
  {
    "objectID": "posts/2021-12-28-linear-regression-house-prices.html#information-sources",
    "href": "posts/2021-12-28-linear-regression-house-prices.html#information-sources",
    "title": "Using Linear Regression to Predict House Sale Prices",
    "section": "Information Sources",
    "text": "Information Sources\nCoding Systems for Categorical Variables in Regression Analysis. (n.d.). UCLA: Statistical Consulting Group. Retrieved December 26, 2020, from https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis-2/\ncosine1509. (2020, August 14). Detecting Multicollinearity with VIF - Python. GeeksforGeeks. https://www.geeksforgeeks.org/detecting-multicollinearity-with-vif-python/\nFrost, J. (2017a, April 2). Multicollinearity in Regression Analysis: Problems, Detection, and Solutions. Statistics By Jim. http://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/\nFrost, J. (2017b, April 12). How to Interpret P-values and Coefficients in Regression Analysis. Statistics By Jim. http://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/\nMordkoff, J. T. (2016). The Assumption(s) of Normality. http://www2.psychology.uiowa.edu/faculty/mordkoff/GradStats/part%201/I.07%20normal.pdf\nPython for Data Science. (2018, March 15). Linear Regression. Python for Data Science. https://pythonfordatascienceorg.wordpress.com/linear-regression-python/"
  },
  {
    "objectID": "posts/2021-12-28-linear-regression-house-prices.html#image-source",
    "href": "posts/2021-12-28-linear-regression-house-prices.html#image-source",
    "title": "Using Linear Regression to Predict House Sale Prices",
    "section": "Image Source",
    "text": "Image Source\nAssis, B. (2018, January 17). Photo by Breno Assis on Unsplash. Unsplash. https://unsplash.com/photos/r3WAWU5Fi5Q"
  },
  {
    "objectID": "posts/2022-07-10-My-Portfolio-Projects.html",
    "href": "posts/2022-07-10-My-Portfolio-Projects.html",
    "title": "📌 My Portfolio Projects",
    "section": "",
    "text": "This post lists my portfolio projects, which are my best original works.\nAmong the blog posts on my website, many of these are “guided projects,” meaning that the data and the research ideas were provided by an online course like Dataquest. Guided projects are only partly original, as I put my own unique twist on them. The portfolio projects showcased below, however, are completely original.\n\n2024 Synergy Software Solutions and Blue Hacks competitions\nMy team (with five students of the BS Applied Mathematics - Master in Data Science program at Ateneo, including myself) participated in Synergy Software Solutions 2024 and Blue Hacks 2024. We won 1st and 2nd place in these competitions, respectively. We had developed a comprehensive personal carbon footprint calculator, as well as a mental health platform for students and guidance counselors.\nSkills used: Python, data cleaning, data mining/web scraping, dashboard development, interactive data visualization, database operations.\nRead more about it in this blog post.\n\n\n2021 Project SPARTA Butuan City Open Data Challenge\nI led a team to participate in this competition from June to November of 2021. I cleaned data about the disaster risk of Butuan City’s agriculture sector vis-a-vis natural hazards. Then, I developed agriHanda, a dashboard visualizing the data in various forms, such as a colored map of the city, a data summary for each barangay (town), and a custom graphing tool. For this, we won the Second Placer award and Best in Ingenious Solution award, along with a Php 11,500 cash prize and freebies.\nSkills used: Python, data cleaning, dashboard development, interactive data visualization, Geographic Information Systems (GIS)\nRead more about the competition in this blog post: agriHanda: an Agricultural Disaster Risk Web App\n\n\nASHS Student Mapping Project\nIn late 2021, I and the officers of the ASHS Programming Varsity (ProgVar) were approached by the school administration with a request to develop an app that would help identify students affected by natural hazards. Thus, I proposed the Student Mapping Project. I set up a database of student locations and developed a web app to be used by the school’s student-led disaster risk committee. The app can take a list of affected areas and generate a list of students who live in those areas, as well as a map showing their geographic distribution. This app helps identify which students should be contacted first and given assistance.\nSkills used: Python, web app development, data transformation, Geographic Information Systems (GIS).\nRead more in this blog post: Student Mapping Project\n\n\nASHS College Applications Dashboard\nIn my school’s Data Analytics Committee (DAC), I made the College Applications Dashboard to create various visualizations of the results of a survey about my schoolmates’ college choices and the factors influencing them.\nSkills used: Python, dashboard development, interactive data visualization.\nRead more in this blog post: College Applications Dashboard\n\n\nMG Pokemon Team Inspector web app\nI made a web app to help Pokemon players evaluate the usefulness of both individual Pokemon and teams of Pokemon. It has a high degree of customization, taking into account each Pokemon’s species, move types, and abilities to calculate offensive capacity, defensive capacity, and overall usefulness. Most of the data is ordinal, so it is visualized using color-coded tables and bar charts, with helpful tooltips that can be viewed by hovering over the chart with your cursor. These visualizations were constructed using the Altair data visualization package.\nSkills used: Python, web app development, data transformation, interactive data visualization\nRead more about this project here: MG Pokemon Team Inspector: a data viz web app for Pokemon players"
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#column-labels",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#column-labels",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Column Labels",
    "text": "Column Labels\nAs seen earlier, all of the column labels are written in camel case. This means that the first letter of each word is capitalized, like CamelCase.\nColumn labels should ideally be written in snake_case so that these are easier to use. Thus, I will convert all column labels to snake case.\n\ncols = list(autos.columns)\n\nnew_cols = []\n\nfor label in cols:\n    # List of indices of the first letter of each word.\n    cap_inds = [0] + [index for index, character in enumerate(label) if character.isupper()]\n    \n    # List of 2-tuples. Each tuple contains the start index of the current word\n    # and the start index of the next word.\n    zipped = list(zip(cap_inds, cap_inds[1:] + [None]))\n    \n    # Split the label into a list of words.\n    # Make them lowercase and combine them with underscores.\n    word_list = [label[i:j] for i, j in zipped]\n    word_list = [word.lower() for word in word_list]\n    new_label = \"_\".join(word_list)\n    \n    new_cols.append(new_label)\n    \nautos.columns = new_cols\n\nautos.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 371528 entries, 0 to 371527\nData columns (total 20 columns):\n #   Column                 Non-Null Count   Dtype \n---  ------                 --------------   ----- \n 0   date_crawled           371528 non-null  object\n 1   name                   371528 non-null  object\n 2   seller                 371528 non-null  object\n 3   offer_type             371528 non-null  object\n 4   price                  371528 non-null  int64 \n 5   abtest                 371528 non-null  object\n 6   vehicle_type           333659 non-null  object\n 7   year_of_registration   371528 non-null  int64 \n 8   gearbox                351319 non-null  object\n 9   power_p_s              371528 non-null  int64 \n 10  model                  351044 non-null  object\n 11  kilometer              371528 non-null  int64 \n 12  month_of_registration  371528 non-null  int64 \n 13  fuel_type              338142 non-null  object\n 14  brand                  371528 non-null  object\n 15  not_repaired_damage    299468 non-null  object\n 16  date_created           371528 non-null  object\n 17  nr_of_pictures         371528 non-null  int64 \n 18  postal_code            371528 non-null  int64 \n 19  last_seen              371528 non-null  object\ndtypes: int64(7), object(13)\nmemory usage: 56.7+ MB\n\n\nNow, all of the column labels are in snake case. However, I will manually change a few of them to make them neater.\n\nconvert = {\n    \"power_p_s\": \"power_ps\",\n    \"nr_of_pictures\": \"num_pictures\",\n    \"year_of_registration\": \"year_reg\",\n    \"kilometer\": \"mileage_km\",\n    \"month_of_registration\": \"month_reg\",\n    \"not_repaired_damage\": \"damage\",\n}\n\nautos.columns = pd.Series(autos.columns).replace(convert)\n\nautos.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 371528 entries, 0 to 371527\nData columns (total 20 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   date_crawled  371528 non-null  object\n 1   name          371528 non-null  object\n 2   seller        371528 non-null  object\n 3   offer_type    371528 non-null  object\n 4   price         371528 non-null  int64 \n 5   abtest        371528 non-null  object\n 6   vehicle_type  333659 non-null  object\n 7   year_reg      371528 non-null  int64 \n 8   gearbox       351319 non-null  object\n 9   power_ps      371528 non-null  int64 \n 10  model         351044 non-null  object\n 11  mileage_km    371528 non-null  int64 \n 12  month_reg     371528 non-null  int64 \n 13  fuel_type     338142 non-null  object\n 14  brand         371528 non-null  object\n 15  damage        299468 non-null  object\n 16  date_created  371528 non-null  object\n 17  num_pictures  371528 non-null  int64 \n 18  postal_code   371528 non-null  int64 \n 19  last_seen     371528 non-null  object\ndtypes: int64(7), object(13)\nmemory usage: 56.7+ MB\n\n\nAll of the column labels are now easy to use."
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#uninformative-columns",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#uninformative-columns",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Uninformative Columns",
    "text": "Uninformative Columns\nNext, we will look for columns filled with mostly 1 value. The other values in them are underrepresented, so we wouldn’t be able to compare groups reliably.\nWe can inspect the columns using DataFrame.describe().\n\nautos.describe(include = \"all\")\n# Include all columns, including text.\n\n\n\n\n\n\n\n\ndate_crawled\nname\nseller\noffer_type\nprice\nabtest\nvehicle_type\nyear_reg\ngearbox\npower_ps\nmodel\nmileage_km\nmonth_reg\nfuel_type\nbrand\ndamage\ndate_created\nnum_pictures\npostal_code\nlast_seen\n\n\n\n\ncount\n371528\n371528\n371528\n371528\n3.715280e+05\n371528\n333659\n371528.000000\n351319\n371528.000000\n351044\n371528.000000\n371528.000000\n338142\n371528\n299468\n371528\n371528.0\n371528.00000\n371528\n\n\nunique\n280500\n233531\n2\n2\nNaN\n2\n8\nNaN\n2\nNaN\n251\nNaN\nNaN\n7\n40\n2\n114\nNaN\nNaN\n182806\n\n\ntop\n2016-03-24 14:49:47\nFord_Fiesta\nprivat\nAngebot\nNaN\ntest\nlimousine\nNaN\nmanuell\nNaN\ngolf\nNaN\nNaN\nbenzin\nvolkswagen\nnein\n2016-04-03 00:00:00\nNaN\nNaN\n2016-04-07 06:45:59\n\n\nfreq\n7\n657\n371525\n371516\nNaN\n192585\n95894\nNaN\n274214\nNaN\n30070\nNaN\nNaN\n223857\n79640\n263182\n14450\nNaN\nNaN\n17\n\n\nmean\nNaN\nNaN\nNaN\nNaN\n1.729514e+04\nNaN\nNaN\n2004.577997\nNaN\n115.549477\nNaN\n125618.688228\n5.734445\nNaN\nNaN\nNaN\nNaN\n0.0\n50820.66764\nNaN\n\n\nstd\nNaN\nNaN\nNaN\nNaN\n3.587954e+06\nNaN\nNaN\n92.866598\nNaN\n192.139578\nNaN\n40112.337051\n3.712412\nNaN\nNaN\nNaN\nNaN\n0.0\n25799.08247\nNaN\n\n\nmin\nNaN\nNaN\nNaN\nNaN\n0.000000e+00\nNaN\nNaN\n1000.000000\nNaN\n0.000000\nNaN\n5000.000000\n0.000000\nNaN\nNaN\nNaN\nNaN\n0.0\n1067.00000\nNaN\n\n\n25%\nNaN\nNaN\nNaN\nNaN\n1.150000e+03\nNaN\nNaN\n1999.000000\nNaN\n70.000000\nNaN\n125000.000000\n3.000000\nNaN\nNaN\nNaN\nNaN\n0.0\n30459.00000\nNaN\n\n\n50%\nNaN\nNaN\nNaN\nNaN\n2.950000e+03\nNaN\nNaN\n2003.000000\nNaN\n105.000000\nNaN\n150000.000000\n6.000000\nNaN\nNaN\nNaN\nNaN\n0.0\n49610.00000\nNaN\n\n\n75%\nNaN\nNaN\nNaN\nNaN\n7.200000e+03\nNaN\nNaN\n2008.000000\nNaN\n150.000000\nNaN\n150000.000000\n9.000000\nNaN\nNaN\nNaN\nNaN\n0.0\n71546.00000\nNaN\n\n\nmax\nNaN\nNaN\nNaN\nNaN\n2.147484e+09\nNaN\nNaN\n9999.000000\nNaN\n20000.000000\nNaN\n150000.000000\n12.000000\nNaN\nNaN\nNaN\nNaN\n0.0\n99998.00000\nNaN\n\n\n\n\n\n\n\nThe top row gives the most frequent value in the column. The freq row tells exactly how often the top value occurs.\nWe can see that in the seller column, \"privat\" appears 371,525 times; this is close to the total number of entries. Also, in the offer_type column, \"Angebot\" appears 371,516 times.\nLet us first check the unique values of the seller column.\n\nautos[\"seller\"].value_counts()\n\nprivat        371525\ngewerblich         3\nName: seller, dtype: int64\n\n\nIn English, these mean “private” and “commercial.” It is likely that private listings are put up by individuals, whereas commercial listings are put up by used car dealer companies. Most listings scraped were apparently private.\nOne idea is to compare private listing prices to commercial listing prices. However, there are only 3 commercial listings, so this comparison would be unreliable. Therefore, we can drop this column.\n\nautos.drop(columns = \"seller\", inplace = True)\n\nSecondly, let’s check the unique values of the offer_type column.\n\nautos[\"offer_type\"].value_counts()\n\nAngebot    371516\nGesuch         12\nName: offer_type, dtype: int64\n\n\nIn English, these mean “offer” and “request.” This isn’t very informative. It could have something to do with the system of sending Offers to buyers in eBay. In any case, it doesn’t seem interesting to analyze, and there are only 12 request listings. Thus, we can also drop this column.\n\nautos.drop(columns = \"offer_type\", inplace = True)\n\nNext, the numerical num_pictures column has a minimum of 0 and a maximum of 0.\n\nautos[\"num_pictures\"].describe()\n\ncount    371528.0\nmean          0.0\nstd           0.0\nmin           0.0\n25%           0.0\n50%           0.0\n75%           0.0\nmax           0.0\nName: num_pictures, dtype: float64\n\n\nThis means that none of the listings for used cars were found to have pictures. This data may be true or it may have come from an error, but either way, it is not useful. Thus, it will be dropped.\n\nautos.drop(columns = \"num_pictures\", inplace = True)"
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#outliers-in-numerical-columns",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#outliers-in-numerical-columns",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Outliers in Numerical Columns",
    "text": "Outliers in Numerical Columns\nNext, we inspect the ranges of numerical columns to see if there are any unusually high or low values.\n\nautos.describe(include = np.number)\n\n\n\n\n\n\n\n\nprice\nyear_reg\npower_ps\nmileage_km\nmonth_reg\npostal_code\n\n\n\n\ncount\n3.715280e+05\n371528.000000\n371528.000000\n371528.000000\n371528.000000\n371528.00000\n\n\nmean\n1.729514e+04\n2004.577997\n115.549477\n125618.688228\n5.734445\n50820.66764\n\n\nstd\n3.587954e+06\n92.866598\n192.139578\n40112.337051\n3.712412\n25799.08247\n\n\nmin\n0.000000e+00\n1000.000000\n0.000000\n5000.000000\n0.000000\n1067.00000\n\n\n25%\n1.150000e+03\n1999.000000\n70.000000\n125000.000000\n3.000000\n30459.00000\n\n\n50%\n2.950000e+03\n2003.000000\n105.000000\n150000.000000\n6.000000\n49610.00000\n\n\n75%\n7.200000e+03\n2008.000000\n150.000000\n150000.000000\n9.000000\n71546.00000\n\n\nmax\n2.147484e+09\n9999.000000\n20000.000000\n150000.000000\n12.000000\n99998.00000\n\n\n\n\n\n\n\nThe following unusual characteristics are noticeable:\n\nprice: The minimum price is 0 (free) and the maximum price is larger than 2 billion.\nyear_reg: The earliest year of registration is 1000, and the latest year is 9999.\npower_ps: The lowest power is 0 PS. The highest is 20,000 PS, which is much higher than the 75th percentile (only 150 PS).\nmonth_reg: The month numbers range from 0 to 12, not 1 to 12.\n\nAll of these columns shall be cleaned of inaccurate data.\n\nPrice in Euros\nIt was seen that the minimum value for price data was 0, and the maximum is over 2,000,000,000. Since this data was scraped from the German eBay, we can assume that these prices are in euros.\nBoth the minimum and maximum values seem to be very unrealistic for the price of a used car, so we want to clean this data.\nLet us create a frequency table of the prices. We will first sort it by price, ascending.\n\nautos[\"price\"].value_counts().sort_index()\n\n0             10778\n1              1189\n2                12\n3                 8\n4                 1\n              ...  \n32545461          1\n74185296          1\n99000000          1\n99999999         15\n2147483647        1\nName: price, Length: 5597, dtype: int64\n\n\nTwo things are noticeable:\n\nThere are over 10,000 instances of 0 euro prices.\nThere are several extremely high outliers, not just the one above 2 billion.\n\nSince it is unlikely that the cars are actually free, 0 may represent missing values. As for the high outliers, these may have resulted from inaccurate scraping.\nThe next question is, what is the most frequent price value? Is it 0?\n\nautos[\"price\"].value_counts().sort_values(ascending = False)\n\n0        10778\n500       5670\n1500      5394\n1000      4649\n1200      4594\n         ...  \n2610         1\n20620        1\n16689        1\n9964         1\n10985        1\nName: price, Length: 5597, dtype: int64\n\n\nIndeed, the most frequent price is 0, which makes it likely that it represents missing values. The next most frequent prices are 500, 1500, and 1000, most likely because these are easy for buyers to remember.\nNow that we know about the low and high outliers, we can clean rows using the interquartile range or IQR. The IQR is bounded by the 25th and 75th percentiles. Reasonable lower and upper bounds can be approximated by multiplying the IQR length by 1.5 and:\n\nsubtracting it from the 25th percentile\nadding it to the 75th percentile\n\nReference: Detection and Removal of Outliers in Python – An Easy to Understand Guide\nBelow, I use this method to find a lower and upper bound, and I drop rows outside of them.\n\nautos_desc = autos[\"price\"].describe()\n\nq25, q75 = autos_desc[[\"25%\", \"75%\"]]\n\niqr = q75 - q25\n\nlower_bound = q25 - 1.5 * iqr\nupper_bound = q75 + 1.5 * iqr\n\nautos = autos.loc[autos[\"price\"].between(lower_bound, upper_bound)]\n\nprint(\"Lower bound:\", lower_bound)\nprint(\"Upper bound:\", upper_bound)\nprint(\"New shape:\", autos.shape)\n\nLower bound: -7925.0\nUpper bound: 16275.0\nNew shape: (343420, 17)\n\n\nNow, there are 343,420 rows remaining in the dataset. This is still quite large; not too many datapoints were removed.\nLet us view the new distribution of prices in a histogram.\n\nax = sns.histplot(\n    data = autos,\n    x = \"price\",\n)\n\nax.set_title(\"Used Car Prices, Cleaned Using IQR\")\nax.grid(True)\n\nplt.show()\n\n\n\n\nUnfortunately, since the lower bound was negative, unrealistically low prices close to 0 were still accepted within the range.\nRemember from earlier that the 2nd most frequent price value was found to be 500 euros. This makes it likely that these were correct prices, rather than incorrectly scraped values. Thus, let us keep all price values greater than or equal to 500.\n\nautos = autos.loc[autos[\"price\"] &gt;= 500]\n\nautos.shape\n\n(307358, 17)\n\n\nNow, 307,358 datapoints remain. Let us look at the final distribution of prices:\n\nax = sns.histplot(\n    data = autos,\n    x = \"price\",\n)\n\nax.set_title(\"Used Car Prices, Cleaned of Low Values\")\nax.grid(True)\n\nplt.show()\n\n\n\n\nThe distribution is still right-skewed, but at least the price range in the dataset is more reasonable now.\n\n\nMetric Horsepower\nWe saw earlier that some power values were at 0 PS, and others were as high as 20,000 PS.\n\n\n\n\n\n\nNote\n\n\n\nA measure of 1 PS (metric horsepower) is equivalent to 0.98 HP (horsepower). (Deriquito 2020)\n\n\n\nautos[\"power_ps\"].describe()\n\ncount    307358.000000\nmean        112.551842\nstd         191.557378\nmin           0.000000\n25%          75.000000\n50%         105.000000\n75%         143.000000\nmax       20000.000000\nName: power_ps, dtype: float64\n\n\nAbove are the new descriptive statistics for metric horsepower, after the transformations that we have done recently.\nAccording to “What is the Average Car Horsepower?” (2021), cars usually have a horsepower of 100 HP to 400 HP, though some have less. Faster cars have 400 HP to 1,000 HP, and some supercars have over 1,000 HP. The equivalent values in PS would be slightly higher.\nConsidering that these are used cars, it is unlikely for them to be very fast much less to be supercars. Thus, we will keep cars with metric horsepowers between 50 and 450, in order to take as many of the realistic values as possible.\n\nautos = autos.loc[autos[\"power_ps\"].between(50, 450)]\n\nautos[\"power_ps\"].describe()\n\ncount    274514.000000\nmean        121.166931\nstd          50.535320\nmin          50.000000\n25%          82.000000\n50%         114.000000\n75%         150.000000\nmax         450.000000\nName: power_ps, dtype: float64\n\n\nLet us view the distribution of power values in a histogram.\n\nax = sns.histplot(\n    data = autos,\n    x = \"power_ps\",\n)\n\nax.set_title(\"Metric Horsepower (PS) of Used Cars\")\nax.grid(True)\n\nplt.show()\n\n\n\n\nLike the price data, the power data are right-skewed. Lower values are more frequent. Still, we managed to capture most (274,514) of the datapoints using this range.\n\n\nYear of Registration\nEarlier, we noticed that the years of registration ranged from 1000 to 9999.\n\nautos[\"year_reg\"].describe()\n\ncount    274514.000000\nmean       2003.658039\nstd          29.529992\nmin        1000.000000\n25%        1999.000000\n50%        2004.000000\n75%        2007.000000\nmax        9999.000000\nName: year_reg, dtype: float64\n\n\nLogically, the years of registration should only range from when automobiles were first mass-produced, in the 1800s or 1900s (Encyclopedia Britannica), to the year when the ads were last seen by the web crawler.\nWe can know when the ads were last seen using the last_seen column of the dataset. The dates are shown as strings with years first, so we can sort them in descending order to find the most recent date.\n\nautos[\"last_seen\"].sort_values(ascending = False)\n\n343211    2016-04-07 14:58:51\n5069      2016-04-07 14:58:50\n72231     2016-04-07 14:58:50\n62402     2016-04-07 14:58:50\n122068    2016-04-07 14:58:50\n                 ...         \n244077    2016-03-05 14:35:28\n177326    2016-03-05 14:25:59\n136842    2016-03-05 14:15:39\n275196    2016-03-05 14:15:16\n311225    2016-03-05 14:15:08\nName: last_seen, Length: 274514, dtype: object\n\n\nIt looks like all of the ads were last seen in 2016, so the cars could not have been registered beyond that year.\nTherefore, a reasonable range of years would be from 1900 to 2016. Let us keep all rows within this range.\n\nautos = autos.loc[autos[\"year_reg\"].between(1900, 2016)]\n\nautos[\"year_reg\"].describe()\n\ncount    264977.000000\nmean       2002.977877\nstd           6.097207\nmin        1910.000000\n25%        1999.000000\n50%        2003.000000\n75%        2007.000000\nmax        2016.000000\nName: year_reg, dtype: float64\n\n\nNow, the years of registration range from 1910 to 2016. There are also 264,977 datapoints remaining.\n\n\nMonths of Registration\nLastly, we noticed that the months of registration included integers from 0 to 12, not 1 to 12. Let us make a discrete histogram in order to understand the distribution.\n\nax = sns.histplot(\n    data = autos,\n    x = \"month_reg\",\n    discrete = True,\n)\n\nax.set_title(\"Used Cars' Months of Registration\")\nax.grid(True)\n\nplt.show()\n\n\n\n\nThe graph above shows that 0 is the least frequent value, appearing under 15,000 times. It seems very likely that it is a placeholder for unknown months. Therefore, we can remove rows with month 0.\n\nautos = autos.loc[autos[\"month_reg\"] &gt;= 1]\n\nautos[\"month_reg\"].describe()\n\ncount    251198.000000\nmean          6.396237\nstd           3.346424\nmin           1.000000\n25%           4.000000\n50%           6.000000\n75%           9.000000\nmax          12.000000\nName: month_reg, dtype: float64"
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#missing-values",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#missing-values",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Missing Values",
    "text": "Missing Values\nNow, we shall deal with the null values in the dataset. First, how many null values are there per column?\n\nautos.isnull().sum()\n\ndate_crawled        0\nname                0\nprice               0\nabtest              0\nvehicle_type     5871\nyear_reg            0\ngearbox          3103\npower_ps            0\nmodel            7422\nmileage_km          0\nmonth_reg           0\nfuel_type        9057\nbrand               0\ndamage          28718\ndate_created        0\npostal_code         0\nlast_seen           0\ndtype: int64\n\n\nThere are thousands of missing values for the vehicle type, gearbox, model, fuel type, and presence of damage. None of these can be easily determined from other columns.\nIf we were to remove all rows with these missing values, we would be left with:\n\nautos.dropna().shape\n\n(208538, 17)\n\n\nWe would have 208,538 rows left. This is just over half of the original number of datapoints we started with, 371,528.\nThis project is meant to showcase data cleaning, and we are assuming that all of the columns in the dataset will be used for analysis. Thus, we will have to delete all rows with missing values.\n\nautos.dropna(inplace = True)\n\nBelow are the final descriptive statistics for the dataset after all of the cleaning.\n\nautos.describe(include = \"all\")\n\n\n\n\n\n\n\n\ndate_crawled\nname\nprice\nabtest\nvehicle_type\nyear_reg\ngearbox\npower_ps\nmodel\nmileage_km\nmonth_reg\nfuel_type\nbrand\ndamage\ndate_created\npostal_code\nlast_seen\n\n\n\n\ncount\n208538\n208538\n208538.000000\n208538\n208538\n208538.000000\n208538\n208538.000000\n208538\n208538.000000\n208538.000000\n208538\n208538\n208538\n208538\n208538.000000\n208538\n\n\nunique\n177458\n114617\nNaN\n2\n8\nNaN\n2\nNaN\n248\nNaN\nNaN\n7\n39\n2\n98\nNaN\n116677\n\n\ntop\n2016-04-02 22:54:55\nBMW_318i\nNaN\ntest\nlimousine\nNaN\nmanuell\nNaN\ngolf\nNaN\nNaN\nbenzin\nvolkswagen\nnein\n2016-04-03 00:00:00\nNaN\n2016-04-07 09:45:41\n\n\nfreq\n5\n616\nNaN\n108258\n62894\nNaN\n166213\nNaN\n17505\nNaN\nNaN\n136145\n44157\n189720\n8271\nNaN\n14\n\n\nmean\nNaN\nNaN\n4984.662522\nNaN\nNaN\n2003.205953\nNaN\n122.635031\nNaN\n128250.990227\n6.381225\nNaN\nNaN\nNaN\nNaN\n51907.756020\nNaN\n\n\nstd\nNaN\nNaN\n3990.028529\nNaN\nNaN\n5.696916\nNaN\n50.515054\nNaN\n35781.848172\n3.350349\nNaN\nNaN\nNaN\nNaN\n25766.228717\nNaN\n\n\nmin\nNaN\nNaN\n500.000000\nNaN\nNaN\n1937.000000\nNaN\n50.000000\nNaN\n5000.000000\n1.000000\nNaN\nNaN\nNaN\nNaN\n1067.000000\nNaN\n\n\n25%\nNaN\nNaN\n1749.000000\nNaN\nNaN\n2000.000000\nNaN\n85.000000\nNaN\n125000.000000\n3.000000\nNaN\nNaN\nNaN\nNaN\n31319.000000\nNaN\n\n\n50%\nNaN\nNaN\n3750.000000\nNaN\nNaN\n2004.000000\nNaN\n116.000000\nNaN\n150000.000000\n6.000000\nNaN\nNaN\nNaN\nNaN\n51371.000000\nNaN\n\n\n75%\nNaN\nNaN\n7300.000000\nNaN\nNaN\n2007.000000\nNaN\n150.000000\nNaN\n150000.000000\n9.000000\nNaN\nNaN\nNaN\nNaN\n72649.000000\nNaN\n\n\nmax\nNaN\nNaN\n16270.000000\nNaN\nNaN\n2016.000000\nNaN\n450.000000\nNaN\n150000.000000\n12.000000\nNaN\nNaN\nNaN\nNaN\n99998.000000\nNaN"
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#strings-to-datetime-objects",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#strings-to-datetime-objects",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Strings to Datetime Objects",
    "text": "Strings to Datetime Objects\nAs we saw earlier, the date_crawled, date_created, and last_seen columns contain dates and times in string format. It would be better to store these as datetime objects so that each part (year, month, etc.) can be accessed in analysis.\nFor this section, I used “Convert the column type from string to datetime format in Pandas dataframe” (2020) as a reference.\nFirst, though, we have to make a format string so that the numbers can be parsed properly.\n\nautos[[\"date_crawled\", \"date_created\", \"last_seen\"]].head()\n\n\n\n\n\n\n\n\ndate_crawled\ndate_created\nlast_seen\n\n\n\n\n3\n2016-03-17 16:54:04\n2016-03-17 00:00:00\n2016-03-17 17:40:17\n\n\n4\n2016-03-31 17:25:20\n2016-03-31 00:00:00\n2016-04-06 10:17:21\n\n\n5\n2016-04-04 17:36:23\n2016-04-04 00:00:00\n2016-04-06 19:17:07\n\n\n6\n2016-04-01 20:48:51\n2016-04-01 00:00:00\n2016-04-05 18:18:39\n\n\n10\n2016-03-26 19:54:18\n2016-03-26 00:00:00\n2016-04-06 10:45:34\n\n\n\n\n\n\n\nAll 3 columns seem to follow the same format:\n{4 digit year}-{2 digit month}-{2 digit day} {24-hour time hour}:{minute}:{second}\nThe equivalent format string is below.\n\nformat_str = \"%Y-%m-%d %H:%M:%S\"\n\nWe can use this string to parse the dates. This can be done quickly by using the pd.to_datetime() vectorized function, instead of using a for-loop with the datetime module.\n\nautos[\"date_crawled\"] = pd.to_datetime(\n    autos[\"date_crawled\"],\n    format = format_str,\n)\n\nautos[\"date_crawled\"]\n\n3        2016-03-17 16:54:04\n4        2016-03-31 17:25:20\n5        2016-04-04 17:36:23\n6        2016-04-01 20:48:51\n10       2016-03-26 19:54:18\n                 ...        \n371516   2016-04-04 09:57:12\n371517   2016-03-28 13:48:07\n371520   2016-03-19 19:53:49\n371524   2016-03-05 19:56:21\n371525   2016-03-19 18:57:12\nName: date_crawled, Length: 208538, dtype: datetime64[ns]\n\n\nWe can see that the values look similar to how they looked before, but the dtype is now datetime64[ns].\nFor example, we can now access the month and day from the datetime objects.\n\nautos[\"date_crawled\"].iloc[0].month\n\n3\n\n\n\nautos[\"date_crawled\"].iloc[0].day\n\n17\n\n\nWe can see that the date in the first row has a month of 3 (March) and a day of 17.\nLet us do the same for the other 2 date columns.\n\nautos[\"date_created\"] = pd.to_datetime(\n    autos[\"date_created\"],\n    format = format_str,\n)\n\nautos[\"last_seen\"] = pd.to_datetime(\n    autos[\"last_seen\"],\n    format = format_str,\n)\n\nautos[[\"date_crawled\", \"date_created\", \"last_seen\"]].describe(\n    datetime_is_numeric = True,\n)\n\n\n\n\n\n\n\n\ndate_crawled\ndate_created\nlast_seen\n\n\n\n\ncount\n208538\n208538\n208538\n\n\nmean\n2016-03-21 13:39:21.839721984\n2016-03-20 19:44:12.604321536\n2016-03-30 09:43:36.598633984\n\n\nmin\n2016-03-05 14:06:23\n2015-08-07 00:00:00\n2016-03-05 14:15:16\n\n\n25%\n2016-03-13 11:06:28\n2016-03-13 00:00:00\n2016-03-23 20:52:41.500000\n\n\n50%\n2016-03-21 18:46:44.500000\n2016-03-21 00:00:00\n2016-04-04 14:39:49.500000\n\n\n75%\n2016-03-29 15:36:22.500000\n2016-03-29 00:00:00\n2016-04-06 11:45:07\n\n\nmax\n2016-04-07 14:36:58\n2016-04-07 00:00:00\n2016-04-07 14:58:51\n\n\n\n\n\n\n\nWe can see that since the 3 columns are in datetime format, they are now treated as numerical values. They have a mean and a list of percentiles.\nData transformation is done, so we can move on to data analysis."
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#most-expensive-car-brands",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#most-expensive-car-brands",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Most Expensive Car Brands",
    "text": "Most Expensive Car Brands\nBelow, a bar graph shows each car brand and the median price. The mean is not used because we know that the price data are right-skewed, not normal.\n\nplt.figure(figsize = (14, 5))\n\nax = sns.barplot(\n    data = autos,\n    x = \"brand\",\n    y = \"price\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Median Prices of Used Cars by Brand\")\nplt.grid(True)\nplt.xticks(rotation = 90)\n\nplt.show()\n\n\n\n\nThe graph shows that Porsche has the highest median price of used cars, at almost 12,000 euros. This is followed by Mini, Land Rover, and Jeep."
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#used-car-prices-by-age",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#used-car-prices-by-age",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Used Car Prices by Age",
    "text": "Used Car Prices by Age\nThe line plot below shows the relationship between a car’s year of registration (later -&gt; newer) and the median price.\n\nplt.figure(figsize = (14, 5))\n\nax = sns.lineplot(\n    data = autos,\n    x = \"year_reg\",\n    y = \"price\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Median Prices of Used Cars by Year of Registration\")\nplt.grid(True)\nplt.xticks(rotation = 90)\n\nplt.show()\n\n\n\n\nThe line plot shows that used car prices are lowest when the year of registration is in the mid-1990s.\nTo the right of the minimum, as the year of registration gets later, the median price steadily becomes higher. This makes sense because newer cars would have a higher value, even if they’re used.\nOn the other hand, to the left of the minimum, the median price increases (albeit erratically) as the year of registration gets earlier. This suggests that very old cars are considered to be valuable because they are rare. It is likely that the avid car collectors are the ones who pay such high prices for these cars."
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#used-car-prices-by-mileage",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#used-car-prices-by-mileage",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Used Car Prices by Mileage",
    "text": "Used Car Prices by Mileage\nThe line plot below shows the relationship between used cars’ mileage in kilometers and the median price.\n\nplt.figure(figsize = (14, 5))\n\nax = sns.lineplot(\n    data = autos,\n    x = \"mileage_km\",\n    y = \"price\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Median Prices of Used Cars by Mileage in Kilometers\")\nplt.grid(True)\nplt.xticks(rotation = 90)\n\nplt.show()\n\n\n\n\nAs expected, price decreases as mileage increases. What is unexpected is that there is a sharp increase in price from around 500 km to 1000 km. This was likely caused by an outlier datapoint which had a low mileage and a low price."
  },
  {
    "objectID": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#effect-of-presence-of-damage-on-used-car-price",
    "href": "posts/2021-05-20-basic-data-cleaning-ebay-car-sales-data.html#effect-of-presence-of-damage-on-used-car-price",
    "title": "Basic Data Cleaning with eBay Car Sales Data",
    "section": "Effect of Presence of Damage on Used Car Price",
    "text": "Effect of Presence of Damage on Used Car Price\nLastly, we will analyze the effect of the presence of unrepaired damage on the median price of used cars.\n\nax = sns.barplot(\n    data = autos,\n    x = \"damage\",\n    y = \"price\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Effect of Presence of Damage on Median Used Car Price\")\nplt.grid(True)\nplt.xticks(rotation = 90)\n\nplt.show()\n\n\n\n\nIt can be seen that the median price of used cars without damage is 4000 euros. The median price is 1500 euros for those with unrepaired damage.\nTherefore, cars with damage generally have lower prices. This makes sense because if the buyer plans to drive the car, they must shoulder the expense of repairing it."
  },
  {
    "objectID": "posts/2021-11-18-basic-statistics-fandango-ratings.html#information-sources",
    "href": "posts/2021-11-18-basic-statistics-fandango-ratings.html#information-sources",
    "title": "Basic Statistics with Fandango Movie Ratings",
    "section": "Information Sources",
    "text": "Information Sources\nDetails for Non-Parametric Alternatives in Case C-Q. (2021). In Biostatistics Open Learning Textbook. University of Florida. https://bolt.mph.ufl.edu/6050-6052/unit-4b/module-13/details-for-non-parametric-alternatives/\nGuided Project: Investigating Fandango Movie Ratings. (n.d.). Dataquest. Retrieved November 18, 2021, from https://www.dataquest.io/c/53/m/288/guided-project%3A-investigating-fandango-movie-ratings\nHickey, W. (2015, October 15). Be Suspicious Of Online Movie Ratings, Especially Fandango’s. FiveThirtyEight. https://fivethirtyeight.com/features/fandango-movies-ratings/\nLaMorte, W. W. (2017, May 4). Mann-Whitney-Table-CriticalValues.pdf. Boston University School of Public Health. https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Nonparametric/Mann-Whitney-Table-CriticalValues.pdf\nNCSS. (n.d.). Chapter 206 Two-Sample T-Test. NCSS Statistics Solutions. Retrieved November 18, 2021, from https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Two-Sample_T-Test.pdf\nscipy.stats.mannwhitneyu—SciPy v1.7.1 Manual. (2021). Scipy. https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\nWaskom, M. (2021). Visualizing distributions of data—Seaborn 0.11.2 documentation. Seaborn. http://seaborn.pydata.org/tutorial/distributions.html#tutorial-kde"
  },
  {
    "objectID": "posts/2021-11-18-basic-statistics-fandango-ratings.html#image-sources",
    "href": "posts/2021-11-18-basic-statistics-fandango-ratings.html#image-sources",
    "title": "Basic Statistics with Fandango Movie Ratings",
    "section": "Image Sources",
    "text": "Image Sources\nLitvin, A. (2018, August 23). Photo by Alex Litvin on Unsplash. Unsplash. https://unsplash.com/photos/MAYsdoYpGuk"
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#standardization",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#standardization",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "Standardization",
    "text": "Standardization\nThe first important technique is standardization. So that each feature will contribute equally to the Euclidean distance, we will standardize each numeric feature. In other words, each value will be converted into a z-score so that the mean of each feature is 0 and its standard deviation is 1. The following equation is used:\n\\(z = \\frac{x - \\bar{x}}{s}\\)\n\n\\(z\\) is the z-score.\n\\(x\\) is a value in a feature.\n\\(\\bar{x}\\) is the mean of the feature.\n\\(s\\) is the sample standard deviation.\n\n\n\nCode\nall_feature_cols = [col for col in cars_df.columns if col != \"price\"]\n\n# Series of feature:data type\nfdt = cars_df[all_feature_cols].dtypes\n\n# Identify numeric features\nall_numeric_features = fdt.index[fdt != \"object\"]\n\n# Standardize\ncars_df[all_numeric_features] = cars_df[all_numeric_features].apply(zscore, axis = 0, ddof = 1)\n\ncars_df[all_numeric_features].head()\n\n\n\n\n\n\n\n\n\nsymboling\nnum-of-doors\nwheel-base\nlength\nwidth\nheight\ncurb-weight\nnum-of-cylinders\nengine-size\nbore\nstroke\ncompression-ratio\nhorsepower\npeak-rpm\ncity-mpg\nhighway-mpg\n\n\n\n\n0\n1.782215\n-1.172839\n-1.678015\n-0.442872\n-0.839080\n-2.117092\n-0.025646\n-0.410180\n0.045098\n0.511697\n-1.803495\n-0.287525\n0.198054\n-0.212806\n-0.677292\n-0.555613\n\n\n1\n1.782215\n-1.172839\n-1.678015\n-0.442872\n-0.839080\n-2.117092\n-0.025646\n-0.410180\n0.045098\n0.511697\n-1.803495\n-0.287525\n0.198054\n-0.212806\n-0.677292\n-0.555613\n\n\n2\n0.163544\n-1.172839\n-0.719041\n-0.250543\n-0.184200\n-0.613816\n0.496473\n1.544506\n0.574066\n-2.388614\n0.701095\n-0.287525\n1.330822\n-0.212806\n-0.990387\n-0.702307\n\n\n3\n0.972880\n0.848214\n0.142410\n0.182198\n0.143240\n0.179580\n-0.426254\n-0.410180\n-0.459826\n-0.516262\n0.479169\n-0.036110\n-0.039037\n0.853987\n-0.207649\n-0.115531\n\n\n4\n0.972880\n0.848214\n0.077395\n0.182198\n0.236794\n0.179580\n0.498371\n0.567163\n0.189362\n-0.516262\n0.479169\n-0.538940\n0.303427\n0.853987\n-1.146935\n-1.289083\n\n\n\n\n\n\n\nThe table above shows the first 5 rows of all of the numeric features. Notice that each feature now contains positive and negative values close to 0 because it was standardized."
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#feature-selection",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#feature-selection",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "Feature Selection",
    "text": "Feature Selection\nThe second technique is feature selection. We must choose features which we think are most relevant to a car’s price. We can only select numeric features since categorical ones cannot be used to calculate Euclidean distance. Thus, we must select from the following features:\n\n\nCode\nall_numeric_features.to_list()\n\n\n['symboling',\n 'num-of-doors',\n 'wheel-base',\n 'length',\n 'width',\n 'height',\n 'curb-weight',\n 'num-of-cylinders',\n 'engine-size',\n 'bore',\n 'stroke',\n 'compression-ratio',\n 'horsepower',\n 'peak-rpm',\n 'city-mpg',\n 'highway-mpg']\n\n\nAll of these features are physical characteristics of a car, except for “symboling”. According to the dataset documentation by Schlimmer (2019), this feature is an “insurance risk rating.” It elaborates:\n\nCars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process “symboling”. A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n\nGiven that this feature is systematically associated with the price of a car, it may be relevant to our model. Thus, we will consider it along with the other numeric features.\nIn order to determine which combination of features is the best, we will use univariate feature selection. “Univariate” refers to the use of a single variable. We will perform a statistical test between each feature and the target. Then, we will select the features with the highest scores from the statistical test (scikit-learn developers, 2021).\nIn our case, we have a regression problem, since we want to predict a continuous variable, car price. Thus, we will use the F-statistic as our score function. According to Frost (2017), the F-statistic indicates the “overall significance” of a linear regression model. In univariate feature selection, we would do the following steps:\n\nFor each feature:\n\nPerform linear regression where the independent variable is the feature and the dependent variable is the target (in this case, price).\nObtain the F-statistic.\n\nCompile a list with the F-statistic of each feature.\nIdentify the features with the highest F-statistics.\n\nThis can be implemented automatically using the scikit-learn’s SelectKBest class. It is called SelectKBest because we can set a parameter k which tells how many features to select. For example, if k = 3, the top three features with the highest F-statistic are selected. This is done below:\n\n\nCode\nskb = SelectKBest(\n    score_func = f_regression,\n    k = 3,\n)\n\nX = cars_df[all_numeric_features]\ny = cars_df[\"price\"]\n\nX_new = skb.fit_transform(X, y)\n\nbest_features = list(skb.get_feature_names_out())\nprint(\"Top 3 features:\", best_features)\n\n\nTop 3 features: ['curb-weight', 'engine-size', 'horsepower']\n\n\nThe results show that curb weight, engine size, and horsepower are the highest-scoring features. However, we will not select these yet for the final model, since other steps still must be discussed."
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#train-test-split-with-stratification",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#train-test-split-with-stratification",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "Train-Test Split with Stratification",
    "text": "Train-Test Split with Stratification\nTrain-test split is the third important technique.\nBefore model training, the dataset has to be split into training and testing sets. We will use 80% of the data in the training set and 20% in the testing set. As the names suggest, the training set is used to train the model or help it learn how to predict car prices. Then, we make predictions on the cars on the testing set to see whether the predictions are accurate.\nBefore we split the data, though, we have to ensure that the frequency distribution of the target is similar between the training and testing sets. Below is a histogram of the frequency distribution of car price across the entire dataset:\n\n\nCode\nsns.histplot(cars_df[\"price\"], bins = 100)\nplt.title(\"Frequency Distribution of Car Price\")\nplt.xlabel(\"Price (USD)\")\nplt.ylabel(\"Number of Cars\")\nplt.show()\n\n\n\n\n\nThe graph shows a right-skewed distribution, which means that most of the car prices are low and there are outliers with high prices. When we split the data into training and testing sets, we want each set to have a similar distribution to this.\nDe Cock (2011) provides a helpful suggestion on how to do this. The article says, “Simply order the original data set by a variable of interest (such as sale price) and select every kth observation to achieve the desired sample size (k=2 for a 50/50 split or k=4 for a 75/25 split).”\nIn our case, we want an 80/20 split. One-fifth of the data will go to the testing set, so we can use k = 5. We will thus order the observations by price, then assign every 5th observation to the testing set. All other observations will go to the training set.\nIn the code below, I have written a custom function stratify_continuous that uses this technique. I then performed a train-test split after stratification. X_train and y_train refer to the features and target in the training set, respectively. X_test and y_test are from the testing set.\n\n\nCode\ndef stratify_continuous(n_folds, y):\n    \"\"\"Stratify a dataset on a continuous target.\"\"\"\n    if n_folds &lt; 2 or n_folds &gt; 10:\n        raise ValueError(\"Please select a number of folds from 2 to 10.\")\n    \n    fold_nums = list(range(n_folds))\n\n    # DataFrame where \"index\" column contains the original indices\n    df = pd.DataFrame(\n        y\n        # Shuffle before ranking so that cars with the same price are ordered randomly.\n        .sample(frac = 1, random_state = 1, ignore_index = False)\n    )\n\n    # This column gives a rank to each value in y. 0 is the rank of the lowest value.\n    # Ties are broken according to order of appearance.\n    df[\"rank\"] = df[y.name].rank(method = \"first\") - 1\n\n    df[\"fold\"] = 0\n\n    for f in fold_nums[1:]:\n        # start at f, then increment by n_folds\n        indices = list(range(f, df.shape[0], n_folds))\n        df.loc[df[\"rank\"].isin(indices), \"fold\"] = f\n\n    # Revert df to original order of indices\n    df = df.reindex(index = y.index)\n\n    # A series that indicates the fold number of each observation according to its original position in y\n    fold_series = df[\"fold\"].copy()\n\n    return fold_series\n\nfolds = stratify_continuous(\n    n_folds = 5,\n    y = cars_df[\"price\"],\n)\n\ndef split_folds(X, y, fold_series, test_fold):\n    \"\"\"Take a dataset whose observations have been grouped into folds,\n    then perform a train-test split.\"\"\"\n\n    if fold_series.dtype != \"int64\":\n        raise AttributeError(\"The fold list does not purely contain integers.\")\n\n    test_mask = (fold_series == test_fold)\n\n    X_train = X.loc[~test_mask].copy()\n    y_train = y.loc[~test_mask].copy()\n\n    X_test = X.loc[test_mask].copy()\n    y_test = y.loc[test_mask].copy()\n\n    return X_train, X_test, y_train, y_test\n\nX_train, X_test, y_train, y_test = split_folds(\n    X = cars_df[all_numeric_features],\n    y = cars_df[\"price\"],\n    fold_series = folds,\n    test_fold = 4,\n)\n\n# Summary statistics for target columns.\ntarget_df = pd.concat(\n    [y_train, y_test],\n    axis = 1,\n    join = \"outer\",\n)\n\ntarget_df.columns = [\"y_train price\", \"y_test price\"]\n\ntarget_df.describe()\n\n\n\n\n\n\n\n\n\ny_train price\ny_test price\n\n\n\n\ncount\n155.000000\n38.000000\n\n\nmean\n13308.658065\n13188.631579\n\n\nstd\n8197.063090\n7737.592975\n\n\nmin\n5118.000000\n5389.000000\n\n\n25%\n7713.500000\n7805.750000\n\n\n50%\n10245.000000\n10295.000000\n\n\n75%\n16530.500000\n16385.750000\n\n\nmax\n45400.000000\n37028.000000\n\n\n\n\n\n\n\nThis table shows summary statistics for the price columns of the two sets. The sets have similar means at around USD 13,200, and they also have similar medians at around USD 10,200.\nLet us compare the price distributions using KDE plots:\n\n\nCode\nsns.kdeplot(y_train, label = \"Training set\")\nsns.kdeplot(y_test, label = \"Testing set\")\n\nplt.title(\"Comparison of Car Prices Between Sets\")\nplt.xlabel(\"Price (USD)\")\nplt.ylabel(\"Probability Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\nThe KDE plots both seem to follow the same shape and have the same center. This shows that the training and testing sets have roughly the same distribution of car prices. Thus, these were stratified correctly."
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#hyperparameter-optimization",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#hyperparameter-optimization",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "Hyperparameter Optimization",
    "text": "Hyperparameter Optimization\nThe fourth technique is hyperparameter optimization. This involves training the KNN model using different hyperparameter values to see which one performs the best.\nA hyperparameter is a value that influences the behavior of a model and has no relation to the data. In the case of KNN, one important hyperparameter is the \\(k\\) value, or the number of neighbors used to make a prediction. If \\(k = 5\\), we take the mean price of the top five most similar cars and call this our prediction. However, if \\(k = 10\\), we take the top ten cars, so the mean price may be different.\nWe can optimize \\(k\\) in this way:\n\nDecide values of \\(k\\) to test.\nFor each \\(k\\) value, fit and evaluate a KNN model.\nIdentify the best-performing model and use its \\(k\\) value in the final model.\n\nIn order to evaluate a model, we need an evaluation metric. In our case, we will use the Root Mean Squared Error or RMSE. This is calculated with the following equation:\n\\(RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\text{actual}_i - \\text{predicted}_i)^2}\\)\n\n\\(n\\) is the sample size.\n\\(\\text{actual}\\) is the actual target value, or in this case, the actual price of a car.\n\\(\\text{predicted}\\) is the predicted target value.\n\nRMSE can be interpreted as the average error of a regression model. For example, if \\(RMSE = 1000\\), this means that the model’s predicted car prices are USD 1000 away from the actual car prices, on average.\nBelow is an example of hyperparameter optimization using RMSE. All of the numeric features were used for this example.\n\n\nCode\nk_values = [1, 3, 5]\n\nk_rmse = pd.Series(dtype = \"float64\")\n\nfor k in k_values:\n    knn = KNeighborsRegressor(\n        n_neighbors = k,\n        algorithm = \"auto\",\n    )\n\n    knn.fit(X_train, y_train)\n\n    y_pred = knn.predict(X_test)\n\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\n    k_rmse.loc[k] = rmse\n\nprint(\"k value and RMSE\")\nk_rmse\n\n\nk value and RMSE\n\n\n1    4086.029491\n3    3146.025226\n5    3251.392477\ndtype: float64\n\n\nThe table above shows that RMSE was lowest for \\(k = 3\\). The RMSE was about USD 3146, which means that on average, the predicted prices are USD 3146 away from the actual prices."
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#k-fold-cross-validation",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#k-fold-cross-validation",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "K-Fold Cross-Validation",
    "text": "K-Fold Cross-Validation\nThe last technique that will be discussed is K-Fold Cross-Validation. Earlier, we split the data into one training set and one testing set. The K-Fold Cross-Validation allows us to obtain a more holistic view of model performance by rotating the observations used in the two sets. In the words of Brownlee (2018), it estimates “how the model is expected to perform in general when used to make predictions on data not used during the training of the model.”\nHere, \\(k\\) has a different meaning. It determines the number of splits to make in a dataset. For example, if \\(k = 5\\), the dataset will be split into 5 folds, each set containing 20% of the total data.\nIn summary, the following steps are performed:\n\nSplit the data into 5 folds: A, B, C, D, E.\nUse fold A as the testing set and use the others as the training set.\nFit and evaluate a KNN model, thus obtaining RMSE.\nRepeat the above process for a total of 5 times, so that each fold is used as a testing set once.\nCompile a list of the five RMSE values obtained.\nCompute the mean RMSE value. This is the final metric of model performance.\n\nK-Fold Cross-Validation can be implemented using scikit-learn’s KFold and cross_val_score . An example of 5-fold cross-validation is shown below.\n\n\nCode\nknn = KNeighborsRegressor(\n    n_neighbors = 5,\n    algorithm = \"auto\",\n)\n\nkf = KFold(5, shuffle = True, random_state = 1)\n\nmses = cross_val_score(\n    estimator = knn,\n    X = cars_df[all_numeric_features],\n    y = cars_df[\"price\"],\n    scoring = \"neg_mean_squared_error\",\n    cv = kf,\n)\n\nmses = pd.Series(mses)\n\nrmses = mses.abs().pow(1/2)\n\nmean_rmse = rmses.mean()\n\nsd_rmse = rmses.std(ddof = 1)\n\nprint(f\"\"\"Regular 5-fold cross-validation\nMean RMSE: {mean_rmse:.2f}\nStandard Deviation RMSE: {sd_rmse:.2f}\nRMSE Values: {rmses.to_list()}\"\"\")\n\n\nRegular 5-fold cross-validation\nMean RMSE: 3722.28\nStandard Deviation RMSE: 565.62\nRMSE Values: [3407.8275635020186, 3902.1144860913682, 3009.7340988268425, 4521.314079941105, 3770.3892479494248]\n\n\nThe mean RMSE above presents a better picture of the model’s performance because it takes into account different possible combinations of training and testing sets.\nNote, however, that the standard deviation of the RMSE was around 566. This means that the RMSE values varied by several hundreds of dollars from model to model during the cross-validation. In simpler terms, the model performance was inconsistent. It performed much better when trained on some folds than when it was trained on other folds.\nThus, we can take k-fold cross-validation a step further by stratifying the folds so that they will have similar price distributions. This will ensure that each fold is representative of the full sample. Thus, I have written a custom function in the code cell below to do this.\n\n\nCode\ndef stratified_kfcv(X, y, fold_series, regression_model):\n    \"\"\"Conduct k-fold cross-validation on a stratified dataset.\"\"\"\n\n    fold_nums = fold_series.unique()\n\n    mse_lst = []\n\n    for f in fold_nums:\n        X_train, X_test, y_train, y_test = split_folds(\n            X = X,\n            y = y,\n            test_fold = f,\n            fold_series = fold_series,\n        )\n        \n        regression_model.fit(X_train, y_train)\n        y_pred = regression_model.predict(X_test)\n\n        mse = mean_squared_error(y_test, y_pred)\n        mse_lst.append(mse)\n\n    return mse_lst\n\nknn = KNeighborsRegressor(\n    n_neighbors = 5,\n    algorithm = \"auto\",\n)\n\nmse_lst = stratified_kfcv(\n    X = cars_df[all_numeric_features],\n    y = cars_df[\"price\"],\n    fold_series = folds,\n    regression_model = knn,\n)\n\nmse_series = pd.Series(mse_lst)\n\nrmse_series = mse_series.pow(1/2)\n\nmean_rmse = rmse_series.mean()\nsd_rmse = rmse_series.std(ddof = 1)\n\nprint(f\"\"\"Stratified 5-fold cross-validation\nMean RMSE: {mean_rmse:.2f}\nStandard Deviation RMSE: {sd_rmse:.2f}\nRMSE Values: {rmse_series.to_list()}\"\"\")\n\n\nStratified 5-fold cross-validation\nMean RMSE: 3369.44\nStandard Deviation RMSE: 387.33\nRMSE Values: [3193.0727214096655, 2883.515369146238, 3844.6421242541865, 3674.5947449327227, 3251.39247707809]\n\n\nThe mean RMSE from stratified CV was USD 3369. This is about USD 400 lower than the result of the regular CV, USD 3722.\nFurthermore, the SD RMSE is equal to 387, which is lower than the previous value of 566. Therefore, the five models trained during cross-validation performed more similarly to each other.\nThus, we can see that stratifying observations before k-fold cross-validation can be more effective at approximating the true performance of the model compared to regular k-fold cross-validation."
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#data-source",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#data-source",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "Data Source",
    "text": "Data Source\nSchlimmer, J. C. (1987, May 19). UCI Machine Learning Repository: Automobile Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/automobile"
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#information-sources",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#information-sources",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "Information Sources",
    "text": "Information Sources\nBrownlee, J. (2018, May 22). A Gentle Introduction to k-fold Cross-Validation. Machine Learning Mastery. https://machinelearningmastery.com/k-fold-cross-validation/\nDataquest. (n.d.). Predicting Car Prices: Machine Learning Project. Dataquest. Retrieved December 21, 2021, from https://www.dataquest.io/c/36/m/155/guided-project%3A-predicting-car-prices\nDe Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education, 19(3), null. https://doi.org/10.1080/10691898.2011.11889627\nFrost, J. (2017, April 4). How to Interpret the F-test of Overall Significance in Regression Analysis. Statistics By Jim. http://statisticsbyjim.com/regression/interpret-f-test-overall-significance-regression/\nMiller, M. (2019, October 18). The Basics: KNN for classification and regression. Medium. https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955\nscikit-learn developers. (2021). 1.13.2. Univariate Feature Selection. Scikit-Learn. https://scikit-learn/stable/modules/feature_selection.html"
  },
  {
    "objectID": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#image-source",
    "href": "posts/2021-12-21-predicting-car-prices-k-nearest-neighbors.html#image-source",
    "title": "Predicting Car Prices using the K Nearest Neighbors Algorithm",
    "section": "Image Source",
    "text": "Image Source\nSmith, C. (2018, January 20). Charger vs Challenger: All-American Muscle Car Comparison. WheelScene. https://wheelscene.com/charger-vs-challenger/"
  },
  {
    "objectID": "posts/2022-07-05-My-Learning-Journey-Data-Science.html",
    "href": "posts/2022-07-05-My-Learning-Journey-Data-Science.html",
    "title": "📌 My Learning Journey in Data Science",
    "section": "",
    "text": "Welcome to my timeline of my data science learning journey. I regularly update this post with new milestones.\n\nHow I started studying data science\nI first found out about data science in March 2020, around the time that the COVID-19 pandemic started. I decided to take online courses about data science in the summer, in order to see if it was interesting. First, I took the Python Core, Data Science, and Machine learning courses on Sololearn.\nHere, I learned basic skills in Python, the Spyder IDE, Jupyter Noteboook, SQL basics, and Python packages such as NumPy, pandas, Matplotlib, and Scikit-learn.\n\n\nData Science in Practical Research\nWhen school started again in September 2020, I was able to apply my data science skills in my project for Practical Research class. My group’s research paper was entitled “The Effect of COVID-19’s Consequences on Philippine Frontliners on their Mental Health: A Descriptive, Correlational Study.” We collected survey responses from 196 frontliners. I wrote the entire analysis in Python, from data cleaning to transformation to modeling.\nI had to learn new things in order to do this, including:\n\nStatsmodels, a statistical package in Python\nDummy-coding categorical variables\nMultiple linear regression\nTesting the assumptions of OLS regression\nInterpreting model results\n\n\n\nDataquest Online Platform\nFrom May 2021 to April 2022, I studied the Data Scientist in Python (DSP) career path on Dataquest. The courses included many guided projects in Jupyter Notebook, some of which have been posted on my blog.\nRead more about what I learned from the DSP path in this blog post.\n\n\n2021 Project SPARTA Butuan City Open Data Challenge\nI led a team to participate in this competition from June to November of 2021. I developed agriHanda, an agricultural disaster risk dashboard for Butuan City, Philippines. For this, we won the Second Placer award and Best in Ingenious Solution award.\nRead more about this competition in this blog post.\n\n\nData science projects in Grade 12\nI made the Student Mapping Project under the Programming Varsity (ProgVar) in order to help identify students who live in areas affected by natural hazards.\nUnder the Data Analytics Committee (DAC), I made the College Applications Dashboard to visualize the results of a survey about my schoolmates’ college choices and the factors influencing them.\n\n\nPassion project: Pokemon Team Inspector\nTo improve my skills in making web apps with interactive visualizations, I made an app that lets you create, edit, and save Pokemon teams, select a specific Pokemon generation whose rules you want to refer to, and analyze your Pokemon individually or as a team.\nLearn more in this blog post.\n\n\nCompetitions in 2024: Synergy Software Solutions and BlueHacks\nThis year, I’m an incoming third year in BS Applied Mathematics - Master in Data Science. My team (with five blockmates in our degree program, including myself) participated in Synergy Software Solutions 2024 and Blue Hacks 2024. We won 1st and 2nd place in these competitions, respectively. We had developed a comprehensive personal carbon footprint calculator, as well as a mental health platform for students and guidance counselors.\nRead more about it in this blog post."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#duplicate-entries",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#duplicate-entries",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Duplicate Entries",
    "text": "Duplicate Entries\nThere may be duplicate entries, i.e., multiple entries for the same date and hour. These must be removed.\n\nhighway.drop_duplicates(\n    subset = [\"date_time\"],\n    keep = \"first\",\n    inplace = True,\n)\n\nhighway.shape\n\n(40575, 9)\n\n\nThere were 7629 duplicate entries removed from the original 48204 entries."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#descriptive-statistics",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#descriptive-statistics",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nNext, we view the descriptive statistics in order to find abnormal values.\n\nhighway.describe(\n    include = \"all\",\n    datetime_is_numeric = True,\n)\n\n\n\n\n\n\n\n\nholiday\ntemp\nrain_1h\nsnow_1h\nclouds_all\nweather_main\nweather_description\ndate_time\ntraffic_volume\n\n\n\n\ncount\n40575\n40575.000000\n40575.000000\n40575.000000\n40575.000000\n40575\n40575\n40575\n40575.000000\n\n\nunique\n12\nNaN\nNaN\nNaN\nNaN\n11\n35\nNaN\nNaN\n\n\ntop\nNone\nNaN\nNaN\nNaN\nNaN\nClouds\nsky is clear\nNaN\nNaN\n\n\nfreq\n40522\nNaN\nNaN\nNaN\nNaN\n15123\n11642\nNaN\nNaN\n\n\nmean\nNaN\n281.316763\n0.318632\n0.000117\n44.199162\nNaN\nNaN\n2015-12-23 22:16:28.835489792\n3290.650474\n\n\nmin\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\nNaN\nNaN\n2012-10-02 09:00:00\n0.000000\n\n\n25%\nNaN\n271.840000\n0.000000\n0.000000\n1.000000\nNaN\nNaN\n2014-02-02 19:30:00\n1248.500000\n\n\n50%\nNaN\n282.860000\n0.000000\n0.000000\n40.000000\nNaN\nNaN\n2016-06-02 14:00:00\n3427.000000\n\n\n75%\nNaN\n292.280000\n0.000000\n0.000000\n90.000000\nNaN\nNaN\n2017-08-02 23:30:00\n4952.000000\n\n\nmax\nNaN\n310.070000\n9831.300000\n0.510000\n100.000000\nNaN\nNaN\n2018-09-30 23:00:00\n7280.000000\n\n\nstd\nNaN\n13.816618\n48.812640\n0.005676\n38.683447\nNaN\nNaN\nNaN\n1984.772909\n\n\n\n\n\n\n\nMost of the descriptives make sense, except for a few details:\n\nThe minimum temp is 0 Kelvin. This is called absolute zero, the lowest possible temperature of any object, equivalent to \\(-273.15^{\\circ}\\text{C}\\). It is unreasonable for the I-94 highway to reach such a low temperature, even in winter.\nThe maximum rain_1h is 9,831 mm, or 9.8 meters. Either this indicates a very high flood, or this is inaccurate.\nThe minimum traffic_volume is 0 cars. This may be possible, but it is still best to inspect the data."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#temperature-outliers",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#temperature-outliers",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Temperature Outliers",
    "text": "Temperature Outliers\nLet us graph a boxplot in order to find outliers among the temperature values.\n\nsns.boxplot(\n    data = highway,\n    y = \"temp\",\n)\n\nplt.title(\"Temperature on I-94 (Kelvin)\")\nplt.ylabel(\"Temperature (Kelvin)\")\nplt.grid(True)\nplt.show()\n\n\n\n\nIndeed, most values fall between 250 Kelvin and 300 Kelvin (\\(-23.15^{\\circ}\\text{C}\\) and \\(26.85^{\\circ}\\text{C}\\)). The only outliers are at 0 Kelvin. This supports the idea that the zeroes are placeholders for missing values.\nHow many missing values are there?\n\n(highway[\"temp\"]\n .value_counts(bins = 10)\n .sort_index()\n)\n\n(-0.311, 31.007]         10\n(31.007, 62.014]          0\n(62.014, 93.021]          0\n(93.021, 124.028]         0\n(124.028, 155.035]        0\n(155.035, 186.042]        0\n(186.042, 217.049]        0\n(217.049, 248.056]       99\n(248.056, 279.063]    17372\n(279.063, 310.07]     23094\nName: temp, dtype: int64\n\n\nOnly 10 datapoints have zero-values in temp. Thus, these can be dropped from the dataset.\n\nhighway = highway.loc[highway[\"temp\"] != 0]\n\nhighway.shape\n\n(40565, 9)\n\n\nNow, there are 40565 rows in the dataset. Temperature outliers have been removed."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#rain-level-outliers",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#rain-level-outliers",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Rain Level Outliers",
    "text": "Rain Level Outliers\nSimilarly, we graph a boxplot below for the rain_1h column.\n\nsns.boxplot(\n    data = highway,\n    y = \"rain_1h\",\n)\n\nplt.title(\"Hourly Rain Level on I-94 (mm)\")\nplt.ylabel(\"Hourly Rain Level (mm)\")\nplt.grid(True)\nplt.show()\n\n\n\n\nMost of the values are close to 0 mm, and there are only a few outliers near 10,000 mm. How many outliers are there?\n\nhighway[\"rain_1h\"].value_counts(bins = 10).sort_index()\n\n(-9.831999999999999, 983.13]    40564\n(983.13, 1966.26]                   0\n(1966.26, 2949.39]                  0\n(2949.39, 3932.52]                  0\n(3932.52, 4915.65]                  0\n(4915.65, 5898.78]                  0\n(5898.78, 6881.91]                  0\n(6881.91, 7865.04]                  0\n(7865.04, 8848.17]                  0\n(8848.17, 9831.3]                   1\nName: rain_1h, dtype: int64\n\n\nThere is only 1 outlying datapoint. Since a 9.8 m flood level is so unrealistic given that most of the other values are small, this datapoint will be dropped.\n\nhighway = highway.loc[highway[\"rain_1h\"] &lt; 1000]\n\nhighway.shape\n\n(40564, 9)\n\n\nThe dataset is left with 40564 rows."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#traffic-volume-outliers",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#traffic-volume-outliers",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Traffic Volume Outliers",
    "text": "Traffic Volume Outliers\nBelow is the boxplot of traffic volume values. We want to see if the zero-values are reasonable or if these are distant outliers.\n\nsns.boxplot(\n    data = highway,\n    y = \"traffic_volume\",\n)\n\nplt.title(\"Traffic Volume on I-94\")\nplt.ylabel(\"Hourly Number of Cars\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe boxplot shows that 0 is within the approximate lower bound. It is not too distant from most of the datapoints to be considered an outlier.\nLet us view a histogram to understand the distribution better.\n\nsns.histplot(\n    data = highway,\n    x = \"traffic_volume\",\n)\n\nplt.title(\"Traffic Volume on I-94\")\nplt.xlabel(\"Hourly Number of Cars\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThis is an unusual distribution. There appear to be 3 peaks:\n\nless than 1000 cars\naround 3000 cars\naround 4500 cars\n\nIt is common that less than 1000 cars pass through this I-94 station per hour. Therefore, it is likely that the 0-values are not outliers and do not need to be dropped from the dataset.\nData cleaning is done, so here are the new descriptive statistics for the dataset.\n\nhighway.describe(\n    include = \"all\",\n    datetime_is_numeric = True,\n)\n\n\n\n\n\n\n\n\nholiday\ntemp\nrain_1h\nsnow_1h\nclouds_all\nweather_main\nweather_description\ndate_time\ntraffic_volume\n\n\n\n\ncount\n40564\n40564.000000\n40564.000000\n40564.000000\n40564.000000\n40564\n40564\n40564\n40564.000000\n\n\nunique\n12\nNaN\nNaN\nNaN\nNaN\n11\n35\nNaN\nNaN\n\n\ntop\nNone\nNaN\nNaN\nNaN\nNaN\nClouds\nsky is clear\nNaN\nNaN\n\n\nfreq\n40511\nNaN\nNaN\nNaN\nNaN\n15123\n11632\nNaN\nNaN\n\n\nmean\nNaN\n281.385602\n0.076353\n0.000117\n44.209299\nNaN\nNaN\n2015-12-24 02:14:28.937974528\n3291.081402\n\n\nmin\nNaN\n243.390000\n0.000000\n0.000000\n0.000000\nNaN\nNaN\n2012-10-02 09:00:00\n0.000000\n\n\n25%\nNaN\n271.850000\n0.000000\n0.000000\n1.000000\nNaN\nNaN\n2014-02-03 02:45:00\n1249.750000\n\n\n50%\nNaN\n282.867500\n0.000000\n0.000000\n40.000000\nNaN\nNaN\n2016-06-02 19:30:00\n3429.000000\n\n\n75%\nNaN\n292.280000\n0.000000\n0.000000\n90.000000\nNaN\nNaN\n2017-08-03 02:15:00\n4952.000000\n\n\nmax\nNaN\n310.070000\n55.630000\n0.510000\n100.000000\nNaN\nNaN\n2018-09-30 23:00:00\n7280.000000\n\n\nstd\nNaN\n13.092942\n0.769729\n0.005677\n38.682163\nNaN\nNaN\nNaN\n1984.638849\n\n\n\n\n\n\n\nDue to data cleaning, the following have changed:\n\nMinimum temp is 243.39 Kelvin\nMaximum rain_1h is 55.63 mm\n\nThese are more reasonable values than before."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#traffic-volume-day-vs.-night",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#traffic-volume-day-vs.-night",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Traffic Volume: Day vs. Night",
    "text": "Traffic Volume: Day vs. Night\nAt the end of the data cleaning, we noticed that there were 3 peaks (most common values) in the traffic volume data. It is possible that this can be explained by comparing traffic volume between daytime and nighttime.\nIn order to do this, we can make a new column half which labels each entry as “day” or “night.” We will consider daytime to be from 6:00 AM to 6:00 PM, or 6:00 to 18:00.\n\nhighway[\"half\"] = (\n    highway[\"date_time\"]\n    .dt.hour.between(6, 17) # Boolean Series where True represents day\n    .replace({True: \"day\", False: \"night\"}) # Replace booleans with strings\n)\n\nhighway[\"half\"].value_counts()\n\nnight    20418\nday      20146\nName: half, dtype: int64\n\n\nThere are 20418 nighttime entries and 20146 daytime entries.\nNow we can compare the day and night histograms for traffic volume.\n\nsns.histplot(\n    data = highway,\n    x = \"traffic_volume\",\n    hue = \"half\",\n    palette = \"RdBu\",\n)\n\nplt.title(\"Traffic Volume on I-94: Day and Night\")\nplt.xlabel(\"Hourly Number of Cars\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe histogram above shows that:\n\nIn the nighttime, the traffic volume is commonly under 1000 or around 3000.\nIn the daytime, the traffic volume is commonly around 5000.\n\nTherefore, traffic is generally heavier in the daytime, between 6:00 AM and 6:00 PM.\nSince we want to determine what influences heavy traffic, it would be best to focus our analysis on the daytime entries. Thus, such entries will be put in a separate DataFrame called daytime.\n\ndaytime = (\n    highway\n    .loc[highway[\"half\"] == \"day\"]\n    .drop(columns = \"half\")\n)\n\ndaytime.describe(\n    include = \"all\",\n    datetime_is_numeric = True,\n)\n\n\n\n\n\n\n\n\nholiday\ntemp\nrain_1h\nsnow_1h\nclouds_all\nweather_main\nweather_description\ndate_time\ntraffic_volume\n\n\n\n\ncount\n20146\n20146.000000\n20146.000000\n20146.000000\n20146.000000\n20146\n20146\n20146\n20146.000000\n\n\nunique\n1\nNaN\nNaN\nNaN\nNaN\n10\n33\nNaN\nNaN\n\n\ntop\nNone\nNaN\nNaN\nNaN\nNaN\nClouds\nsky is clear\nNaN\nNaN\n\n\nfreq\n20146\nNaN\nNaN\nNaN\nNaN\n8360\n4971\nNaN\nNaN\n\n\nmean\nNaN\n282.018771\n0.076210\n0.000124\n47.342500\nNaN\nNaN\n2015-12-26 11:32:49.582050816\n4784.630100\n\n\nmin\nNaN\n243.390000\n0.000000\n0.000000\n0.000000\nNaN\nNaN\n2012-10-02 09:00:00\n1.000000\n\n\n25%\nNaN\n272.220000\n0.000000\n0.000000\n1.000000\nNaN\nNaN\n2014-02-04 08:15:00\n4311.000000\n\n\n50%\nNaN\n283.640000\n0.000000\n0.000000\n40.000000\nNaN\nNaN\n2016-06-07 11:30:00\n4943.000000\n\n\n75%\nNaN\n293.370000\n0.000000\n0.000000\n90.000000\nNaN\nNaN\n2017-08-06 09:45:00\n5678.000000\n\n\nmax\nNaN\n310.070000\n44.450000\n0.510000\n100.000000\nNaN\nNaN\n2018-09-30 17:00:00\n7280.000000\n\n\nstd\nNaN\n13.330019\n0.737429\n0.005673\n37.808456\nNaN\nNaN\nNaN\n1293.502893\n\n\n\n\n\n\n\nThis DataFrame contains only 20146 rows. The descriptive statistics are naturally somewhat different from before."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#effect-of-units-of-time",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#effect-of-units-of-time",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Effect of Units of Time",
    "text": "Effect of Units of Time\nIt is possible that traffic volume is influenced by certain units of time. For example, it could be influenced by the month, the day of the week, or the hour of the day. In this section, we investigate these factors.\n\nBy Month\nFirst, does the month affect the traffic volume?\nLet us make a new column that indicates the month as a number.\n\ndaytime[\"month\"] = daytime[\"date_time\"].dt.month\n\ndaytime[\"month\"]\n\n0        10\n1        10\n2        10\n3        10\n4        10\n         ..\n48191     9\n48192     9\n48194     9\n48196     9\n48197     9\nName: month, Length: 20146, dtype: int64\n\n\nThen, we can calculate and graph the average traffic volume per month. The median will be used instead of the mean since the data are not normally distributed.\nTable:\n\n(daytime\n .groupby(\"month\")\n .median()\n [[\"traffic_volume\"]]\n)\n\n\n\n\n\n\n\n\ntraffic_volume\n\n\nmonth\n\n\n\n\n\n1\n4651.5\n\n\n2\n4886.0\n\n\n3\n5062.0\n\n\n4\n5105.0\n\n\n5\n5052.0\n\n\n6\n5050.0\n\n\n7\n4799.5\n\n\n8\n5056.0\n\n\n9\n4925.5\n\n\n10\n5056.0\n\n\n11\n4876.0\n\n\n12\n4722.0\n\n\n\n\n\n\n\nLine chart:\n\nsns.lineplot(\n    data = daytime,\n    x = \"month\",\n    y = \"traffic_volume\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Effect of Month on Traffic Volume\")\nplt.xlabel(\"Month\"),\nplt.ylabel(\"Median Hourly Number of Cars\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe line chart shows that the median traffic volume is highest in April, possibly because this is in spring. Traffic volume is lowest in January and December since these are in the middle of winter. July also has less traffic since it is in the middle of summer.\n\n\nBy Day of the Week\nNext, we will investigate the effect of the day of the week on the traffic volume.\n\ndaytime[\"day\"] = daytime[\"date_time\"].dt.dayofweek\n\n(daytime\n .groupby(\"day\")\n .median()\n [[\"traffic_volume\"]]\n)\n\n\n\n\n\n\n\n\ntraffic_volume\n\n\nday\n\n\n\n\n\n0\n4971.5\n\n\n1\n5268.0\n\n\n2\n5355.0\n\n\n3\n5404.0\n\n\n4\n5399.0\n\n\n5\n4194.0\n\n\n6\n3737.0\n\n\n\n\n\n\n\nNote that 0 means Monday and 6 means Sunday.\nThe corresponding line chart is shown below.\n\nsns.lineplot(\n    data = daytime,\n    x = \"day\",\n    y = \"traffic_volume\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Effect of Day of Week on Traffic Volume\")\nplt.xlabel(\"Day of the Week\"),\nplt.ylabel(\"Median Hourly Number of Cars\")\nplt.grid(True)\nplt.show()\n\n\n\n\nThe line chart shows that the traffic volume is very high from Monday to Friday, then dips by 1000 cars on Saturday and Sunday. It makes sense that traffic is heavier on weekdays and lighter on weekends.\n\n\nBy Hour of the Day\nLastly, we will investigate the effect of the time of day on the traffic volume. Since we have narrowed the dataset down to daytime entries, only hours from 6:00 AM to 6:00 PM are included.\nFirst, though, let’s make a column that indicates weekdays and weekends so that we can compare them.\n\ndaytime[\"day_type\"] = daytime[\"day\"].replace({\n    0: \"business day\",\n    1: \"business day\",\n    2: \"business day\",\n    3: \"business day\",\n    4: \"business day\",\n    5: \"weekend\",\n    6: \"weekend\",\n})\n\nNext, below is the table of median traffic volume grouped by the day of the week and the type of day.\n\ndaytime[\"hour\"] = daytime[\"date_time\"].dt.hour\n\n(daytime\n .groupby([\"hour\", \"day_type\"])\n .median()\n [[\"traffic_volume\"]]\n)\n\n\n\n\n\n\n\n\n\ntraffic_volume\n\n\nhour\nday_type\n\n\n\n\n\n6\nbusiness day\n5588.0\n\n\nweekend\n1092.0\n\n\n7\nbusiness day\n6320.5\n\n\nweekend\n1547.0\n\n\n8\nbusiness day\n5751.0\n\n\nweekend\n2268.0\n\n\n9\nbusiness day\n5053.0\n\n\nweekend\n3147.0\n\n\n10\nbusiness day\n4484.5\n\n\nweekend\n3722.0\n\n\n11\nbusiness day\n4714.0\n\n\nweekend\n4114.0\n\n\n12\nbusiness day\n4921.0\n\n\nweekend\n4442.5\n\n\n13\nbusiness day\n4919.5\n\n\nweekend\n4457.0\n\n\n14\nbusiness day\n5232.0\n\n\nweekend\n4457.0\n\n\n15\nbusiness day\n5740.0\n\n\nweekend\n4421.0\n\n\n16\nbusiness day\n6403.5\n\n\nweekend\n4438.0\n\n\n17\nbusiness day\n5987.0\n\n\nweekend\n4261.5\n\n\n\n\n\n\n\nThe hours are in 24-hour time; 17:00 represents the hour from 5:00 PM to 6:00 PM.\nIn order to understand this table better, we can visualize it in the line chart below.\n\nsns.lineplot(\n    data = daytime,\n    x = \"hour\",\n    y = \"traffic_volume\",\n    hue = \"day_type\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Effect of Hour of Day on Traffic Volume\")\nplt.xlabel(\"Hour of the Day\"),\nplt.ylabel(\"Median Hourly Number of Cars\")\nplt.legend(title = \"Day Type\")\nplt.grid(True)\nplt.show()\n\n\n\n\nOn business days, traffic is heaviest at 7:00 AM and 4:00 PM. These are the times when people travel to or from work. Traffic is lightest around noontime.\nOn weekends, traffic volume increases from 6:00 AM to 12:00 PM and plateaus from there on. People are free to travel at any time on weekends since most don’t have work. However, the number of cars is still lower on weekends compared to business days."
  },
  {
    "objectID": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#effect-of-weather",
    "href": "posts/2021-05-25-indicators-heavy-traffic-i-94-highway.html#effect-of-weather",
    "title": "Indicators of Heavy Traffic on the I-94 Highway",
    "section": "Effect of Weather",
    "text": "Effect of Weather\nUp until now, we have investigated the possible effects of different units of time on the traffic volume. In this section, we focus on how traffic is affected by the weather.\nThe following are the weather-related columns in the dataset:\n\ntemp\nrain_1h\nsnow_1h\nclouds_all\nweather_main\nweather_description\n\nThe first 4 are numerical and the last 2 are categorical.\n\nNumerical Weather Columns\nLet us inspect the Pearson’s correlation coefficient between traffic volume and each of the numerical weather columns.\n\ndaytime.corr().loc[\n    [\"temp\", \"rain_1h\", \"snow_1h\", \"clouds_all\"],\n    [\"traffic_volume\"]\n]\n\n\n\n\n\n\n\n\ntraffic_volume\n\n\n\n\ntemp\n0.124311\n\n\nrain_1h\n-0.022817\n\n\nsnow_1h\n-0.004145\n\n\nclouds_all\n0.000621\n\n\n\n\n\n\n\nAll 4 numerical weather columns appear to have very weak correlations with traffic volume.\nThe highest correlation involves temperature, but the coefficient is only 12.43. This indicates a weak positive relationship. As temperature increases, traffic volume also increases, but not consistently.\nWe can understand the correlation better using a scatter plot.\n\nsns.scatterplot(\n    data = daytime,\n    x = \"temp\",\n    y = \"traffic_volume\",\n    ci = None,\n)\n\nplt.title(\"Effect of Temperature on Traffic Volume\")\nplt.xlabel(\"Temperature (Kelvin)\"),\nplt.ylabel(\"Median Hourly Number of Cars\")\nplt.grid(True)\nplt.show()\n\n\n\n\nUnfortunately, the datapoints are scattered quite consistently throughout all combinations of temperature and traffic volume. The correlation is weak; temperature is not a reliable indicator of traffic. Neither are the other numerical weather columns, since their coefficients were even weaker.\n\n\nCategorical Weather Columns\nNext, we’ll see if the categorical weather columns can serve as better indicators of heavy traffic.\n\nShort Descriptions of Weather\nThe weather_main column contains short, 1-word descriptions of the weather.\nWhat are the categories under weather_main?\n\ndaytime[\"weather_main\"].value_counts()\n\nClouds          8360\nClear           5821\nRain            2392\nMist            1441\nSnow            1165\nHaze             472\nDrizzle          223\nThunderstorm     175\nFog               90\nSmoke              7\nName: weather_main, dtype: int64\n\n\nClouds, Clear, and Rain are the most frequent descriptions of the weather.\nNext, let us investigate the effect of the weather description on the traffic volume. The table is shown below.\n\n(daytime\n .groupby(\"weather_main\")\n .median()\n [[\"traffic_volume\"]]\n)\n\n\n\n\n\n\n\n\ntraffic_volume\n\n\nweather_main\n\n\n\n\n\nClear\n4936.0\n\n\nClouds\n4974.5\n\n\nDrizzle\n5132.0\n\n\nFog\n5588.0\n\n\nHaze\n4817.5\n\n\nMist\n5053.0\n\n\nRain\n4975.0\n\n\nSmoke\n4085.0\n\n\nSnow\n4570.0\n\n\nThunderstorm\n4875.0\n\n\n\n\n\n\n\nThis is visualized in the bar plot below.\n\nsns.barplot(\n    data = daytime,\n    x = \"weather_main\",\n    y = \"traffic_volume\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Effect of Short Weather Description on Traffic Volume\")\nplt.xlabel(\"Short Weather Description\"),\nplt.ylabel(\"Median Hourly Number of Cars\")\nplt.xticks(rotation = 45)\nplt.grid(True)\nplt.show()\n\n\n\n\nThe traffic volume appears to be mostly consistent across short weather descriptions. Notably:\n\nTraffic is heaviest during fog (5588 cars).\nTraffic is lightest when there is smoke (4085 cars).\n\nHowever, the effects are quite small, reaching only up to a difference of 1000 cars.\n\n\nLong Descriptions of Weather\nThe weather_description column contains longer descriptions of the weather.\nBelow are its categories.\n\ndaytime[\"weather_description\"].value_counts()\n\nsky is clear                           4971\nbroken clouds                          2683\novercast clouds                        2526\nscattered clouds                       2068\nmist                                   1441\nlight rain                             1415\nfew clouds                             1083\nSky is Clear                            850\nlight snow                              811\nmoderate rain                           666\nhaze                                    472\nheavy snow                              249\nheavy intensity rain                    207\nlight intensity drizzle                 160\nproximity thunderstorm                  136\nsnow                                     90\nfog                                      90\nproximity shower rain                    89\ndrizzle                                  57\nthunderstorm                             22\nlight shower snow                        11\nlight intensity shower rain               7\nsmoke                                     7\nthunderstorm with light rain              7\nvery heavy rain                           7\nheavy intensity drizzle                   6\nthunderstorm with heavy rain              5\nthunderstorm with rain                    3\nsleet                                     3\nlight rain and snow                       1\nfreezing rain                             1\nproximity thunderstorm with drizzle       1\nproximity thunderstorm with rain          1\nName: weather_description, dtype: int64\n\n\nNotice that there is a sky is clear value and a Sky is Clear value with different capitalization. It is likely that these two categories mean the same thing, so let us combine them.\n\ndaytime[\"weather_description\"].replace(\n    {\"Sky is Clear\": \"sky is clear\"},\n    inplace = True,\n)\n\ndaytime[\"weather_description\"].value_counts().head()\n\nsky is clear        5821\nbroken clouds       2683\novercast clouds     2526\nscattered clouds    2068\nmist                1441\nName: weather_description, dtype: int64\n\n\nThe sky is clear category now has 5821 entries.\nNow that that’s cleaned, let’s make a table showing the effect of the long weather description on the traffic volume.\n\n(daytime\n .groupby(\"weather_description\")\n .median()\n [[\"traffic_volume\"]]\n)\n\n\n\n\n\n\n\n\ntraffic_volume\n\n\nweather_description\n\n\n\n\n\nbroken clouds\n4925.0\n\n\ndrizzle\n5132.0\n\n\nfew clouds\n4977.0\n\n\nfog\n5588.0\n\n\nfreezing rain\n4762.0\n\n\nhaze\n4817.5\n\n\nheavy intensity drizzle\n5824.5\n\n\nheavy intensity rain\n4922.0\n\n\nheavy snow\n4673.0\n\n\nlight intensity drizzle\n5086.5\n\n\nlight intensity shower rain\n4695.0\n\n\nlight rain\n5011.0\n\n\nlight rain and snow\n5544.0\n\n\nlight shower snow\n4324.0\n\n\nlight snow\n4601.0\n\n\nmist\n5053.0\n\n\nmoderate rain\n4940.0\n\n\novercast clouds\n4968.0\n\n\nproximity shower rain\n4910.0\n\n\nproximity thunderstorm\n4849.5\n\n\nproximity thunderstorm with drizzle\n6667.0\n\n\nproximity thunderstorm with rain\n5730.0\n\n\nscattered clouds\n5032.5\n\n\nsky is clear\n4936.0\n\n\nsleet\n5174.0\n\n\nsmoke\n4085.0\n\n\nsnow\n4032.0\n\n\nthunderstorm\n5578.0\n\n\nthunderstorm with heavy rain\n5278.0\n\n\nthunderstorm with light rain\n4073.0\n\n\nthunderstorm with rain\n4270.0\n\n\nvery heavy rain\n4802.0\n\n\n\n\n\n\n\nThe table is visualized in the bar graph below.\n\nplt.figure(figsize = (14, 5))\n\nsns.barplot(\n    data = daytime,\n    x = \"weather_description\",\n    y = \"traffic_volume\",\n    estimator = np.median,\n    ci = None,\n)\n\nplt.title(\"Effect of Long Weather Description on Traffic Volume\")\nplt.xlabel(\"Long Weather Description\"),\nplt.ylabel(\"Median Hourly Number of Cars\")\nplt.xticks(rotation = 45, ha = \"right\")\nplt.grid(True)\nplt.show()\n\n\n\n\nSimilar to the bar graph of short weather descriptions, most of the values are around 5000 cars. Notably:\n\nTraffic is heaviest in a proximity thunderstorm with drizzle (6667 cars).\n\nThe word “proximity” was likely used to emphasize that the storm was very close to the station.\nIt makes sense that a drizzling thunderstorm directly over the highway would reduce visibility and make traffic pile up.\n\nTraffic is lightest in snow (4032 cars).\n\nIt is likely that more people choose not to travel outside when it is snowing."
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#duplicate-rows",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#duplicate-rows",
    "title": "Optimizing Hacker News Posts",
    "section": "Duplicate Rows",
    "text": "Duplicate Rows\nBelow, I use pandas to delete duplicate rows except for the first instance of each duplicate.\nRows will be considered as duplicates if they are exactly alike in all features. I decided on this because it is possible for two posts to have the same title and/or url but be posted at different times or by different users. Thus, we cannot identify duplicates based on one or two features alone.\n\nhn = hn.drop_duplicates(keep = \"first\")\nprint(hn.shape)\n\n(293119, 7)\n\n\nNo duplicates were found. All rows were kept."
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#posts-without-comments",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#posts-without-comments",
    "title": "Optimizing Hacker News Posts",
    "section": "Posts without Comments",
    "text": "Posts without Comments\nOur research questions involve the number of comments on each post. However, there are many posts with 0 comments.\nTo illustrate this, below a frequency table of the number of comments on each post.\n\ndef freq_comments(df = hn):\n    \n    \"\"\"Function to make a frequency table of the number of comments per post\n    specifically for the Hacker News dataset.\"\"\"\n\n    freq_df = df[\"num_comments\"].value_counts().reset_index()\n    \n    freq_df.columns = [\"num_comments\", \"frequency\"]\n    \n    freq_df = freq_df.sort_values(\n        by = \"num_comments\",\n    ).reset_index(\n        drop = True,\n    )\n\n    return freq_df\n\nfreq_df1 = freq_comments()\n\nfreq_df1\n\n\n\n\n\n\n\n\nnum_comments\nfrequency\n\n\n\n\n0\n0\n212718\n\n\n1\n1\n28055\n\n\n2\n2\n9731\n\n\n3\n3\n5016\n\n\n4\n4\n3272\n\n\n...\n...\n...\n\n\n543\n1007\n1\n\n\n544\n1120\n1\n\n\n545\n1448\n1\n\n\n546\n1733\n1\n\n\n547\n2531\n1\n\n\n\n\n548 rows × 2 columns\n\n\n\nThe table above shows that posts with 0 comments are most frequent.\nLet us plot the table on a histogram.\n\ndef hist_comments(df, title):\n\n    \"\"\"Function to make a histogram of the number of comments per post\n    specifically for the Hacker News dataset.\"\"\"\n    \n    chart = alt.Chart(df).mark_bar().encode(\n        x = alt.X(\n            \"num_comments:Q\",\n            title = \"Number of Comments\",\n            bin = alt.Bin(step = 1)\n        ),\n        y = alt.Y(\n            \"frequency:Q\",\n            title = \"Frequency\",\n        ),\n    ).properties(\n        title = title,\n        width = 700,\n        height = 400,\n    )\n    \n    return chart\n    \nhist_comments(freq_df1, \"Histogram of Number of Comments per Post\")\n\n\n\n\n\n\nThere are so many posts with 0 comments that we cannot see the histogram bins for other numbers of comments.\nConsidering that the dataset is large and most rows have 0 comments, it would be best to drop all rows with 0 comments. This would make analysis less computationally expensive and allow us to answer our research questions.\n\nwith_comments = hn[\"num_comments\"] &gt; 0\nhn = hn.loc[with_comments].reset_index(drop = True)\n\nprint(hn.shape)\n\n(80401, 7)\n\n\nNow, the dataset is left with only 80,401 rows. This will be easier to work with.\nBelow is the new histogram.\n\nfreq_df2 = freq_comments()\n\nhist_comments(freq_df2, \"Histogram of Number of Comments per Post\")\n\n\n\n\n\n\nThe distribution is still heavily right-skewed since many posts have very few comments. What’s important is that unnecessary data has been removed."
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#missing-values",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#missing-values",
    "title": "Optimizing Hacker News Posts",
    "section": "Missing Values",
    "text": "Missing Values\nFinally, let us remove rows with missing values. In order to answer our research questions, we only need the following columns:\n\ntitle\nnum_comments\ncreated_at\n\nThus, we will delete rows with missing values in this column.\n\nhn.dropna(\n    subset = [\"title\", \"num_comments\", \"created_at\"],\n    inplace = True,\n)\n\nprint(hn.shape)\n\n(80401, 7)\n\n\nThe number of rows did not change from 80401. Therefore, no missing values were found in these columns, and no rows were dropped.\nData cleaning is now done."
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#string-template-for-time",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#string-template-for-time",
    "title": "Optimizing Hacker News Posts",
    "section": "String Template for Time",
    "text": "String Template for Time\nBefore analying, we need to inspect the “created_at” column of the dataset.\n\nhn_ask = hn.loc[\n    hn[\"post_type\"] == \"Ask\"\n].reset_index(\n    drop = True,\n)\n\nhn_ask[[\"created_at\"]].head()\n\n\n\n\n\n\n\n\ncreated_at\n\n\n\n\n0\n9/26/2016 2:53\n\n\n1\n9/26/2016 1:17\n\n\n2\n9/25/2016 22:48\n\n\n3\n9/25/2016 21:50\n\n\n4\n9/25/2016 19:30\n\n\n\n\n\n\n\nThe strings in this column appear to follow the following format:\n\nmonth/day/year hour:minute\n\nWith the datetime module, the following is the equivalent formatting template.\n\ntemplate = \"%m/%d/%Y %H:%M\""
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#parsing-times",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#parsing-times",
    "title": "Optimizing Hacker News Posts",
    "section": "Parsing Times",
    "text": "Parsing Times\nThe time data can now be parsed and used for analysis.\n\nhn_ask[\"created_at\"] = pd.to_datetime(\n    hn_ask[\"created_at\"],\n    format = template,\n)\n\nhn_ask[\"created_at\"].head()\n\n0   2016-09-26 02:53:00\n1   2016-09-26 01:17:00\n2   2016-09-25 22:48:00\n3   2016-09-25 21:50:00\n4   2016-09-25 19:30:00\nName: created_at, dtype: datetime64[ns]\n\n\nThe column is now in datetime format.\nWith this data, we will make 2 dictionaries. The hours_posts dictionary will count the number of posts at certain hours. The hours_comments dictionary will count the number of comments received by posts made at certain hours.\n\nhours_posts = {}\nhours_comments = {}\n\nfor index, row in hn_ask.iterrows():\n    date_dt = row[\"created_at\"]\n    num_comments = row[\"num_comments\"]\n    \n    # extract hour\n    hour = date_dt.hour\n    \n    # update dictionaries\n    hours_posts.setdefault(hour, 0)\n    hours_posts[hour] += 1\n    \n    hours_comments.setdefault(hour, 0)\n    hours_comments[hour] += num_comments\n\nThe hours were parsed and mapped to their respective counts of posts and comments.\nThe code below transforms the dictionaries into DataFrames for ease of use.\n\ndef hour_to_df(dct, data_label):\n    \n    \"\"\"Make a DataFrame from a dictionary that maps\n    an 'hour' column to another column, named by `data_label`.\"\"\"\n    \n    result = pd.DataFrame.from_dict(\n        dct,\n        orient = \"index\",\n    ).reset_index(\n    ).rename(columns = {\n        \"index\": \"hour\",\n        0: data_label,\n    }).sort_values(\n        by = \"hour\",\n    ).reset_index(\n        drop = True,\n    )\n    \n    return result\n    \nhours_posts_df = hour_to_df(\n    hours_posts,\n    data_label = \"num_posts\",\n)\n\nhours_comments_df = hour_to_df(\n    hours_comments,\n    data_label = \"num_comments\",\n)\n\nhours_posts_df.head()\n\n\n\n\n\n\n\n\nhour\nnum_posts\n\n\n\n\n0\n0\n228\n\n\n1\n1\n222\n\n\n2\n2\n227\n\n\n3\n3\n210\n\n\n4\n4\n184\n\n\n\n\n\n\n\n\nhours_comments_df.head()\n\n\n\n\n\n\n\n\nhour\nnum_comments\n\n\n\n\n0\n0\n2261\n\n\n1\n1\n2068\n\n\n2\n2\n2996\n\n\n3\n3\n2152\n\n\n4\n4\n2353\n\n\n\n\n\n\n\nThe hours have been parsed, and the tables have been generated.\nAdditionally, another DataFrame is created below. It calculates the median number of comments per post by the hour posted.\n\nhn_ask[\"hour\"] = hn_ask[\"created_at\"].dt.hour\n\nhours_median = hn_ask.pivot_table(\n    index = \"hour\",\n    values = \"num_comments\",\n    aggfunc = np.median,\n).reset_index()\n\nhours_median\n\n\n\n\n\n\n\n\nhour\nnum_comments\n\n\n\n\n0\n0\n3.0\n\n\n1\n1\n3.0\n\n\n2\n2\n4.0\n\n\n3\n3\n3.0\n\n\n4\n4\n4.0\n\n\n5\n5\n3.0\n\n\n6\n6\n3.0\n\n\n7\n7\n4.0\n\n\n8\n8\n3.5\n\n\n9\n9\n3.0\n\n\n10\n10\n4.0\n\n\n11\n11\n4.0\n\n\n12\n12\n4.0\n\n\n13\n13\n4.0\n\n\n14\n14\n3.0\n\n\n15\n15\n4.0\n\n\n16\n16\n3.0\n\n\n17\n17\n4.0\n\n\n18\n18\n3.0\n\n\n19\n19\n3.0\n\n\n20\n20\n4.0\n\n\n21\n21\n3.0\n\n\n22\n22\n4.0\n\n\n23\n23\n4.0"
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#number-of-posts-by-hour-of-the-day",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#number-of-posts-by-hour-of-the-day",
    "title": "Optimizing Hacker News Posts",
    "section": "Number of Posts by Hour of the Day",
    "text": "Number of Posts by Hour of the Day\nBelow is a table showing the total number of posts that were created, grouped by hour of the day.\n\nhours_posts_df\n\n\n\n\n\n\n\n\nhour\nnum_posts\n\n\n\n\n0\n0\n228\n\n\n1\n1\n222\n\n\n2\n2\n227\n\n\n3\n3\n210\n\n\n4\n4\n184\n\n\n5\n5\n165\n\n\n6\n6\n176\n\n\n7\n7\n156\n\n\n8\n8\n190\n\n\n9\n9\n176\n\n\n10\n10\n218\n\n\n11\n11\n250\n\n\n12\n12\n272\n\n\n13\n13\n323\n\n\n14\n14\n377\n\n\n15\n15\n467\n\n\n16\n16\n412\n\n\n17\n17\n402\n\n\n18\n18\n450\n\n\n19\n19\n418\n\n\n20\n20\n392\n\n\n21\n21\n407\n\n\n22\n22\n286\n\n\n23\n23\n276\n\n\n\n\n\n\n\nThis table is in 24-hour time. Hour 13 refers to 1:00 PM. The table shows how many posts are made for every hour in the day.\nBelow is a line chart that shows this visually.\n\nchart = alt.Chart(hours_posts_df).mark_line().encode(\n    x = alt.X(\n        \"hour:O\", title = \"Hour of the Day\",\n        axis = alt.Axis(labelAngle = 0)\n    ),\n    y = alt.Y(\"num_posts:Q\", title = \"Number of Posts\"),\n).properties(\n    title = \"Number of Posts by Hour of the Day\",\n    width = 700,\n    height = 400,\n).configure_axis(\n    grid = True,\n)\n\nchart\n\n\n\n\n\n\nThe histogram clearly shows that Hacker News users most actively make posts between 15:00 and 18:00, or from 3:00 PM to 6:00 PM.\nThe most active hour for posting is 3:00 PM - 4:00 PM."
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#number-of-comments-by-hour-posted",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#number-of-comments-by-hour-posted",
    "title": "Optimizing Hacker News Posts",
    "section": "Number of Comments by Hour Posted",
    "text": "Number of Comments by Hour Posted\nNext, a similar analysis is done for the total number of comments written by Hacker News users, grouped by the hour that the original posts were created. Below is the table for this data.\n\nhours_comments_df\n\n\n\n\n\n\n\n\nhour\nnum_comments\n\n\n\n\n0\n0\n2261\n\n\n1\n1\n2068\n\n\n2\n2\n2996\n\n\n3\n3\n2152\n\n\n4\n4\n2353\n\n\n5\n5\n1838\n\n\n6\n6\n1587\n\n\n7\n7\n1584\n\n\n8\n8\n2362\n\n\n9\n9\n1477\n\n\n10\n10\n3011\n\n\n11\n11\n2794\n\n\n12\n12\n4226\n\n\n13\n13\n7219\n\n\n14\n14\n4970\n\n\n15\n15\n18525\n\n\n16\n16\n4458\n\n\n17\n17\n5536\n\n\n18\n18\n4824\n\n\n19\n19\n3949\n\n\n20\n20\n4462\n\n\n21\n21\n4500\n\n\n22\n22\n3369\n\n\n23\n23\n2297\n\n\n\n\n\n\n\nBelow is the line chart that visualizes the table.\n\nchart = alt.Chart(hours_comments_df).mark_line().encode(\n    x = alt.X(\n        \"hour:O\", title = \"Hour of the Day\",\n        axis = alt.Axis(labelAngle = 0)\n    ),\n    y = alt.Y(\"num_comments:Q\", title = \"Number of Comments\"),\n).properties(\n    title = \"Number of Comments by Hour Posted\",\n    width = 700,\n    height = 400,\n).configure_axis(\n    grid = True,\n)\n\nchart\n\n\n\n\n\n\nThe line chart shows that many comments are made on posts created from 13:00 to 17:00, or 1:00 PM - 5:00 PM.\nNotably, there was a total of over 18,000 comments on posts created at 3:00 PM. This may suggest that Hacker News users most actively comment at around this time.\nHowever, there is also the possibility that this total was influenced by a few outliers. Let’s check the distribution of the number of comments made at 3:00 PM.\n\nalt.Chart(\n    hn_ask.loc[hn_ask[\"hour\"] == 15]\n).mark_bar().encode(\n    x = alt.X(\"num_comments:Q\"),\n    y = alt.Y(\"count()\")\n).properties(\n    title = \"Distribution of Number of Comments per Post (3:00 PM)\",\n    width = 700,\n    height = 200,\n).configure_axis(\n    grid = True,\n)\n\n\n\n\n\n\n\nIndeed, there are several outlier posts with a very high number of comments, going up to 1000. These influenced the spike in the line chart.\nTherefore, we can’t say that a post will definitely get many comments if it is posted at 3:00 PM. However, we can say that generally a lot of comments are made in the afternoon from 1:00 PM to 5:00 PM."
  },
  {
    "objectID": "posts/2021-05-11-optimizing-hacker-news-posts.html#median-number-of-comments-per-post-by-hour-posted",
    "href": "posts/2021-05-11-optimizing-hacker-news-posts.html#median-number-of-comments-per-post-by-hour-posted",
    "title": "Optimizing Hacker News Posts",
    "section": "Median Number of Comments per Post, by Hour Posted",
    "text": "Median Number of Comments per Post, by Hour Posted\nThe table below shows the median number of comments per post, by the hour of posting.\n\nhours_median\n\n\n\n\n\n\n\n\nhour\nnum_comments\n\n\n\n\n0\n0\n3.0\n\n\n1\n1\n3.0\n\n\n2\n2\n4.0\n\n\n3\n3\n3.0\n\n\n4\n4\n4.0\n\n\n5\n5\n3.0\n\n\n6\n6\n3.0\n\n\n7\n7\n4.0\n\n\n8\n8\n3.5\n\n\n9\n9\n3.0\n\n\n10\n10\n4.0\n\n\n11\n11\n4.0\n\n\n12\n12\n4.0\n\n\n13\n13\n4.0\n\n\n14\n14\n3.0\n\n\n15\n15\n4.0\n\n\n16\n16\n3.0\n\n\n17\n17\n4.0\n\n\n18\n18\n3.0\n\n\n19\n19\n3.0\n\n\n20\n20\n4.0\n\n\n21\n21\n3.0\n\n\n22\n22\n4.0\n\n\n23\n23\n4.0\n\n\n\n\n\n\n\nThis is visualized in the line chart below, which looks quite different from the previous two charts.\n\nchart = alt.Chart(hours_median).mark_line().encode(\n    x = alt.X(\n        \"hour:O\", title = \"Hour of the Day\",\n        axis = alt.Axis(labelAngle = 0)\n    ),\n    y = alt.Y(\"num_comments:Q\", title = \"Median Number of Comments per Post\"),\n).properties(\n    title = \"Median Number of Comments per Post, by Hour Posted\",\n    width = 700,\n    height = 400,\n).configure_axis(\n    grid = True,\n)\n\nchart\n\n\n\n\n\n\nThis graph shows that the median number of comments per post is very consistent throughout the day. It ranges from 3 comments to 4 comments.\nThis brings us a new question. We’ve seen that Hacker News users most actively post and comment in the afternoon. So, why does the median number of comments per post not increase in the afternoon?\nA possible explanation is that since the site is oversaturated with new posts in the afternoon, only the very best posts receive attention. The rest are lost in the flood of new posts."
  },
  {
    "objectID": "posts/2022-01-21-neural-network-classify-handwritten-digits.html#data-source",
    "href": "posts/2022-01-21-neural-network-classify-handwritten-digits.html#data-source",
    "title": "Using a Neural Network to Classify Handwritten Digits",
    "section": "Data Source",
    "text": "Data Source\nAlpaydin, E., & Kaynak, C. (1998). UCI Machine Learning Repository: Optical Recognition of Handwritten Digits Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\nThe data was obtained via the following scikit-learn feature:\nscikit-learn developers. (2021). Sklearn.datasets.load_digits. Scikit-Learn. https://scikit-learn/stable/modules/generated/sklearn.datasets.load_digits.html"
  },
  {
    "objectID": "posts/2022-01-21-neural-network-classify-handwritten-digits.html#information-sources",
    "href": "posts/2022-01-21-neural-network-classify-handwritten-digits.html#information-sources",
    "title": "Using a Neural Network to Classify Handwritten Digits",
    "section": "Information Sources",
    "text": "Information Sources\nDataquest. (n.d.). Building A Handwritten Digits Classifier—Introduction. Dataquest. Retrieved January 21, 2022, from https://app.dataquest.io/c/50/m/244/guided-project%3A-building-a-handwritten-digits-classifier/1\nscikit-learn developers. (2021a). 3.3. Metrics and scoring: Quantifying the quality of predictions. Scikit-Learn. https://scikit-learn/stable/modules/model_evaluation.html\nscikit-learn developers. (2021b). Sklearn.model_selection.GridSearchCV. Scikit-Learn. https://scikit-learn/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
  },
  {
    "objectID": "posts/2022-01-21-neural-network-classify-handwritten-digits.html#image-source",
    "href": "posts/2022-01-21-neural-network-classify-handwritten-digits.html#image-source",
    "title": "Using a Neural Network to Classify Handwritten Digits",
    "section": "Image Source",
    "text": "Image Source\nAdair, C. (2016, February 26). Photo by Clint Adair on Unsplash. Unsplash. https://unsplash.com/photos/BW0vK-FA3eg"
  },
  {
    "objectID": "posts/2022-04-16-Online-Course-Dataquest-Data-Scientist-Python.html",
    "href": "posts/2022-04-16-Online-Course-Dataquest-Data-Scientist-Python.html",
    "title": "Completed an Online Course: Dataquest Data Scientist in Python",
    "section": "",
    "text": "My certificate for the course. View and verify the certificate here.\n\n\nToday, I completed the Data Scientist in Python (DSP) career path on the Dataquest platform. DSP is a collection of 35 courses revolving around the data science workflow in the Python programming language. It takes an estimated 325 hours to complete the path. For more information, check out the path’s webpage.\nIt has been a year since I started studying the DSP path in May of 2021. At the time, I had finished some courses on another platform, Sololearn, but I had not felt ready to apply the skills I had studied there. Now, after the DSP path, I am much more comfortable with working in Python. I would say that the Dataquest platform is very effective because of its unique pedagogy, which involves teaching using text and diagrams instead of videos, as well as providing many hands-on coding exercises.\nHere is an outline of the topics that I learned through the DSP path:\n\nPython basics\n\nvariables, data types, loops, conditional statements, data structures, functions\nJupyter notebook\n\nData analysis and visualization\n\npandas and NumPy packages for data manipulation\nMatplotlib and Seaborn for data visualization\ninformation design principles\n\nData cleaning\n\npandas for aggregation, joining, vectorized transformations, regular expression matching, handling missing data\n\nCommand line\n\nBash basics\ntext and CSV processing\nvirtual environments\nJupyter console\nGit and GitHub for version control\n\nData sources\n\nSQL basics using sqlite\nAPIs and web scraping\n\nStatistics and probability\n\nsampling, distributions, averages, variability\nconditional probability, Bayes’ theorem, Naive Bayes algorithm\nhypothesis testing basics\n\nMachine learning\n\nlinear regression\nlogistic regression\nK nearest neighbors\nK-means clustering\ndecision tree\nrandom forest\n\nDeep learning\n\nartifical neural network (ANN)\nKaggle competition fundamentals\n\nProcessing large datasets\n\nApache Spark basics\nSpark DataFrames\nSpark SQL\n\n\nWith these skills, I am ready to create more inquisitive and insightful data science projects.\nOther than the DSP path, I also plan to study other things on Dataquest, such as the Data Engineer in Python path and the Microsoft Power BI path.\n\nReferences\nDataquest Labs, Inc. (2022). Data Scientist in Python Career Path. Dataquest. https://www.dataquest.io/path/data-scientist/"
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#apple-app-store-dataset",
    "href": "posts/2021-05-08-profitable-app-profiles.html#apple-app-store-dataset",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Apple App Store dataset",
    "text": "Apple App Store dataset\n\nprint(data_apple.shape)\n\n(7197, 16)\n\n\nThe dataset has 7197 rows (1 row per app), and 16 columns which describe these apps.\nAccording to the Kaggle documentation (Mobile App Store ( 7200 apps)), the following are the columns and their meanings.\n\n“id” : App ID\n“track_name”: App Name\n“size_bytes”: Size (in Bytes)\n“currency”: Currency Type\n“price”: Price amount\n“ratingcounttot”: User Rating counts (for all version)\n“ratingcountver”: User Rating counts (for current version)\n“user_rating” : Average User Rating value (for all version)\n“userratingver”: Average User Rating value (for current version)\n“ver” : Latest version code\n“cont_rating”: Content Rating\n“prime_genre”: Primary Genre\n“sup_devices.num”: Number of supporting devices\n“ipadSc_urls.num”: Number of screenshots showed for display\n“lang.num”: Number of supported languages\n“vpp_lic”: Vpp Device Based Licensing Enabled\n\nA sample of the first 5 rows of the dataset is shown below.\n\ndata_apple.head()\n\n\n\n\n\n\n\n\nid\ntrack_name\nsize_bytes\ncurrency\nprice\nrating_count_tot\nrating_count_ver\nuser_rating\nuser_rating_ver\nver\ncont_rating\nprime_genre\nsup_devices.num\nipadSc_urls.num\nlang.num\nvpp_lic\n\n\n\n\n0\n284882215\nFacebook\n389879808\nUSD\n0.0\n2974676\n212\n3.5\n3.5\n95.0\n4+\nSocial Networking\n37\n1\n29\n1\n\n\n1\n389801252\nInstagram\n113954816\nUSD\n0.0\n2161558\n1289\n4.5\n4.0\n10.23\n12+\nPhoto & Video\n37\n0\n29\n1\n\n\n2\n529479190\nClash of Clans\n116476928\nUSD\n0.0\n2130805\n579\n4.5\n4.5\n9.24.12\n9+\nGames\n38\n5\n18\n1\n\n\n3\n420009108\nTemple Run\n65921024\nUSD\n0.0\n1724546\n3842\n4.5\n4.0\n1.6.2\n9+\nGames\n40\n5\n1\n1\n\n\n4\n284035177\nPandora - Music & Radio\n130242560\nUSD\n0.0\n1126879\n3594\n4.0\n4.5\n8.4.1\n12+\nMusic\n37\n4\n1\n1"
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-dataset",
    "href": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-dataset",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Google Play Store dataset",
    "text": "Google Play Store dataset\n\nprint(data_google.shape)\n\n(10841, 13)\n\n\nThe dataset has 10841 rows and 13 columns.\nThe column names are self-explanatory, so the Kaggle documentation (Google Play Store Apps) does not describe them.\n\nprint(list(data_google.columns))\n\n['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver']\n\n\nBelow is a sample of the dataset.\n\ndata_google.head()\n\n\n\n\n\n\n\n\nApp\nCategory\nRating\nReviews\nSize\nInstalls\nType\nPrice\nContent Rating\nGenres\nLast Updated\nCurrent Ver\nAndroid Ver\n\n\n\n\n0\nPhoto Editor & Candy Camera & Grid & ScrapBook\nART_AND_DESIGN\n4.1\n159\n19M\n10,000+\nFree\n0\nEveryone\nArt & Design\nJanuary 7, 2018\n1.0.0\n4.0.3 and up\n\n\n1\nColoring book moana\nART_AND_DESIGN\n3.9\n967\n14M\n500,000+\nFree\n0\nEveryone\nArt & Design;Pretend Play\nJanuary 15, 2018\n2.0.0\n4.0.3 and up\n\n\n2\nU Launcher Lite – FREE Live Cool Themes, Hide ...\nART_AND_DESIGN\n4.7\n87510\n8.7M\n5,000,000+\nFree\n0\nEveryone\nArt & Design\nAugust 1, 2018\n1.2.4\n4.0.3 and up\n\n\n3\nSketch - Draw & Paint\nART_AND_DESIGN\n4.5\n215644\n25M\n50,000,000+\nFree\n0\nTeen\nArt & Design\nJune 8, 2018\nVaries with device\n4.2 and up\n\n\n4\nPixel Draw - Number Art Coloring Book\nART_AND_DESIGN\n4.3\n967\n2.8M\n100,000+\nFree\n0\nEveryone\nArt & Design;Creativity\nJune 20, 2018\n1.1\n4.4 and up"
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#inaccurate-data",
    "href": "posts/2021-05-08-profitable-app-profiles.html#inaccurate-data",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Inaccurate Data",
    "text": "Inaccurate Data\nThis Kaggle discussion about the Google Play dataset indicates that row 10472 (excluding the header) has an error.\nBelow, I have printed row 0 and row 10472 so that these can be compared.\n\ndata_google.iloc[[0, 10472]]\n\n\n\n\n\n\n\n\nApp\nCategory\nRating\nReviews\nSize\nInstalls\nType\nPrice\nContent Rating\nGenres\nLast Updated\nCurrent Ver\nAndroid Ver\n\n\n\n\n0\nPhoto Editor & Candy Camera & Grid & ScrapBook\nART_AND_DESIGN\n4.1\n159\n19M\n10,000+\nFree\n0\nEveryone\nArt & Design\nJanuary 7, 2018\n1.0.0\n4.0.3 and up\n\n\n10472\nLife Made WI-Fi Touchscreen Photo Frame\n1.9\n19.0\n3.0M\n1,000+\nFree\n0\nEveryone\nNaN\nFebruary 11, 2018\n1.0.19\n4.0 and up\nNaN\n\n\n\n\n\n\n\nAs we look at row 10472 in the context of the column headers and row 0, the following things become clear.\n\nThe “Category” value is not present. Thus, all values to the right of it have been shifted leftward.\nThe “Android Ver” column was left with a missing value.\n\nThus, this row will be removed.\n\nif data_google.iloc[10472, 0] == 'Life Made WI-Fi Touchscreen Photo Frame':\n    \n    # This if-statement prevents more rows from being deleted\n    # if the cell is run again.\n    \n    data_google.drop(10472, inplace = True)\n    print(\"The inaccurate row was deleted.\")\n\nThe inaccurate row was deleted."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#duplicate-data",
    "href": "posts/2021-05-08-profitable-app-profiles.html#duplicate-data",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Duplicate Data",
    "text": "Duplicate Data\nThere are also duplicate app entries in the Google Play dataset. We can consider a row as a duplicates if another row exists that has the same “App” value.\nHere, I count the total number of duplicate rows. This turns out to be 1979 rows.\n\ndef count_duplicates(df, col_name):\n    \n    \"\"\"Count the number of duplicate rows in a DataFrame.\n    `col_name` is the name of the column to be used as a basis\n    for duplicate values.\"\"\"\n    \n    all_apps = {}\n    \n    for index, row in df.iterrows():\n        name = row[col_name]\n        all_apps.setdefault(name, []).append(index)\n            \n    duplicate_inds = [ind\n                      for lst in all_apps.values()\n                      for ind in lst\n                      if len(lst) &gt; 1]\n    \n    n_duplicates = \"Duplicates: {}\".format(len(duplicate_inds))\n    duplicate_rows = df.iloc[duplicate_inds]\n    \n    return n_duplicates, duplicate_rows\n    \ngoogle_dupes = count_duplicates(data_google, \"App\")\nprint(google_dupes[0])\n\nDuplicates: 1979\n\n\nAs an example, there are 4 rows for Instagram:\n\nig_filter = data_google[\"App\"] == \"Instagram\"\nig_rows = data_google.loc[ig_filter]\n\n\nig_rows\n\n\n\n\n\n\n\n\nApp\nCategory\nRating\nReviews\nSize\nInstalls\nType\nPrice\nContent Rating\nGenres\nLast Updated\nCurrent Ver\nAndroid Ver\n\n\n\n\n2545\nInstagram\nSOCIAL\n4.5\n66577313\nVaries with device\n1,000,000,000+\nFree\n0\nTeen\nSocial\nJuly 31, 2018\nVaries with device\nVaries with device\n\n\n2604\nInstagram\nSOCIAL\n4.5\n66577446\nVaries with device\n1,000,000,000+\nFree\n0\nTeen\nSocial\nJuly 31, 2018\nVaries with device\nVaries with device\n\n\n2611\nInstagram\nSOCIAL\n4.5\n66577313\nVaries with device\n1,000,000,000+\nFree\n0\nTeen\nSocial\nJuly 31, 2018\nVaries with device\nVaries with device\n\n\n3909\nInstagram\nSOCIAL\n4.5\n66509917\nVaries with device\n1,000,000,000+\nFree\n0\nTeen\nSocial\nJuly 31, 2018\nVaries with device\nVaries with device\n\n\n\n\n\n\n\nLooking closely, we can see that duplicate rows are not exactly identical. The “Reviews” column, which shows the total number of reviews of the app, has different values.\nIt can be inferred that the row with the largest value is the newest entry for the app. Therefore, all duplicate rows will be dropped except for the ones with the largest “Reviews” values.\n\ndef remove_duplicates(df, name_col, reviews_col):\n\n    # Each key-value pair will follow the format:\n    # {\"App Name\": maximum number of reviews among all duplicates}\n    reviews_max = {}\n\n    for index, row in df.iterrows():\n        name = row[name_col]\n        n_reviews = int(row[reviews_col])\n\n        if n_reviews &gt; reviews_max.get(name, -1):\n            reviews_max[name] = n_reviews\n\n    # List of duplicate indices to drop,\n    # excluding the row with the highest number of reviews\n    # among that app's duplicate rows.\n    indices_to_drop = []\n\n    # Rows with names that have already been added into this list\n    # will be dropped.\n    already_added = []\n\n    for index, row in df.iterrows():\n        name = row[name_col]\n        n_reviews = int(row[reviews_col])\n\n        if (name not in already_added) and (n_reviews == reviews_max[name]):\n            already_added.append(name)\n        else:\n            indices_to_drop.append(index)\n\n    # Remove duplicates and return the clean dataset.\n    clean = df.drop(indices_to_drop)\n    return clean\n\nandroid_clean = remove_duplicates(data_google, \"App\", \"Reviews\")\nprint(android_clean.shape)\n\n(9659, 13)\n\n\nAfter duplicates were removed, the Google Play dataset was left with 9659 rows.\nAs for the Apple App Store dataset, there are 4 duplicate rows.\n\napple_dupes = count_duplicates(data_apple, \"track_name\")\n\nprint(apple_dupes[0])\n\napple_dupes[1]\n\nDuplicates: 4\n\n\n\n\n\n\n\n\n\nid\ntrack_name\nsize_bytes\ncurrency\nprice\nrating_count_tot\nrating_count_ver\nuser_rating\nuser_rating_ver\nver\ncont_rating\nprime_genre\nsup_devices.num\nipadSc_urls.num\nlang.num\nvpp_lic\n\n\n\n\n2948\n1173990889\nMannequin Challenge\n109705216\nUSD\n0.0\n668\n87\n3.0\n3.0\n1.4\n9+\nGames\n37\n4\n1\n1\n\n\n4463\n1178454060\nMannequin Challenge\n59572224\nUSD\n0.0\n105\n58\n4.0\n4.5\n1.0.1\n4+\nGames\n38\n5\n1\n1\n\n\n4442\n952877179\nVR Roller Coaster\n169523200\nUSD\n0.0\n107\n102\n3.5\n3.5\n2.0.0\n4+\nGames\n37\n5\n1\n1\n\n\n4831\n1089824278\nVR Roller Coaster\n240964608\nUSD\n0.0\n67\n44\n3.5\n4.0\n0.81\n4+\nGames\n38\n0\n1\n1\n\n\n\n\n\n\n\nThe “rating_count_tot” column in the Apple App Store dataset is like the “Reviews” column in the Google Play dataset. It tells the total number of reviews so far. Therefore, Apple App Store dataset duplicates can be removed by keeping the rows with the highest rating count totals.\n\nios_clean = remove_duplicates(data_apple, \"track_name\", \"rating_count_tot\")\nprint(ios_clean.shape)\n\n(7195, 16)\n\n\nFrom 7197 rows, there are now 7195 rows in the Apple App Store dataset."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#non-english-apps",
    "href": "posts/2021-05-08-profitable-app-profiles.html#non-english-apps",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Non-English Apps",
    "text": "Non-English Apps\nThe hypothetical app company who will use this analysis is a company that only makes apps in English. Thus, all apps with non-English titles shall be removed from the datasets.\nThe task now is to identify titles which are not in English. It is known that in the ASCII table, the characters most commonly used in English are within codes 0 to 127. Some English app titles may have special characters or emojis, though, so I will only remove titles which have more than 3 characters outside of the normal range.\n\ndef is_english(text):\n    unicode = [ord(char) for char in text]\n    normal = [(code &gt;= 0 and code &lt;= 127) for code in unicode]\n    non_english = len(text) - sum(normal)\n\n    return non_english &lt;= 3\n\ndef keep_english(df, name_col):\n    \n    \"\"\"Return a new DataFrame containing only rows with English names.\"\"\"\n    \n    remove_indices = []\n    \n    for index, row in df.iterrows():\n        name = row[name_col]\n        if not is_english(name):\n            remove_indices.append(index)\n            \n    return df.drop(remove_indices)\n\nandroid_clean = keep_english(android_clean, \"App\")\nios_clean = keep_english(ios_clean, \"track_name\")\n\nprint(\"Google Play Store Dataset:\", android_clean.shape)\nprint(\"Apple App Store Dataset:\", ios_clean.shape)\n\nGoogle Play Store Dataset: (9614, 13)\nApple App Store Dataset: (6181, 16)\n\n\nNow, there are only English apps in both datasets."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#paid-apps",
    "href": "posts/2021-05-08-profitable-app-profiles.html#paid-apps",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Paid Apps",
    "text": "Paid Apps\nAs mentioned earlier, the app company only makes free apps. Therefore, data on paid apps is irrelevant to this analysis. Paid apps shall be identified and removed from both datasets.\n\ndef remove_paid(df, price_col):\n    \n    \"\"\"Return a new DataFrame without paid apps.\"\"\"\n    \n    remove_indices = []\n    \n    for index, row in df.iterrows():\n        price = str(row[price_col])\n        \n        # Keep characters that are numeric or periods.\n        price = float(re.sub(\"[^0-9.]\", \"\", price))\n        \n        if price != 0.0:\n            remove_indices.append(index)\n            \n    return df.drop(remove_indices)\n        \nandroid_clean = remove_paid(android_clean, \"Price\")\nios_clean = remove_paid(ios_clean, \"price\")\n\nprint(\"Google Play Store Dataset:\", android_clean.shape)\nprint(\"Apple App Store Dataset:\", ios_clean.shape)\n\nGoogle Play Store Dataset: (8864, 13)\nApple App Store Dataset: (3220, 16)\n\n\nThe datasets were left with 8864 apps in Google Play and 3220 apps in the App Store."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#missing-data",
    "href": "posts/2021-05-08-profitable-app-profiles.html#missing-data",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Missing Data",
    "text": "Missing Data\nLastly, let us remove rows with missing data. Note that it would be wasteful to remove rows with missing data in columns that we will not inspect. Therefore, we will only remove rows with missing data in relevant columns. (Why these are relevant will be explained later.) These would be the following.\nGoogle Play Store dataset\n\nApp\nCategory\nInstalls\nGenres\n\nApple App Store dataset\n\ntrack_name\nprime_genre\nrating_count_tot\n\nI will now remove all rows with missing values in these columns.\n\nandroid_clean.dropna(\n    subset = [\"App\", \"Category\", \"Installs\", \"Genres\"],\n    inplace = True,\n)\n\nios_clean.dropna(\n    subset = [\"track_name\", \"prime_genre\", \"rating_count_tot\"],\n    inplace = True,\n)\n\nprint(\"Google Play Store Dataset:\", android_clean.shape)\nprint(\"Apple App Store Dataset:\", ios_clean.shape)\n\nGoogle Play Store Dataset: (8864, 13)\nApple App Store Dataset: (3220, 16)\n\n\nThese are the same shapes as before. Therefore, there were no missing values in the relevant columns. No datapoints were removed at this step.\nData cleaning is done, so now we can move on to the analysis."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#apple-app-store-prime-genres",
    "href": "posts/2021-05-08-profitable-app-profiles.html#apple-app-store-prime-genres",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Apple App Store: Prime Genres",
    "text": "Apple App Store: Prime Genres\nFirst, the frequency table of Apple App Store prime genres shall be analyzed. Below, I have ordered the table by frequency, descending. I have also made bar graphs showing the top 10 positions in each frequency table.\n\nsr_to_df(apple_genres, \"percentage\", n_head = 5)\n\n\n\n\n\n\n\n\nname\npercentage\n\n\n\n\n0\nGames\n58.136646\n\n\n1\nEntertainment\n7.888199\n\n\n2\nPhoto & Video\n4.968944\n\n\n3\nEducation\n3.664596\n\n\n4\nSocial Networking\n3.291925\n\n\n\n\n\n\n\n\ndef bar_n(series, chart_title, ylabel, n = 10, perc = False):\n    \n    \"\"\"Takes a series and outputs a bar graph of the first n items.\"\"\"\n    \n    series.index.name = \"name\"\n    df = series.rename(\"number\").reset_index()\n    df[\"number\"] = [round(i, 2) for i in df[\"number\"]]\n    df = df[:n]\n    \n    bar = alt.Chart(df).mark_bar().encode(\n        x = alt.X(\"name\", title = \"Name\", sort = \"-y\"),\n        y = alt.Y(\"number\", title = ylabel),\n    )\n    \n    text = bar.mark_text(\n        align = 'center',\n        baseline = 'middle',\n        dy = -5, # Nudge text upward\n    ).encode(\n        text = 'number:Q'\n    )\n    \n    chart = (bar + text).properties(\n        title = chart_title,\n        width = 700,\n        height = 400,\n    )\n    \n    return chart\n\nbar_n(\n    apple_genres,\n    \"Top 10 Most Common Prime Genres of iOS Apps\",\n    \"Percentage of Apps\",\n    perc = True,\n)\n\n\n\n\n\n\nThe top 5 most common prime genres in the Apple App Store are Games, Entertainment, Photo & Video, Education, and Social Networking. Games are at the top, occupying over 58% of all apps. This is a much higher percentage than any other single genre occupies.\n\n\n\n\n\n\nImportant\n\n\n\nThe general impression is that there are many more iOS apps that are entertainment-related apps compared to practical apps."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-categories",
    "href": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-categories",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Google Play Store: Categories",
    "text": "Google Play Store: Categories\nNext, below is the frequency table for Google Play Store app categories.\n\nsr_to_df(google_categories, \"percentage\", 5)\n\n\n\n\n\n\n\n\nname\npercentage\n\n\n\n\n0\nFAMILY\n18.907942\n\n\n1\nGAME\n9.724729\n\n\n2\nTOOLS\n8.461191\n\n\n3\nBUSINESS\n4.591606\n\n\n4\nLIFESTYLE\n3.903430\n\n\n\n\n\n\n\n\nbar_n(\n    google_categories,\n    \"Top 10 Most Common Categories of Android Apps\",\n    \"Percentage of Apps\",\n    perc = True,\n)\n\n\n\n\n\n\nThe picture here seems to be different. The most common category is Family occupying almost 19% of all apps, followed by Game, Tools, Business, and Lifestyle.\n\n\n\n\n\n\nImportant\n\n\n\nThe table suggests that practical app categories are more common in Google Play than in the Apple App Store."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-genres",
    "href": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-genres",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Google Play Store: Genres",
    "text": "Google Play Store: Genres\nLastly, below is the frequency table for Google Play Store app genres.\n\nsr_to_df(google_genres, \"percentage\", 5)\n\n\n\n\n\n\n\n\nname\npercentage\n\n\n\n\n0\nTools\n8.449910\n\n\n1\nEntertainment\n6.069495\n\n\n2\nEducation\n5.347473\n\n\n3\nBusiness\n4.591606\n\n\n4\nProductivity\n3.892148\n\n\n\n\n\n\n\n\nbar_n(\n    google_genres,\n    \"Top 10 Most Common Genres of Android Apps\",\n    \"Percentage of Apps\",\n)\n\n\n\n\n\n\nThere are 114 genres in this table, so it is not fully displayed. However, it would appear that the top 5 genres are Tools (8%), Entertainment, Education, Business, and Lifestyle. Like with the categories, practical apps are very common.\nHowever, I noticed something special about this frequency table. Some genres are actually combinations of multiple genres, separated by semi-colons. If I can extract and count individual genres from these combined genres, then I can get a more accurate idea of app genres in the Google Play Store.\n\n\n\n\n\n\nNote\n\n\n\nThis frequency table will show numbers instead of percentages. Since the genres overlap, the percentages would add up to greater than 100%.\n\n\n\nfreq = {}\nfor value in android_clean[\"Genres\"]:\n    genres = value.split(\";\")\n    for genre in genres:\n        freq.setdefault(genre, 0)\n        freq[genre] += 1\n\ngoogle_genres_split = pd.Series(freq).sort_values(ascending = False)\n\nsr_to_df(google_genres_split, n_head = 5)\n\n\n\n\n\n\n\n\nname\nnumber\n\n\n\n\n0\nTools\n750\n\n\n1\nEducation\n606\n\n\n2\nEntertainment\n569\n\n\n3\nBusiness\n407\n\n\n4\nLifestyle\n347\n\n\n\n\n\n\n\n\nbar_n(\n    google_genres_split,\n    \"Top 10 Most Common Genres of Android Apps (Split Up)\",\n    \"Number of Apps\",\n)\n\n\n\n\n\n\nIt can be seen that the frequency table has slightly different placements now. However, the top genres are still Tools, Education, Entertainment, Business, and Lifestyle. Practical app genres are very common in the Google Play Store. They are more common here than in the Apple App Store.\n\n\n\n\n\n\nImportant\n\n\n\nBased on the results, the Google Play Store has a selection of apps that is more balanced between entertainment and practicality.\n\n\n\nGoing back to the the frequency table of Categories, since it seems that each Category represents a group of Genres. For example, one would expect apps in the Simulation, Arcade, Puzzle, Strategy, etc. genres to be under the Game category. It was shown earlier that this category is the 2nd most common in the Google Play Store.\nThe Categories column is more general and gives a more accurate picture of the common types of apps. Thus, from here on, I will be analyzing only the “Category” column and not the “Genres” column.\n\n\n\n\n\n\nNote\n\n\n\nI will now use “app type” to generally refer to the Apple App Store’s “prime_genre” values or the Google Play Store’s “Category” values."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#apple-app-store-rating-counts",
    "href": "posts/2021-05-08-profitable-app-profiles.html#apple-app-store-rating-counts",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Apple App Store: Rating Counts",
    "text": "Apple App Store: Rating Counts\nIn the Apple App Store dataset, there is no column that indicates the number of users.\n\nprint(list(ios_clean.columns))\n\n['id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic']\n\n\nHowever, the \"rating_count_tot\" column exists. It indicates the total number of ratings given to each app. We can use it as a proxy for the number of users of each app.\nThe function below will return a Series showing the average number of users per app within each type. (Not the total number of users per type.)\n\ndef users_by_type(df, type_col, users_col, moct = \"mean\"):\n    \n    \"\"\"Return a Series that maps each app type to\n    the average number of users per app for that type.\n    Specify 'mean' or 'median' for the measure of central tendency.\"\"\"\n    \n    dct = {}\n    \n    for index, row in df.iterrows():\n        app_type = row[type_col]\n        users = row[users_col]\n        \n        dct.setdefault(app_type, []).append(users)\n    \n    dct2 = {}\n    \n    for app_type in dct:\n        counts = dct[app_type]\n        if moct == \"mean\":\n            dct2[app_type] = np.mean(counts)\n        elif moct == \"median\":\n            dct2[app_type] = np.median(counts)\n        \n    result = pd.Series(dct2).sort_values(ascending = False)\n    return result\n\nios_users = users_by_type(ios_clean, \"prime_genre\", \"rating_count_tot\")\n\nsr_to_df(ios_users, n_head = 5)\n\n\n\n\n\n\n\n\nname\nnumber\n\n\n\n\n0\nNavigation\n86090.333333\n\n\n1\nReference\n74942.111111\n\n\n2\nSocial Networking\n71548.349057\n\n\n3\nMusic\n57326.530303\n\n\n4\nWeather\n52279.892857\n\n\n\n\n\n\n\n\nbar_n(\n    ios_users,\n    \"Top 10 Most Popular iOS App Types\",\n    \"Mean Number of Users per App\",\n)\n\n\n\n\n\n\nThe top 5 iOS app types with the highest mean average number of users per app are Navigation, Reference, Social Networking, Music, and Weather.\nHowever, these mean averages may be skewed by a few particularly popular apps. For example, let us look at the number of users of the top 5 Navigation apps.\n\nios_nav = ios_clean[[\n    \"track_name\",\n    \"rating_count_tot\",\n]].loc[\n    ios_clean[\"prime_genre\"] == \"Navigation\"\n].sort_values(\n    by = \"rating_count_tot\",\n    ascending = False,\n).set_index(\n    \"track_name\",\n)\n\n# `ios_nav` is still a DataFrame at this point.\n# It becomes a Series below.\nios_nav = ios_nav[\"rating_count_tot\"]\n\nsr_to_df(ios_nav, n_head = 5)\n\n\n\n\n\n\n\n\ntrack_name\nnumber\n\n\n\n\n0\nWaze - GPS Navigation, Maps & Real-time Traffic\n345046\n\n\n1\nGoogle Maps - Navigation & Transit\n154911\n\n\n2\nGeocaching®\n12811\n\n\n3\nCoPilot GPS – Car Navigation & Offline Maps\n3582\n\n\n4\nImmobilienScout24: Real Estate Search in Germany\n187\n\n\n\n\n\n\n\n\nbar_n(\n    ios_nav,\n    \"iOS Navigation Apps by Popularity\",\n    \"Number of Users\",\n)\n\n\n\n\n\n\nClearly, the distribution is skewed because Waze has such a high number of users. Therefore, a better measure of central tendency to use would be the median, not the mean.\nLet us repeat the analysis using the median this time:\n\nios_users = users_by_type(\n    ios_clean,\n    \"prime_genre\",\n    \"rating_count_tot\",\n    moct = \"median\",\n)\n\nsr_to_df(ios_users, n_head = 5)\n\n\n\n\n\n\n\n\nname\nnumber\n\n\n\n\n0\nProductivity\n8737.5\n\n\n1\nNavigation\n8196.5\n\n\n2\nReference\n6614.0\n\n\n3\nShopping\n5936.0\n\n\n4\nSocial Networking\n4199.0\n\n\n\n\n\n\n\n\nbar_n(\n    ios_users,\n    \"Top 10 Most Popular iOS App Types\",\n    \"Median Number of Users per App\",\n)\n\n\n\n\n\n\nThe top 5 most popular iOS apps by median number of users per app are: - Productivity - Navigation - Reference - Shopping - Social Networking\nThese placements are quite different from the top 5 most common iOS apps (Games, Entertainment, Photo & Video, Education, and Social Networking).\n\n\n\n\n\n\n\nImportant\n\n\n\nWe can say the following about the Apple App Store.\n\n\n\nApps for entertainment and fun, notably Games, are the most common apps.\nApps for practical purposes, notably Productivity, are the most popular apps."
  },
  {
    "objectID": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-installs",
    "href": "posts/2021-05-08-profitable-app-profiles.html#google-play-store-installs",
    "title": "Profitable App Profiles for iOS and Android",
    "section": "Google Play Store: Installs",
    "text": "Google Play Store: Installs\nLet us see which columns in the Google Play Store dataset can tell us about the number of users per app.\n\nandroid_clean.head()\n\n\n\n\n\n\n\n\nApp\nCategory\nRating\nReviews\nSize\nInstalls\nType\nPrice\nContent Rating\nGenres\nLast Updated\nCurrent Ver\nAndroid Ver\n\n\n\n\n0\nPhoto Editor & Candy Camera & Grid & ScrapBook\nART_AND_DESIGN\n4.1\n159\n19M\n10,000+\nFree\n0\nEveryone\nArt & Design\nJanuary 7, 2018\n1.0.0\n4.0.3 and up\n\n\n2\nU Launcher Lite – FREE Live Cool Themes, Hide ...\nART_AND_DESIGN\n4.7\n87510\n8.7M\n5,000,000+\nFree\n0\nEveryone\nArt & Design\nAugust 1, 2018\n1.2.4\n4.0.3 and up\n\n\n3\nSketch - Draw & Paint\nART_AND_DESIGN\n4.5\n215644\n25M\n50,000,000+\nFree\n0\nTeen\nArt & Design\nJune 8, 2018\nVaries with device\n4.2 and up\n\n\n4\nPixel Draw - Number Art Coloring Book\nART_AND_DESIGN\n4.3\n967\n2.8M\n100,000+\nFree\n0\nEveryone\nArt & Design;Creativity\nJune 20, 2018\n1.1\n4.4 and up\n\n\n5\nPaper flowers instructions\nART_AND_DESIGN\n4.4\n167\n5.6M\n50,000+\nFree\n0\nEveryone\nArt & Design\nMarch 26, 2017\n1.0\n2.3 and up\n\n\n\n\n\n\n\nThe “Installs” column seems like the best indicator of the number of users.\n\nandroid_clean[[\"App\", \"Installs\"]]\n\n\n\n\n\n\n\n\nApp\nInstalls\n\n\n\n\n0\nPhoto Editor & Candy Camera & Grid & ScrapBook\n10,000+\n\n\n2\nU Launcher Lite – FREE Live Cool Themes, Hide ...\n5,000,000+\n\n\n3\nSketch - Draw & Paint\n50,000,000+\n\n\n4\nPixel Draw - Number Art Coloring Book\n100,000+\n\n\n5\nPaper flowers instructions\n50,000+\n\n\n...\n...\n...\n\n\n10836\nSya9a Maroc - FR\n5,000+\n\n\n10837\nFr. Mike Schmitz Audio Teachings\n100+\n\n\n10838\nParkinson Exercices FR\n1,000+\n\n\n10839\nThe SCP Foundation DB fr nn5n\n1,000+\n\n\n10840\niHoroscope - 2018 Daily Horoscope & Astrology\n10,000,000+\n\n\n\n\n8864 rows × 2 columns\n\n\n\nThe column contains strings which indicate the general range of how many users installed the apps. Since we cannot find the exact number of installs, we will simply remove the “+” signs and convert the numbers into integers.\n\nandroid_clean[\"Installs\"] = [int(re.sub(\"[,+]\", \"\", text))\n                            for text in android_clean[\"Installs\"]]\n\nandroid_clean[[\"Installs\"]]\n\n\n\n\n\n\n\n\nInstalls\n\n\n\n\n0\n10000\n\n\n2\n5000000\n\n\n3\n50000000\n\n\n4\n100000\n\n\n5\n50000\n\n\n...\n...\n\n\n10836\n5000\n\n\n10837\n100\n\n\n10838\n1000\n\n\n10839\n1000\n\n\n10840\n10000000\n\n\n\n\n8864 rows × 1 columns\n\n\n\nLet us now see which app categories are most popular. We will use the median average here, as we did for iOS apps.\n\nandroid_users = users_by_type(\n    android_clean,\n    \"Category\",\n    \"Installs\",\n    moct = \"median\",\n)\n\nsr_to_df(android_users, n_head = 10)\n\n\n\n\n\n\n\n\nname\nnumber\n\n\n\n\n0\nENTERTAINMENT\n1000000.0\n\n\n1\nEDUCATION\n1000000.0\n\n\n2\nGAME\n1000000.0\n\n\n3\nPHOTOGRAPHY\n1000000.0\n\n\n4\nSHOPPING\n1000000.0\n\n\n5\nWEATHER\n1000000.0\n\n\n6\nVIDEO_PLAYERS\n1000000.0\n\n\n7\nCOMMUNICATION\n500000.0\n\n\n8\nFOOD_AND_DRINK\n500000.0\n\n\n9\nHEALTH_AND_FITNESS\n500000.0\n\n\n\n\n\n\n\n\nbar_n(\n    android_users,\n    \"Top 10 Most Popular Android App Types\",\n    \"Median Number of Users per App\",\n    n = 10,\n)\n\n\n\n\n\n\nSince the top 5 spots all had the same median number of users per app (1000000), the graph was expanded to include the top 10 spots.\nIt appears that the types of Android apps with the highest median number of users per app are:\n\nGAME\nVIDEO_PLAYERS\nWEATHER\nEDUCATION\nENTERTAINMENT\nPHOTOGRAPHY\nSHOPPING\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe can say the following about the Google Play Store.\n\n\n\nBoth fun apps and practical apps are very common.\nThe most popular apps are also a mix of fun apps and practical apps."
  },
  {
    "objectID": "posts/2023-08-01-MG-Pokemon-Team-Inspector.html",
    "href": "posts/2023-08-01-MG-Pokemon-Team-Inspector.html",
    "title": "MG Pokemon Team Inspector: a data viz web app for Pokemon players",
    "section": "",
    "text": "I’ve been dissatisfied for a long time with websites that analyze Pokemon based on type effectiveness because they tend to have one or more of these drawbacks:\n\nThey don’t consider both offense and defense simultaneously.\nThey don’t let you input custom Pokemon with any type combination, move types, and two Abilities of your choice.\nThey make you input the Ability of each Pokemon manually. Some people may not realize that their Pokemon has a Hidden Ability that affects its type matchups.\nThey only use the type effectiveness rules for the latest generation of Pokemon games, making their insights inaccurate to older games.\nThey don’t let you assess how useful one team of Pokemon is against another specific team.\n\nWith this in mind, I made an improved web app, which you can access here:\nhttps://mg-pokemon-team-inspector.streamlit.app/\nFirst, you would input your Pokemon team into the app and it would be displayed like this:\n\n\n\n\n\nYou have the following options in managing your teams: add Pokemon, edit Pokemon, save the team so you can load it into the app later, or delete the team.\nYou can choose a Pokemon’s species from the complete Pokedex and input the types of damaging moves your Pokemon has. In some cases, you will be prompted to indicate which Ability your Pokemon has, since this affects its type matchups. All of this information will be used later in the analysis.\n\n\n\n\n\nDifferent Pokemon generations have different rules about which types are effective against others and which Abilities affect a Pokemon’s type defenses.\nThe app takes care of all of that automatically. All you have to do is select the generation of the game you’re playing in the Select Generation page.\nIf you have some Pokemon that are incompatible with the selected generation, the app tells you the problem so you can fix it, like in this picture:\n\n\n\n\n\nNext, you can look at the Analyze Individual Pokemon page. There are three aspects to choose from: Balance, Offense, and Defense. The Balance aspect considers both the offense and defense of the Pokemon against each possible enemy type. It displays labels describing the Pokemon’s usefulness against each type.\nFor example, the picture below shows an analysis of the Pokemon Vespiquen. This Pokemon is labeled as a “Hard Counter” against Grass type Pokemon because it both deals supereffective damage, and takes not-very-effective damage, when fighting Grass types.\nPlus, the rightmost column of the table tells you which of your Pokemon’s move types deal the most damage against a particular type of enemy.\n\n\n\n\n\nThe Offense and Defense aspects have similar tables, but they also have bar charts and other messages to the user. These summarize the information and note important things to look out for.\nHere’s the summary for the Defense analysis of one Pokemon:\n\n\n\n\n\nFinally, the most useful feature of the app is the Analyze Whole Team page, showing you how many of your Pokemon are bad, OK, useful, and optimal against each enemy type. This can help you think of ways to modify your team to cover a wider range of situations.\n\n\n\n\n\nIn this case, I hovered my mouse over the bottom right cell, so that’s why a message popped up and said, “Dewott is a Hard Counter against Steel.” I can hover over any cell in the table to get an explanation like this to help me interpret it.\nIn the leftmost column, I can immediately see that I have at least one optimal Pokemon against most types of enemies, except for Psychic and Dragon. I have some Pokemon that are useful against these types, but not optimal. I may want to adjust my team to fix this.\nAlso, if I encounter an enemy of a particular type, like Fighting, I can look at the row in the table with the same name. It shows me that I should avoid using Magnemite and Lucario against Fighting types, but Roselia is useful and Vespiquen is optimal.\nUnder that chart, there is also a “Detailed View” that lets you choose one enemy type and see the exact numbers behind the insights.\n\n\n\n\n\nThe Team vs Team page shows a similar analysis, except that you load two teams first, then choose which team is “your” team and which is the “enemy” team. This is useful if you know which Pokemon a gym leader has, for example, and you want to be ready to fight their team.\nThis project used the Streamlit framework for web app development, the pandas package for data manipulation, and the Altair package for data visualization.\nThanks for reading!\n\nCredits\nThe app was developed by Miguel Antonio H. Germar.\nI used the Pokedex dataset Pokedex_Ver_SV2.csv, which was downloaded from the following webpage; it is provided publicly without any explicit license. I used the data on Pokemon species names, single and dual typings, Abilities, and Hidden Abilities from this dataset.\nTakamasa Kato. “Pokemon All Status Data (Gen1 to 9).” Kaggle, April 14, 2023. https://www.kaggle.com/datasets/takamasakato/pokemon-all-status-data.\nI created datasets with type effectiveness multipliers (for example, 2 is the multiplier of Fire attacks against Grass Pokemon) by manually copying the numbers in the type chart in the following reference:\nPokémon Database. “Pokémon Type Chart: Strengths and Weaknesses.” Pokémon Database, 2023. https://pokemondb.net/type.\nI obtained the list of Pokemon Abilities that affect type effectiveness (e.g., Dry Skin, Earth Eater, etc.) from the Pokemon Calculator website:\nFennel, S. (2023). Pokémon Type Calculator Defense Page. Pokémon Type Calculator. https://www.pkmn.help/defense\nThen, I used Bulbapedia to learn how exactly those Abilities affect type effectiveness multipliers."
  },
  {
    "objectID": "posts/2024-07-22-BlueHacks-Synergy-Software-Solutions-2024.html",
    "href": "posts/2024-07-22-BlueHacks-Synergy-Software-Solutions-2024.html",
    "title": "📌 1st and 2nd place in Two Coding Competitions",
    "section": "",
    "text": "Our team, S1nergy, at Synergy Software Solutions 2024. Left to right: Aames Juriel Morales, Armand Sotelo, Franz Andrei Layug, Val Allen Eltagonde, and Miguel Antonio Germar (me).\n\n\n\nSynergy 2024\nOn June 26, 2024, our team presented the eSUKAT project at Synergy Software Solutions 2024 (hosted in the UP Diliman EEEI) and won first place. We had become a team since we are all incoming 3rd year BS Applied Mathematics - Master in Data Science students at Ateneo. In late May, we worked on a business proposal based on the prompt, “Design a simple app that allows users to calculate their carbon footprint based on their daily activities, such as transportation, energy use, and food intake.” This led us to develop an app with the following features implemented:\n\neSukat: Emission Trackers\n\nFood consumption tracker. You can record your food consumption in the app by selecting food and drink items and indicating how much of it they consumed (in grams, mL, or money spent in Php). You can view and edit your food consumption from the past 14 days. The app calculates the equivalent carbon emissions caused by the production of your food in kilograms of CO\\(_2\\) equivalents. For these calculations, we used emissions factors from Climatiq, a database compiling emission factors calculated by credible institutions.\nElectricity tracker. If you upload a picture of your Meralco bill, the app will read the electricity consumption data from the past two years. You can keep uploading your Meralco bill every month to update the tracker, and you can also type your electricity consumption manually if your bill was not read correctly. Then, the app estimates the corresponding carbon emissions based on the energy mix of nonrenewable and renewable energy types in your electric grid (Luzon, Visayas, or Mindanao).\nTransportation tracker. After you input your starting point, destination, and mode of transport, the app calculates the distance travelled and the corresponding carbon emissions. If you use multiple modes of transport throughout the day, then you can simply input different trips separately to get an accurate idea of your transport emissions.\n\neSulong: Data Dashboard\n\nPersonal dashboard. You can view your carbon emissions from the past year, compare emissions from the past 2 years, or view an analysis of all of the years since you started using the app. The dashboard displays simple and relevant information, such as: a pie chart showing how much of your emissions came from food, electricity, or transport; bar graphs showing how your emissions changed over time; and indicators of how much your emissions have increased or decreased compared to the same month last year, and the equivalent number of native Philippine trees that would be needed to offset those emissions.\n\n\nOur business proposal describes the other features we had planned for the app and lists our references for calculating carbon emissions. Also, here is the Synergy Facebook page and a post announcing the winners.\nOur original code repository for the eSUKAT project is private, but a forked version can be publicly viewed here: MiguelAHG/synergy-eSUKAT-fork4.\n\n\nBlue Hacks 2024\n\n\n\n\n\n\nOur team, S1, at Blue Hacks 2024. I’m the one in a green sweater.\n\n\nEarlier this year, I had also joined Blue Hacks 2024 with the same team. This was an overnight hackathon hosted in Ateneo; we had 24 hours to make an app that helps people with their mental health. We got 2nd place for Bughaw, an online mental health platform meant to be set up within a school community. It has the following features:\n\nStudent Portal\n\nSchedule appointment. You can quickly schedule an appointment with a guidance counselor based on their available time slots. Requirements like submitting student information and choosing a schedule in a calendar are all done on one page in the app. Our rationale was that the current way of scheduling a counseling appointment in Ateneo is rather confusing.\nChat with guidance counselor. This is a private chat between the student and their assigned counselor, to ease communication regarding scheduling and counseling matters without having to use personal accounts on messaging platforms.\nSupport group chat. If the school decides to group students into online support groups, this feature of the app lets them communicate anonymously, and it is set up so that each group has a guidance counselor to moderate the chat.\nBrief mental health surveys. There is a daily mood tracker, as well as more specific surveys on mental health disorder symptoms which the app can ask students to do once per few weeks or a month.\nMental health resources. One feature is a regularly updated feed of mental health-related articles and research from different websites, implemented via RSS. There is also a feature that lets you select which negative emotion you want to reduce and gives tips on how to do that.\n\nGuidance Counselor Portal\n\nThis lists the counselor’s scheduled appointments, enables the counselor to chat with their assigned students and chat in the support group chat they are moderating, and view anonymized aggregated results of the brief mental health surveys.\n\n\nHere is the Blue Hacks page and a post announcing the winners.\nOur official code repository for the Bughaw project is not publicly available because it is in the account of the event organizers. A mirror of the repository, which we set up for test-deploying the app online, is here: MiguelAHG/sigma-one-test."
  },
  {
    "objectID": "posts/2022-07-05-College-Applications-Dashboard.html",
    "href": "posts/2022-07-05-College-Applications-Dashboard.html",
    "title": "ASHS College Applications Dashboard",
    "section": "",
    "text": "I worked with some members of the Data Analytics Committee (DAC), one of my extracurriculars in senior high school, to conduct a study on our schoolmates’ college applications and the factors that went into them. We disseminated a survey where students could input their college choices. For each college, they would tell us the college location, the interests (like STEM-oriented, ABM-oriented, etc.) related to their chosen courses, and the characteristics of the college that most influenced their choice to apply. Then, I cleaned the data and visualized it in the College Applications Dashboard.\nThe dashboard can be viewed here: ASHS College Apps Dashboard\nShown below is an example of a graph in the dashboard. First, the user may select whether they want to see data for local college applications, international ones, or both. The graph updates accordingly. The y-axis shows labels for college characteristics, such as: perceived high quality of education, having a specific course, higher employability of graduates, perceived popularity, low tuition compared to my other choices, etc. The x-axis shows the Score from 0% to 100%, which is calculated as the average proportion of a student’s college applications where they said that a particular characteristic influenced their choice. If you hover over a bar, a tooltip shows up with the full title of the characteristic and the score. Furthermore, the headline above the graph is there to highlight the most popular characteristic and help the viewer know how to interpret the graph.\n\n\n\n\n\nThis project used the Global Administrative Areas (GADM) database, which is free for non-commercial use.\n\nReferences\nUniversity of Berkeley, Museum of Vertebrate Zoology and the International Rice Research Institute. (2018, April). Global Administrative Areas (GADM) Version 3.4. GADM. https://gadm.org"
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#main-formulas",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#main-formulas",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Main Formulas",
    "text": "Main Formulas\nWe want to compare the probability that a given message is spam to the probability that it is ham. Thus, we use the following formulas:\n\\(P(Spam|w_1, w_2, \\dots , w_n) \\propto P(Spam) \\cdot \\Pi_{i=1}^n P(w_i|Spam)\\)\n\\(P(Ham|w_1, w_2, \\dots , w_n) \\propto P(Ham) \\cdot \\Pi_{i=1}^n P(w_i|Ham)\\)\n\n\n\n\n\n\nNote\n\n\n\nThese formulas are not the same as the Bayes Theorem. To understand how these were derived from the Bayes Theorem, see the Appendix of this post.\n\n\nThese two formulas are identical except for the \\(Spam\\) or \\(Ham\\) event. Let us just look at the first equation to unpack it.\nThe probability of event \\(B\\) given that event \\(A\\) has happened can be represented as \\(P(B|A)\\) (“probability of B given A”). Thus, the left side of the formula, \\(P(Spam|w_1, w_2, \\dots , w_n)\\), represents the probability of spam given the contents of a message. Each variable \\(w_i\\) represents one word in the message. For example, \\(w_1\\) is the first word in the message, and so on.\nIn the middle, the “directly proportional to” sign (\\(\\propto\\)) is used instead of the equals sign. The left and right sides are not equal, but one increases as the other increases.\nAt the right side, \\(P(Spam)\\) simply refers to the probability that any message is spam. It can be calculated as the number of spam messages in the dataset over the total number of messages.\nFinally, the formula ends with \\(\\Pi_{i=1}^n P(w_i|Spam)\\). The \\(P(w_i|Spam)\\) part refers to the probability of a certain word occurring given that the message is known to be spam. We must calculate this probability for each word in the message. Then, because the uppercase pi (\\(\\Pi\\)) refers to a product, we must multiply the word probabilities together."
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#additive-smoothing-and-vocabulary",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#additive-smoothing-and-vocabulary",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Additive Smoothing and Vocabulary",
    "text": "Additive Smoothing and Vocabulary\nIn order to calculate \\(P(w_i|Spam)\\), we need to use the following formula:\n\\(P(w_i | Spam) = \\frac{N_{w_i | Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\\)\nWe use an almost identical equation for \\(P(w_i|Ham)\\) as well:\n\\(P(w_i | Ham) = \\frac{N_{w_i | Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\\)\nAgain, let us just unpack the first formula. \\(N_{w_i|Spam}\\) refers to the number of times that the word appears in the dataset’s spam messages.\n\\(\\alpha\\) is the additive smoothing parameter. We will use \\(\\alpha = 1\\). This is added to the numerator to prevent it from becoming zero. If it does become zero, the entire product in the main formula will become zero.\n\\(N_{Spam}\\) refers to the total number of words in all of the spam messages. Duplicate words are not removed when this is calculated.\nLastly, \\(N_{Vocabulary}\\) refers to the number of words in the vocabulary. This is the set of all unique words found in any of the messages, whether spam or non-spam. Duplicates are removed."
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#model-parameters",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#model-parameters",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Model Parameters",
    "text": "Model Parameters\nIn the first step, we will calculate the parameters of the model, which include the following.\n\n\\(P(Spam), P(Ham)\\)\n\\(N_{Vocabulary}\\)\n\\(N_{Spam}, N_{Ham}\\)\n\\(N_{w_i|Spam}, N_{w_i|Ham}\\)\n\nWe will calculate these values first so that we can plug them into the equation later on when we predict whether new messages are spam or non-spam.\n\n\\(P_{Spam}, P_{Ham}\\)\nThe probability of spam is equal to the number of spam messages over the total number of messages. The same goes for ham messages.\n\n\nCode\np_label = {}\np_label[\"spam\"] = y_train.eq(\"spam\").sum() / y_train.shape[0]\np_label[\"ham\"] = 1 - p_label[\"spam\"]\n\nprint(f\"P(Spam) = {p_label['spam'] * 100:.2f}%\")\nprint(f\"P(Ham) = {p_label['ham'] * 100:.2f}%\")\n\n\nP(Spam) = 13.41%\nP(Ham) = 86.59%\n\n\n\n\nMessage Preprocessing\nBelow are the messages:\n\n\nCode\nX_train.head()\n\n\n\n\n\n\n\n\n\nsms\n\n\n\n\n0\nMarvel Mobile Play the official Ultimate Spide...\n\n\n1\nThank you, winner notified by sms. Good Luck! ...\n\n\n2\nFree msg. Sorry, a service you ordered from 81...\n\n\n3\nThanks for your ringtone order, ref number R83...\n\n\n4\nPRIVATE! Your 2003 Account Statement for shows...\n\n\n\n\n\n\n\nIn order to get individual words, we make all words lowercase and remove punctuation marks and other non-word characters. We then turn each message into a list of its words.\n\n\nCode\ndef preprocess_messages(series):\n    result = (\n        series\n        .str.lower()\n        # Delete all non-word characters.\n        .str.replace(r\"[^a-z0-9 ]\", \"\", regex = True)\n        .str.strip()\n        .str.split()\n    )\n\n    return result\n\nX_train = pd.DataFrame(preprocess_messages(X_train.sms))\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nsms\n\n\n\n\n0\n[marvel, mobile, play, the, official, ultimate...\n\n\n1\n[thank, you, winner, notified, by, sms, good, ...\n\n\n2\n[free, msg, sorry, a, service, you, ordered, f...\n\n\n3\n[thanks, for, your, ringtone, order, ref, numb...\n\n\n4\n[private, your, 2003, account, statement, for,...\n\n\n\n\n\n\n\n\n\nVocabulary\nUsing the preprocessed messages, we can form a set of all of the unique words that they contain.\n\n\nCode\nvocab = set()\nfor lst in X_train.sms:\n    vocab.update(lst)\n\n# Use a Series to delete items that are blank or only contain whitespace.\nvocab_series = pd.Series(list(vocab))\nvocab_series = vocab_series.loc[~vocab_series.str.match(\"^\\s*$\")]\nvocab = set(vocab_series)\n\nn_vocab = len(vocab)\n\nprint(f\"Number of words in the vocabulary: {n_vocab}\\nFirst few items:\")\nlist(vocab)[:10]\n\n\nNumber of words in the vocabulary: 8385\nFirst few items:\n\n\n['silent',\n 'receivea',\n 'dismay',\n 'noise',\n 'platt',\n 'li',\n 'woohoo',\n 'ucall',\n 'wondarfull',\n '80082']\n\n\nAbove are the first 10 items in the vocabulary. In total, \\(N_{Vocabulary} = 8385\\).\n\n\n\\(N_{Spam}, N_{Ham}\\)\nUsing the vocabulary, we can transform the messages to show the number of times that each word appears in each message.\n\n\nCode\ndef to_word_counts(series, vocab = vocab):\n    vocab_lst = list(sorted(vocab))\n\n    word_counts = pd.DataFrame({\n        w: [0] * series.shape[0]\n        for w in vocab_lst\n    })\n\n    for index, word_lst in series.iteritems():\n        for w in word_lst:\n            if w in vocab:\n                word_counts.loc[index, w] += 1\n\n    return word_counts\n\nword_counts = to_word_counts(X_train.sms)\n\nword_counts.head()\n\n\n\n\n\n\n\n\n\n0\n008704050406\n0089my\n0121\n01223585236\n01223585334\n0125698789\n020603\n0207\n02070836089\n...\nzebra\nzed\nzeros\nzhong\nzindgi\nzoe\nzogtorius\nzoom\nzouk\nzyada\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 8385 columns\n\n\n\nIn the table above, each row represents a message. Each column represents a unique word in the vocabulary. The cells show the number of times that each word appeared in each message.\nNow, we can calculate \\(N_{Spam}, N_{Ham}\\):\n\n\nCode\ndef count_n(label, word_counts = word_counts):\n    n_label = (\n        word_counts\n        .loc[y_train == label, :]\n        # Sum all of the numbers in the df.\n        .sum()\n        .sum()\n    )\n    return n_label\n\nn_label = {}\n\nfor label in [\"spam\", \"ham\"]:\n    n_label[label] = count_n(label)\n\nprint(f\"Number of words in spam messages: {n_label['spam']}\")\nprint(f\"Number of words in ham messages: {n_label['ham']}\")\n\n\nNumber of words in spam messages: 14037\nNumber of words in ham messages: 53977\n\n\nThe result is that \\(N_{Spam} = 14037\\) and \\(N_{Ham} = 53977\\).\n\n\n\\(N_{w_i|Spam}, N_{w_i|Ham}\\)\nFinally, we can use the word counts to determine these two parameters. Recall that \\(N_{w_i|Spam}\\) is the number of times that a certain word \\(w_i\\) appeared in the spam messages.\n\n\nCode\nfull_train = pd.concat(\n    [y_train, word_counts],\n    axis = 1,\n)\n\nn_word_given_label = full_train.pivot_table(\n    values = vocab_lst,\n    index = \"label\",\n    aggfunc = np.sum,\n)\n\nn_word_given_label\n\n\n\n\n\n\n\n\n\n0\n008704050406\n0089my\n0121\n01223585236\n01223585334\n0125698789\n020603\n0207\n02070836089\n...\nzebra\nzed\nzeros\nzhong\nzindgi\nzoe\nzogtorius\nzoom\nzouk\nzyada\n\n\nlabel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nham\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n1\n1\n1\n0\n1\n1\n0\n1\n\n\nspam\n3\n1\n1\n1\n1\n1\n0\n4\n2\n1\n...\n1\n4\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n2 rows × 8385 columns\n\n\n\nThe table above can be used to access these parameters. For example, if we want to access \\(N_{w_i | Spam}\\) for the word “hello”, we can look at the value where the “spam” row and “hello” column intersect. The value is the number of times that “hello” appeared in the spam messages.\n\n\nCode\nn_hello_spam = n_word_given_label.at[\"spam\", \"hello\"]\n\nprint(f\"Number of times that 'hello' appeared in spam messages: {n_hello_spam}\")\n\n\nNumber of times that 'hello' appeared in spam messages: 3"
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#predictive-function",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#predictive-function",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Predictive Function",
    "text": "Predictive Function\nNow that all of the parameters have been found, we can write a function that will take a new message and classify it as spam or non-spam. This function will use the formulas explained earlier.\n\ndef predict(word_lst, out = \"both\", alpha = 1, vocab = vocab, p_label = p_label, n_label = n_label, n_word_given_label = n_word_given_label):\n    \"\"\"Given the list of words in a message, predict whether it is spam or ham.\nword_lst: The preprocessed list of words in the message.\nout: \"both\" to output both probabilities and prediction in a tuple. \"pred\" to output only the prediction as a string.\"\"\"\n\n    # Set up a Series to store results\n    results = pd.Series(dtype = np.float64)\n\n    for label in [\"spam\", \"ham\"]:\n        # Use P(Spam) or P(Ham)\n        final = p_label[label]\n\n        # Iterate through words in the message.\n        for w in word_lst:\n            # Only include a word if it is already in the vocabulary.\n            if w in vocab:\n                # Calculate P(w1, w2, ..., wn | Spam) using the formula.\n                p_word_given_label = (\n                    (n_word_given_label.at[label, w] + alpha)\n                    / (n_label[label] + alpha * n_vocab)\n                )\n\n                # Multiply the result into the final value.\n                final *= p_word_given_label\n\n        results[label] = final\n    \n    # The prediction is the label with the higher probability in the Series.\n    # If the probabilities are equal, the prediction is \"uncertain\"\n    if results[\"spam\"] == results[\"ham\"]:\n        prediction = \"uncertain\"\n    else:\n        prediction = results.idxmax()\n\n    if out == \"both\":\n        return results, prediction\n    elif out == \"pred\":\n        return prediction\n\nLet us try using this function to predict whether a message is spam or ham. We will use this example: “you won a prize claim it now by sending credit card details”.\n\n\nCode\nresults, prediction = predict(\"you won a prize claim it now by sending credit card details\".split())\n\nprint(\"Results:\")\nfor label, value in results.iteritems():\n    print(f\"P({label} | message) is proportional to {value}\")\nprint(f\"This message is predicted to be {prediction}.\")\n\n\nResults:\nP(spam | message) is proportional to 2.3208952599406518e-35\nP(ham | message) is proportional to 1.8781562825001382e-41\nThis message is predicted to be spam.\n\n\nThe algorithm determined that \\(P(Spam|w_1, w_2, \\dots , w_n) \\propto 2.32 \\cdot 10^{-35}\\), whereas \\(P(Ham|w_1, w_2, \\dots , w_n) \\propto 1.88 \\cdot 10^{-41}\\). Since the probability for spam was higher, it predicted that the message was spam."
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#model-evaluation",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#model-evaluation",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nThe final step is to evaluate the predictive function. We will use the function to predict labels for the messages in the testing set. Then, we will show the predicted labels side-by-side with the real labels.\n\n\nCode\n# Preprocess testing messages\nX_test_preprocessed = preprocess_messages(X_test.sms)\n\n# Make predictions\ny_pred = X_test_preprocessed.apply(predict, out = \"pred\")\ny_pred.name = \"prediction\"\n\n# Concatenate\nfull_test = pd.concat(\n    [y_test, y_pred, X_test],\n    axis = 1\n)\n\nfull_test.head()\n\n\n\n\n\n\n\n\n\nlabel\nprediction\nsms\n\n\n\n\n0\nspam\nspam\nEngland v Macedonia - dont miss the goals/team...\n\n\n1\nspam\nspam\nSMS. ac Sptv: The New Jersey Devils and the De...\n\n\n2\nspam\nspam\nPlease call our customer service representativ...\n\n\n3\nspam\nspam\nURGENT! Your Mobile No. was awarded £2000 Bonu...\n\n\n4\nspam\nspam\nSunshine Quiz Wkly Q! Win a top Sony DVD playe...\n\n\n\n\n\n\n\nThe table above shows the first 5 rows of the testing set. We can see that the algorithm correctly predicted that the first 5 rows were spam.\n\nAccuracy\nWe will now calculate the overall accuracy of the model by dividing the number of correct predictions by the total number of predictions.\n\n\nCode\nacc = y_test.eq(y_pred).sum() / y_pred.shape[0] * 100\n\nprint(f\"Accuracy: {acc:.2f}%\")\n\n\nAccuracy: 98.74%\n\n\nThe model turned out to have a very high accuracy of 98.74%. This shows that it is effective at filtering spam from non-spam.\nHowever, considering that spam and non-spam did not have equal representation in the data, with only 13% of all messages being spam, the accuracy may be misleading (Vallantin, 2018). For example, if a model is instructed to always predict that a message is ham, it would still have an accuracy of around 87%. Thus, to get a better picture of the performance of the model, we will make a confusion matrix and use other evaluation metrics such as precision, recall, and F1.\n\n\nConfusion Matrix\nA confusion matrix is a table that gives insight into how well a model was able to predict labels for datapoints (Brownlee, 2016).\n\n\nCode\ncm = confusion_matrix(y_test, y_pred, labels = [\"ham\", \"spam\"])\n\ndisp = ConfusionMatrixDisplay(cm, display_labels = [\"ham\", \"spam\"])\ndisp.plot()\n\nplt.title(\"Confusion Matrix of Results\")\nplt.show()\n\n\n\n\n\nIn this case, we only have two labels (spam and ham), so there are two rows and two columns. The vertical axis refers to the true labels, whereas the horizontal axis refers to the predicted labels.\nFor example, if we want to know how many true spam messages were incorrectly predicted to be ham, we can look at the “spam” row and “ham” column. The value is 13, which means that 13 messages were incorrectly predicted in this way.\nIf we consider ham messages as “negative” and spam messages as “positive,” then the confusion matrix above shows the True Negatives (TN) in the top left, False Negatives (FN) in the top right, False Positives (FP) in the bottom left, and True Positives (TP) in the bottom right.\n\n\nRecall, Precision, F1\nRecall, Precision, and F1 are other metrics of model performance which may be useful when the labels are not represented equally in the data. Their equations are shown below; these were taken from Brownlee (2020).\nRecall is the number of true positives divided by the total number of actual positives. It measures the “ability of the classifier to find all the positive samples. (scikit-learn developers, 2021)”\n\\(Recall = \\frac{TP}{TP + FN}\\)\nPrecision is the number of true positives divided by the total number of predicted positives. It is the “ability of the classifier not to label as positive a sample that is negative” (scikit-learn developers, 2021). In other words, it is the ability of the model to avoid Type I error (Bhandari, 2021).\n\\(Precision = \\frac{TP}{TP+FP}\\)\nFinally, \\(F_{\\beta}\\) is the “weighted harmonic mean of the precision and recall,” and \\(F_1\\) is the version where both of these factors are given equal importance (scikit-learn developers, 2021).\n\\(F1 = \\frac{2 TP}{2 TP + FP + FN}\\)\nLet us calculate these metrics for the spam filter using the confusion matrix.\n\n\nCode\ntp = 136\ntn = 964\nfp = 1\nfn = 13\n\nmetrics = {}\nmetrics[\"recall\"] = tp / (tp + fn)\nmetrics[\"precision\"] = tp / (tp + fp)\nmetrics[\"f1\"] = 2 * tp / (2 * tp + fp + fn)\n\nfor key in metrics:\n    print(f\"{key}: {metrics[key] * 100:.2f}%\")\n\n\nrecall: 91.28%\nprecision: 99.27%\nf1: 95.10%\n\n\nAll three metrics are above 90%. In particular, precision is 99.27%, which means that the model is very good at avoiding labelling ham messages as spam.\nTherefore, we can say that the naive bayes algorithm performs very well with the data that it was given."
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#data-source",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#data-source",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Data Source",
    "text": "Data Source\nAlmeida, T. A., & Hidalgo, J. M. G. (2012, June 22). SMS Spam Collection Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/sms+spam+collection#"
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#information-sources",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#information-sources",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Information Sources",
    "text": "Information Sources\nBhandari, P. (2021, January 18). Type I and Type II errors. Scribbr. https://www.scribbr.com/statistics/type-i-and-type-ii-errors/\nBrownlee, J. (2016, November 17). What is a Confusion Matrix in Machine Learning. Machine Learning Mastery. https://machinelearningmastery.com/confusion-matrix-machine-learning/\nBrownlee, J. (2020, August 27). How to Calculate Precision, Recall, F1, and More for Deep Learning Models. Machine Learning Mastery. https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\nDataquest. (n.d.). Guided Project: Building A Spam Filter With Naive Bayes. Dataquest. Retrieved December 14, 2021, from https://www.dataquest.io/c/74/m/433/guided-project%3A-building-a-spam-filter-with-naive-bayes\nDevanesan, J. (2020, August 19). Phishing scams dominate the Philippines cybercrime landscape. Tech Wire Asia. https://techwireasia.com/2020/08/phishing-scams-dominate-the-philippines-cybercrime-landscape/\nFTC CI. (2020, February 19). How To Recognize and Report Spam Text Messages. Federal Trade Commission Consumer Information. https://www.consumer.ftc.gov/articles/how-recognize-and-report-spam-text-messages\nGupta, S. (2020, February 28). Pros and cons of various Classification ML algorithms. Medium. https://towardsdatascience.com/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6\nscikit-learn developers. (2021). 1.9. Naive Bayes. Scikit-Learn. https://scikit-learn/stable/modules/naive_bayes.html\nscikit-learn developers. (2021). 3.3. Metrics and scoring: Quantifying the quality of predictions. Scikit-Learn. https://scikit-learn/stable/modules/model_evaluation.html\nThomas, L. (2020, September 18). How to use stratified sampling. Scribbr. https://www.scribbr.com/methodology/stratified-sampling/ Vadapalli, P. (2021, January 5). Naive Bayes Explained: Function, Advantages & Disadvantages, Applications in 2021. UpGrad Blog. https://www.upgrad.com/blog/naive-bayes-explained/\nVallantin, L. (2018, September 6). Why you should not trust only in accuracy to measure machine learning performance. Medium. https://medium.com/@limavallantin/why-you-should-not-trust-only-in-accuracy-to-measure-machine-learning-performance-a72cf00b4516\nYap, C. (2021, November 25). Millions of spam messages blocked in Philippines as scams surge. Yahoo News. https://ph.news.yahoo.com/millions-of-spam-messages-blocked-in-philippines-as-scams-surge-005658569.html"
  },
  {
    "objectID": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#image-source",
    "href": "posts/2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html#image-source",
    "title": "Naive Bayes Algorithm for Detecting Spam Messages",
    "section": "Image Source",
    "text": "Image Source\nJohnson, H. (2020, February 24). Photo by Hannes Johnson on Unsplash. Unsplash. https://unsplash.com/photos/mRgffV3Hc6c"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hi, I’m Migs Germar.\nWelcome to my data science blog. Feel free to explore my posts below. I recommend checking out My Portfolio Projects, which lists my best works. You can also click a category on the right side of the page to filter the posts.\nIf you’d like to get in touch, see the About page.\nThanks for visiting!\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n📌 My Portfolio Projects\n\n\n\n\n\n\n\nportfolio\n\n\n\n\nA list of my best original works and links to view them.\n\n\n\n\n\n\nJul 22, 2024\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\n📌 My Learning Journey in Data Science\n\n\n\n\n\n\n\njournal\n\n\n\n\nA timeline of important milestones in my data science learning journey.\n\n\n\n\n\n\nJul 5, 2022\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\n📌 Guide to Start Learning Data Science\n\n\n\n\n\n\n\nguide\n\n\n\n\nI share a guide for how beginners can start learning data science, based on my personal experience.\n\n\n\n\n\n\nJul 10, 2022\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\n📌 1st and 2nd place in Two Coding Competitions\n\n\n\n\n\n\n\npython\n\n\ncompetition\n\n\npandas\n\n\nseaborn\n\n\nstreamlit\n\n\ngit\n\n\n\n\nOur team won 1st and 2nd place in two coding competitions, Synergy Software Solutions 2024 and Blue Hacks 2024, respectively. We had developed a comprehensive personal carbon footprint calculator, as well as a mental health platform for students and guidance counselors.\n\n\n\n\n\n\nJul 22, 2024\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nMG Pokemon Team Inspector: a data viz web app for Pokemon players\n\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\npandas\n\n\naltair\n\n\n\n\nI developed a web app where Pokemon players can input their teams and generate visualizations based on type effectiveness.\n\n\n\n\n\n\nAug 1, 2023\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nASHS College Applications Dashboard\n\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\nschool-project\n\n\n\n\nI developed a dashboard visualizing my senior high school’s college application choices and the factors influencing them.\n\n\n\n\n\n\nJul 5, 2022\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nASHS Student Mapping Project: A Tool for Disaster Risk Reduction in the Online Set-up\n\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\nschool-project\n\n\n\n\nI developed an app for identifying students in my school who are affected by natural hazards.\n\n\n\n\n\n\nMay 10, 2022\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nCompleted an Online Course: Dataquest Data Scientist in Python\n\n\n\n\n\n\n\npython\n\n\nonline-course\n\n\n\n\nToday, I finished a 325-hour online course. In this post, I show my certificate and outline the topics that I learned.\n\n\n\n\n\n\nApr 16, 2022\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nUsing a Neural Network to Classify Handwritten Digits\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\nscikit-learn\n\n\n\n\nI compare the performance of a neural network to that of a K Nearest Neighbors model in classifying images of handwritten numbers. I also demonstrate the use of Grid Search for hyperparameter optimization.\n\n\n\n\n\n\nJan 21, 2022\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nComparison of Regression Models for Predicting Bike Rentals\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\nsklearn\n\n\nstatsmodels\n\n\n\n\nI compare the performance of three machine learning models (linear regression, decision tree, random forest) in predicting the number of bike rentals that may occur at a given time in Washington, D.C.\n\n\n\n\n\n\nJan 16, 2022\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nUsing Linear Regression to Predict House Sale Prices\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nseaborn\n\n\nmatplotlib\n\n\nsklearn\n\n\nstatsmodels\n\n\n\n\nI interpret linear regression results to determine features that significantly affect house sale prices. I then use the same model to predict house prices, and evaluate the model using stratified k-fold cross-validation.\n\n\n\n\n\n\nDec 28, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Car Prices using the K Nearest Neighbors Algorithm\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\nscipy\n\n\nsklearn\n\n\n\n\nI use various machine learning workflow techniques to arrive at the optimal K Nearest Neighbors (KNN) regression model for predicting car prices.\n\n\n\n\n\n\nDec 21, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nagriHanda: an Agricultural Disaster Risk Web App\n\n\n\n\n\n\n\npython\n\n\ncompetition\n\n\npandas\n\n\naltair\n\n\nstreamlit\n\n\ngit\n\n\n\n\nI developed a web app that won two awards in the Project SPARTA PH Open Data Challenge for Butuan City.\n\n\n\n\n\n\nDec 17, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nNaive Bayes Algorithm for Detecting Spam Messages\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nscikit-learn\n\n\n\n\nI code a multinomial naive bayes algorithm, step-by-step, in order to identify messages as spam or non-spam. I then test the model and evaluate it using various metrics of model performance.\n\n\n\n\n\n\nDec 14, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nBasic Statistics with Fandango Movie Ratings\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\nscipy\n\n\n\n\nI apply basic statistics concepts and hypothesis testing (i.e., Mann-Whitney U rank test) in comparing Fandango movie rating distributions from 2015 and 2016.\n\n\n\n\n\n\nNov 18, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nAnswering Business Questions for an Online Music Store using SQL\n\n\n\n\n\n\n\nsql\n\n\nsqlite\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\naltair\n\n\n\n\nI use intermediate SQL techniques like views, joins, aggregations, and set operations in order to solve 4 scenarios about a hypothetical online music store. Results are communicated with Matplotlib and Altair visualizations.\n\n\n\n\n\n\nJul 31, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nUsing SQL to Query and Analyze a CIA Factbook Database\n\n\n\n\n\n\n\nsql\n\n\nsqlite\n\n\npython\n\n\npandas\n\n\naltair\n\n\n\n\nI use SQL to explore a database of facts about countries in the world. In addition, I use Pandas and Altair to perform data transformations and create interactive charts on population-related issues.\n\n\n\n\n\n\nJul 24, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nHow Student Demographic Affects SAT Performance in NYC\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\nstatsmodels\n\n\n\n\nI combine multiple datasets about NYC high schools to explore the effect of students’ demographics on SAT performance. The project ends with a multiple linear regression model showing that the effect of race percentages on a school’s average SAT score is significant.\n\n\n\n\n\n\nJun 13, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nEmployee Exit Survey Data Cleaning and Aggregation\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\n\n\nI use intermediate Pandas techniques in cleaning and analyzing employee exit surveys from Australian government institutes.\n\n\n\n\n\n\nJun 1, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nMaking an Explanatory Chart of USD-PHP Exchange Rates\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\n\n\nI detail the process of creating an explanatory chart that compares exchange rate trends before and during the COVID-19 pandemic.\n\n\n\n\n\n\nMay 28, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nIndicators of Heavy Traffic on the I-94 Highway\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\n\n\nI conduct exploratory data analysis and Seaborn visualizations in order to find indicators of heavy traffic on I-94.\n\n\n\n\n\n\nMay 25, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nBasic Data Cleaning with eBay Car Sales Data\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\nmatplotlib\n\n\nseaborn\n\n\n\n\nI clean a messy dataset of eBay car sales data before performing simple analyses about car prices.\n\n\n\n\n\n\nMay 20, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nOptimizing Hacker News Posts\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\naltair\n\n\n\n\nHacker News is a popular tech posting site. I used data analysis in order to determine the best practices for optimizing the number of comments on your post.\n\n\n\n\n\n\nMay 11, 2021\n\n\nMigs Germar\n\n\n\n\n\n\n  \n\n\n\n\nProfitable App Profiles for iOS and Android\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\naltair\n\n\n\n\nIn this project, I analyze app store data and determine profitable app profiles for a hypothetical app company.\n\n\n\n\n\n\nMay 8, 2021\n\n\nMigs Germar\n\n\n\n\n\n\nNo matching items"
  }
]