{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /python/pandas/numpy/matplotlib/scikit-learn/2021/12/14/Naive-Bayes-Algorithm-Detecting-Spam-Messages\n",
    "author: Migs Germar\n",
    "\n",
    "branch: master\n",
    "categories:\n",
    "- python\n",
    "- pandas\n",
    "- matplotlib\n",
    "- scikit-learn\n",
    "date: '2021-12-14'\n",
    "description: I code a multinomial naive bayes algorithm, step-by-step, in order to\n",
    "  identify messages as spam or non-spam. I then test the model and evaluate it using\n",
    "  various metrics of model performance.\n",
    
    "image: images/spam-unsplash-hannes_johnson.jpg\n",
    "output-file: 2021-12-14-naive-bayes-algorithm-detecting-spam-messages.html\n",
    "\n",
    "title: Naive Bayes Algorithm for Detecting Spam Messages\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "mounted-prime",
   "metadata": {},
   "source": [
    "![](images/spam-unsplash-hannes_johnson.jpg){fig-align=\"center\"}\n",
    "\n",
    "<center><a href = \"https://unsplash.com/photos/mRgffV3Hc6c\">Unsplash | Hannes Johnson</a></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aware-valuable",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The Multinomial Naive Bayes Algorithm is a machine learning algorithm based on the Bayes Theorem. It calculates the probability that an event $B$ occurred given that event $A$ occurred. Thus, it is usually used in classification problems. (Vadapalli, 2021)\n",
    "\n",
    "In this project, we will use the algorithm to determine the probability that a message is spam given its contents. We will then use this probability to decide whether to treat new messages as spam or not. For example, if the probability of being spam is over 50%, then we may treat the message as spam.\n",
    "\n",
    "Identifying spam is important in the Philippines because phishing campaigns went up by 200% after the pandemic began (Devanesan, 2020), and a telecommunications provider recently had to block around 71 million spam messages (Yap, 2021). Such messages may attempt to steal personal information, steal money from an account, or install malware (FTC, 2020). Thus, machine learning can be a very helpful tool in preventing such harm from occurring.\n",
    "\n",
    "Though the algorithm can be easily implemented using existing functions such as those in the [scikit-learn package](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes), I will manually code the algorithm step-by-step in order to explain the mathematical intuition behind it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "administrative-negotiation",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "I wrote this notebook by following a guided project on the [Dataquest](https://www.dataquest.io/) platform, specifically the [Guided Project: Building a Spam Filter with Naive Bayes](https://app.dataquest.io/c/74/m/433/guided-project%3A-building-a-spam-filter-with-naive-bayes/1/exploring-the-dataset) The general project flow came from Dataquest. The mathematical explanations are also based on what I learned from Dataquest.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "suited-challenge",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "Below are the packages necessary for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "christian-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "quality-twelve",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "We will use the SMS Spam Collection Dataset by Almeida and Hidalgo in 2012. It can be downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee9c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   5572 non-null   object\n",
      " 1   sms     5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "sms_df = pd.read_csv(\n",
    "    \"./private/Naive-Bayes-Files/SMSSpamCollection\",\n",
    "    # Tab-separated\n",
    "    sep = \"\\t\",\n",
    "    header = None,\n",
    "    names = [\"label\", \"sms\"]\n",
    ")\n",
    "\n",
    "sms_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f83348e4",
   "metadata": {},
   "source": [
    "The dataset has 2 columns and 5572 rows.\n",
    "\n",
    "- The `label` column contains \"ham\" if the message is legitimate, or \"spam\" if it is spam.\n",
    "- The `sms` column contains individual SMS messages.\n",
    "\n",
    "For example, below are the first 5 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0347763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                sms\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "sms_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3794d4b3",
   "metadata": {},
   "source": [
    "# Training and Testing Sets\n",
    "\n",
    "The messages will be split into two sets. The training set, comprising 80% of the total data, will be used to train the Naive Bayes Algorithm. The testing set, with 20% of the total data, will be used to test the model's accuracy.\n",
    "\n",
    "First, however, let us calculate what percentage of the messages in the dataset are spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085c9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of spam messages: 13.41%\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "spam_perc = sms_df[\"label\"].eq(\"spam\").sum() / sms_df.shape[0] * 100\n",
    "print(f\"Percentage of spam messages: {spam_perc:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d793148",
   "metadata": {},
   "source": [
    "Only 13% of the messages are spam. Therefore, spam and non-spam messages are not equally represented in this dataset, and this may be problematic. However, this is all the data we have, so the best we can do is to ensure that both the training and testing sets have around 13% of their messages as spam.\n",
    "\n",
    "This is an example of *proportionate stratified sampling* (Thomas, 2020). We first separate the data into two strata (spam and non-spam). We then take 80% of the messages from each strata as the training set. The remaining 20% of each strata is set aside for the testing set. Thus, each of the two sets will contain around 13% spam.\n",
    "\n",
    "This has been done with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5e0d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 4458\n",
      "Percentage of training messages that are spam: 13.41%\n",
      "Number of rows in testing set: 1114\n",
      "Percentage of testing messages that are spam: 13.38%\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "# Note: I could have used `train_test_split` from sklearn, but I coded this manually for the sake of grasping the logic.\n",
    "split_lists = {\n",
    "    \"training\": [],\n",
    "    \"testing\": [],\n",
    "}\n",
    "\n",
    "# Stratify the dataset\n",
    "for label in \"spam\", \"ham\":\n",
    "    stratum = sms_df.loc[sms_df[\"label\"] == label]\n",
    "\n",
    "    train_part = stratum.sample(\n",
    "        # Sample 80% of the data points\n",
    "        frac = 0.8,\n",
    "        random_state = 1,\n",
    "    )\n",
    "\n",
    "    # The other 20% that were not sampled go to the testing set.\n",
    "    test_part = stratum.loc[~stratum.index.isin(train_part.index)]\n",
    "\n",
    "    split_lists[\"training\"].append(train_part)\n",
    "    split_lists[\"testing\"].append(test_part)\n",
    "\n",
    "split_dfs = pd.Series(dtype = \"object\")\n",
    "for key in split_lists:\n",
    "    # Concatenate spam and non-spam parts into one DataFrame.\n",
    "    set_df = pd.concat(split_lists[key]).reset_index()\n",
    "    split_dfs[key] = set_df\n",
    "\n",
    "    perc_spam = set_df.label.eq('spam').sum() / set_df.shape[0] * 100\n",
    "\n",
    "    print(f\"Number of rows in {key} set: {set_df.shape[0]}\")\n",
    "    print(f\"Percentage of {key} messages that are spam: {perc_spam:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac9d4de6",
   "metadata": {},
   "source": [
    "We can see that the percentage of spam messages is roughly the same between the two sets. This will help the accuracy of the model later on.\n",
    "\n",
    "Now, the two sets will be further split into `X` and `y`. `y` refers to the **target**, or the variable that we are trying to predict. In this case, we are trying to predict whether a message is spam or non-spam, so the \"label\" column is the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b829d88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ham\n",
       "1     ham\n",
       "2    spam\n",
       "3     ham\n",
       "4     ham\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "sms_df.label.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "565535cc",
   "metadata": {},
   "source": [
    "On the other hand, `X` refers to the **features**, which are information used to predict the target. We only have one feature column as of now, which is the \"sms\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79d7bdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Go until jurong point, crazy.. Available only ...\n",
       "1                        Ok lar... Joking wif u oni...\n",
       "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    U dun say so early hor... U c already then say...\n",
       "4    Nah I don't think he goes to usf, he lives aro...\n",
       "Name: sms, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "sms_df.sms.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e7696d",
   "metadata": {},
   "source": [
    "\n",
    "Thus, we end up with four final objects:\n",
    "\n",
    "- `X_train`: The messages in the training data.\n",
    "- `X_test`: The messages in the testing data.\n",
    "- `y_train`: The labels in the training data. These correspond to `X_train`.\n",
    "- `y_test`: The labels in the testing data. These correspond to `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d351cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "# The four objects listed above.\n",
    "X_train = split_dfs.training[[\"sms\"]].copy()\n",
    "X_test = split_dfs.testing[[\"sms\"]].copy()\n",
    "y_train = split_dfs.training[\"label\"].copy()\n",
    "y_test = split_dfs.testing[\"label\"].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "133d0a53",
   "metadata": {},
   "source": [
    "# The Algorithm\n",
    "\n",
    "Now, let's discuss the multinomial naive bayes algorithm. Conditional probability is necessary in order to understand it. For our use case, let $Spam$ be the event that a message is spam, and $Ham$ be the event for non-spam.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "The mathematical explanations below are not my own ideas. I learned these from the Dataquest course on Naive Bayes.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f62660e8",
   "metadata": {},
   "source": [
    "\n",
    "## Main Formulas\n",
    "\n",
    "We want to compare the probability that a given message is spam to the probability that it is ham. Thus, we use the following formulas:\n",
    "\n",
    "$P(Spam|w_1, w_2, \\dots , w_n) \\propto P(Spam) \\cdot \\Pi_{i=1}^n P(w_i|Spam)$\n",
    "\n",
    "$P(Ham|w_1, w_2, \\dots , w_n) \\propto P(Ham) \\cdot \\Pi_{i=1}^n P(w_i|Ham)$\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "These formulas are not the same as the Bayes Theorem. To understand how these were derived from the Bayes Theorem, see the Appendix of this post.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9a754c1",
   "metadata": {},
   "source": [
    "These two formulas are identical except for the $Spam$ or $Ham$ event. Let us just look at the first equation to unpack it.\n",
    "\n",
    "The probability of event $B$ given that event $A$ has happened can be represented as $P(B|A)$ (\"probability of B given A\"). Thus, the left side of the formula, $P(Spam|w_1, w_2, \\dots , w_n)$, represents the probability of spam given the contents of a message. Each variable $w_i$ represents one word in the message. For example, $w_1$ is the first word in the message, and so on.\n",
    "\n",
    "In the middle, the \"directly proportional to\" sign ($\\propto$) is used instead of the equals sign. The left and right sides are not equal, but one increases as the other increases.\n",
    "\n",
    "At the right side, $P(Spam)$ simply refers to the probability that any message is spam. It can be calculated as the number of spam messages in the dataset over the total number of messages.\n",
    "\n",
    "Finally, the formula ends with $\\Pi_{i=1}^n P(w_i|Spam)$. The $P(w_i|Spam)$ part refers to the probability of a certain word occurring given that the message is known to be spam. We must calculate this probability for each word in the message. Then, because the uppercase pi ($\\Pi$) refers to a product, we must multiply the word probabilities together.\n",
    "\n",
    "## Additive Smoothing and Vocabulary\n",
    "\n",
    "In order to calculate $P(w_i|Spam)$, we need to use the following formula:\n",
    "\n",
    "$P(w_i | Spam) = \\frac{N_{w_i | Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}$\n",
    "\n",
    "We use an almost identical equation for $P(w_i|Ham)$ as well:\n",
    "\n",
    "$P(w_i | Ham) = \\frac{N_{w_i | Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}$\n",
    "\n",
    "Again, let us just unpack the first formula. $N_{w_i|Spam}$ refers to the number of times that the word appears in the dataset's spam messages.\n",
    "\n",
    "$\\alpha$ is the **additive smoothing parameter**. We will use $\\alpha = 1$. This is added to the numerator to prevent it from becoming zero. If it does become zero, the entire product in the main formula will become zero.\n",
    "\n",
    "$N_{Spam}$ refers to the total number of words in all of the spam messages. Duplicate words are not removed when this is calculated.\n",
    "\n",
    "Lastly, $N_{Vocabulary}$ refers to the number of words in the **vocabulary**. This is the set of all *unique* words found in any of the messages, whether spam or non-spam. Duplicates are removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b262259",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "Based on the theory behind the algorithm, I have written a set of steps to implement it. I will use these steps as the pseudocode for this project.\n",
    "\n",
    "1. Determine the model **parameters**. These are the variables in the formulas shown earlier. Only the training data will be used for this.\n",
    "    - Find $P(Spam), P(Ham)$.\n",
    "        - Divide the number of spam messages by the total number of messages.\n",
    "        - Do the same for ham messages.\n",
    "    - Preprocess the messages to focus on individual words.\n",
    "        - Make all words lowercase.\n",
    "        - Remove punctuation marks.\n",
    "    - Form a vocabulary. \n",
    "        - Make a set of all the words in the messages, without duplicates.\n",
    "        - $N_{Vocabulary}$ is the number of words in this set.\n",
    "    - Find $N_{Spam}, N_{Ham}$.\n",
    "        - Count the number of times each word appears in each message.\n",
    "        - Count the total number of words in spam messages. Do the same for ham messages.\n",
    "    - Find $N_{w_i|Spam}, N_{w_i|Ham}$ for each word in the vocabulary.\n",
    "        - Sum up the word counts in spam messages to get $N_{w_i|Spam}$.\n",
    "        - Do the same for ham messages to get $N_{w_i|Spam}$.\n",
    "1. Write a **predictive function**. This takes a new message and predicts whether it is spam or not.\n",
    "    - Plug the values that we calculated previously into the equation.\n",
    "    - Return $P(Spam|w_1, w_2, \\dots , w_n)$, $P(Ham|w_1, w_2, \\dots , w_n)$, and the prediction (\"spam\" or \"ham\").\n",
    "1. **Evaluate** the model using the testing data.\n",
    "    - Make predictions for all messages in the testing set.\n",
    "    - Divide the number of correct predictions by the total number of predictions. This will result in the accuracy of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "875dda10",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "In the first step, we will calculate the parameters of the model, which include the following.\n",
    "\n",
    "- $P(Spam), P(Ham)$\n",
    "- $N_{Vocabulary}$\n",
    "- $N_{Spam}, N_{Ham}$\n",
    "- $N_{w_i|Spam}, N_{w_i|Ham}$\n",
    "\n",
    "We will calculate these values first so that we can plug them into the equation later on when we predict whether new messages are spam or non-spam.\n",
    "\n",
    "### $P_{Spam}, P_{Ham}$\n",
    "\n",
    "The probability of spam is equal to the number of spam messages over the total number of messages. The same goes for ham messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27625206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam) = 13.41%\n",
      "P(Ham) = 86.59%\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "p_label = {}\n",
    "p_label[\"spam\"] = y_train.eq(\"spam\").sum() / y_train.shape[0]\n",
    "p_label[\"ham\"] = 1 - p_label[\"spam\"]\n",
    "\n",
    "print(f\"P(Spam) = {p_label['spam'] * 100:.2f}%\")\n",
    "print(f\"P(Ham) = {p_label['ham'] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d636af99",
   "metadata": {},
   "source": [
    "### Message Preprocessing\n",
    "\n",
    "Below are the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2199aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marvel Mobile Play the official Ultimate Spide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you, winner notified by sms. Good Luck! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free msg. Sorry, a service you ordered from 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thanks for your ringtone order, ref number R83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRIVATE! Your 2003 Account Statement for shows...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sms\n",
       "0  Marvel Mobile Play the official Ultimate Spide...\n",
       "1  Thank you, winner notified by sms. Good Luck! ...\n",
       "2  Free msg. Sorry, a service you ordered from 81...\n",
       "3  Thanks for your ringtone order, ref number R83...\n",
       "4  PRIVATE! Your 2003 Account Statement for shows..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e15d584d",
   "metadata": {},
   "source": [
    "In order to get individual words, we make all words lowercase and remove punctuation marks and other non-word characters. We then turn each message into a list of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e4708f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[marvel, mobile, play, the, official, ultimate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[thank, you, winner, notified, by, sms, good, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[free, msg, sorry, a, service, you, ordered, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[thanks, for, your, ringtone, order, ref, numb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[private, your, 2003, account, statement, for,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sms\n",
       "0  [marvel, mobile, play, the, official, ultimate...\n",
       "1  [thank, you, winner, notified, by, sms, good, ...\n",
       "2  [free, msg, sorry, a, service, you, ordered, f...\n",
       "3  [thanks, for, your, ringtone, order, ref, numb...\n",
       "4  [private, your, 2003, account, statement, for,..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "def preprocess_messages(series):\n",
    "    result = (\n",
    "        series\n",
    "        .str.lower()\n",
    "        # Delete all non-word characters.\n",
    "        .str.replace(r\"[^a-z0-9 ]\", \"\", regex = True)\n",
    "        .str.strip()\n",
    "        .str.split()\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "X_train = pd.DataFrame(preprocess_messages(X_train.sms))\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6688f586",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Using the preprocessed messages, we can form a set of all of the unique words that they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e51db73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the vocabulary: 8385\n",
      "First few items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['silent',\n",
       " 'receivea',\n",
       " 'dismay',\n",
       " 'noise',\n",
       " 'platt',\n",
       " 'li',\n",
       " 'woohoo',\n",
       " 'ucall',\n",
       " 'wondarfull',\n",
       " '80082']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "vocab = set()\n",
    "for lst in X_train.sms:\n",
    "    vocab.update(lst)\n",
    "\n",
    "# Use a Series to delete items that are blank or only contain whitespace.\n",
    "vocab_series = pd.Series(list(vocab))\n",
    "vocab_series = vocab_series.loc[~vocab_series.str.match(\"^\\s*$\")]\n",
    "vocab = set(vocab_series)\n",
    "\n",
    "n_vocab = len(vocab)\n",
    "\n",
    "print(f\"Number of words in the vocabulary: {n_vocab}\\nFirst few items:\")\n",
    "list(vocab)[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6960ae6",
   "metadata": {},
   "source": [
    "Above are the first 10 items in the vocabulary. In total, $N_{Vocabulary} = 8385$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dd39ffb",
   "metadata": {},
   "source": [
    "### $N_{Spam}, N_{Ham}$\n",
    "\n",
    "Using the vocabulary, we can transform the messages to show the number of times that each word appears in each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b13e04a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>zebra</th>\n",
       "      <th>zed</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  008704050406  0089my  0121  01223585236  01223585334  0125698789  \\\n",
       "0  0             0       0     0            0            0           0   \n",
       "1  0             0       0     0            0            0           0   \n",
       "2  0             0       0     0            0            0           0   \n",
       "3  0             0       0     0            0            0           0   \n",
       "4  0             0       0     0            0            0           0   \n",
       "\n",
       "   020603  0207  02070836089  ...  zebra  zed  zeros  zhong  zindgi  zoe  \\\n",
       "0       0     0            0  ...      0    0      0      0       0    0   \n",
       "1       0     0            0  ...      0    0      0      0       0    0   \n",
       "2       0     0            0  ...      0    0      0      0       0    0   \n",
       "3       0     0            0  ...      0    0      0      0       0    0   \n",
       "4       0     0            0  ...      0    0      0      0       0    0   \n",
       "\n",
       "   zogtorius  zoom  zouk  zyada  \n",
       "0          0     0     0      0  \n",
       "1          0     0     0      0  \n",
       "2          0     0     0      0  \n",
       "3          0     0     0      0  \n",
       "4          0     0     0      0  \n",
       "\n",
       "[5 rows x 8385 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "def to_word_counts(series, vocab = vocab):\n",
    "    vocab_lst = list(sorted(vocab))\n",
    "\n",
    "    word_counts = pd.DataFrame({\n",
    "        w: [0] * series.shape[0]\n",
    "        for w in vocab_lst\n",
    "    })\n",
    "\n",
    "    for index, word_lst in series.iteritems():\n",
    "        for w in word_lst:\n",
    "            if w in vocab:\n",
    "                word_counts.loc[index, w] += 1\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "word_counts = to_word_counts(X_train.sms)\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95e4b74d",
   "metadata": {},
   "source": [
    "In the table above, each row represents a message. Each column represents a unique word in the vocabulary. The cells show the number of times that each word appeared in each message.\n",
    "\n",
    "Now, we can calculate $N_{Spam}, N_{Ham}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a7dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in spam messages: 14037\n",
      "Number of words in ham messages: 53977\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "def count_n(label, word_counts = word_counts):\n",
    "    n_label = (\n",
    "        word_counts\n",
    "        .loc[y_train == label, :]\n",
    "        # Sum all of the numbers in the df.\n",
    "        .sum()\n",
    "        .sum()\n",
    "    )\n",
    "    return n_label\n",
    "\n",
    "n_label = {}\n",
    "\n",
    "for label in [\"spam\", \"ham\"]:\n",
    "    n_label[label] = count_n(label)\n",
    "\n",
    "print(f\"Number of words in spam messages: {n_label['spam']}\")\n",
    "print(f\"Number of words in ham messages: {n_label['ham']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13794f69",
   "metadata": {},
   "source": [
    "The result is that $N_{Spam} = 14037$ and $N_{Ham} = 53977$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9a138fd",
   "metadata": {},
   "source": [
    "### $N_{w_i|Spam}, N_{w_i|Ham}$\n",
    "\n",
    "Finally, we can use the word counts to determine these two parameters. Recall that $N_{w_i|Spam}$ is the number of times that a certain word $w_i$ appeared in the spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a37699cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>zebra</th>\n",
       "      <th>zed</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 8385 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  008704050406  0089my  0121  01223585236  01223585334  0125698789  \\\n",
       "label                                                                        \n",
       "ham    0             0       0     0            0            0           1   \n",
       "spam   3             1       1     1            1            1           0   \n",
       "\n",
       "       020603  0207  02070836089  ...  zebra  zed  zeros  zhong  zindgi  zoe  \\\n",
       "label                             ...                                          \n",
       "ham         0     0            0  ...      0    0      1      1       1    0   \n",
       "spam        4     2            1  ...      1    4      0      0       0    1   \n",
       "\n",
       "       zogtorius  zoom  zouk  zyada  \n",
       "label                                \n",
       "ham            1     1     0      1  \n",
       "spam           0     0     1      0  \n",
       "\n",
       "[2 rows x 8385 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "full_train = pd.concat(\n",
    "    [y_train, word_counts],\n",
    "    axis = 1,\n",
    ")\n",
    "\n",
    "n_word_given_label = full_train.pivot_table(\n",
    "    values = vocab_lst,\n",
    "    index = \"label\",\n",
    "    aggfunc = np.sum,\n",
    ")\n",
    "\n",
    "n_word_given_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50af5094",
   "metadata": {},
   "source": [
    "The table above can be used to access these parameters. For example, if we want to access $N_{w_i | Spam}$ for the word \"hello\", we can look at the value where the \"spam\" row and \"hello\" column intersect. The value is the number of times that \"hello\" appeared in the spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b8585a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times that 'hello' appeared in spam messages: 3\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "n_hello_spam = n_word_given_label.at[\"spam\", \"hello\"]\n",
    "\n",
    "print(f\"Number of times that 'hello' appeared in spam messages: {n_hello_spam}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ce7c274",
   "metadata": {},
   "source": [
    "## Predictive Function\n",
    "\n",
    "Now that all of the parameters have been found, we can write a function that will take a new message and classify it as spam or non-spam. This function will use the formulas explained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a99ad01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word_lst, out = \"both\", alpha = 1, vocab = vocab, p_label = p_label, n_label = n_label, n_word_given_label = n_word_given_label):\n",
    "    \"\"\"Given the list of words in a message, predict whether it is spam or ham.\n",
    "word_lst: The preprocessed list of words in the message.\n",
    "out: \"both\" to output both probabilities and prediction in a tuple. \"pred\" to output only the prediction as a string.\"\"\"\n",
    "\n",
    "    # Set up a Series to store results\n",
    "    results = pd.Series(dtype = np.float64)\n",
    "\n",
    "    for label in [\"spam\", \"ham\"]:\n",
    "        # Use P(Spam) or P(Ham)\n",
    "        final = p_label[label]\n",
    "\n",
    "        # Iterate through words in the message.\n",
    "        for w in word_lst:\n",
    "            # Only include a word if it is already in the vocabulary.\n",
    "            if w in vocab:\n",
    "                # Calculate P(w1, w2, ..., wn | Spam) using the formula.\n",
    "                p_word_given_label = (\n",
    "                    (n_word_given_label.at[label, w] + alpha)\n",
    "                    / (n_label[label] + alpha * n_vocab)\n",
    "                )\n",
    "\n",
    "                # Multiply the result into the final value.\n",
    "                final *= p_word_given_label\n",
    "\n",
    "        results[label] = final\n",
    "    \n",
    "    # The prediction is the label with the higher probability in the Series.\n",
    "    # If the probabilities are equal, the prediction is \"uncertain\"\n",
    "    if results[\"spam\"] == results[\"ham\"]:\n",
    "        prediction = \"uncertain\"\n",
    "    else:\n",
    "        prediction = results.idxmax()\n",
    "\n",
    "    if out == \"both\":\n",
    "        return results, prediction\n",
    "    elif out == \"pred\":\n",
    "        return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "867e3041",
   "metadata": {},
   "source": [
    "Let us try using this function to predict whether a message is spam or ham. We will use this example: \"you won a prize claim it now by sending credit card details\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f942c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "P(spam | message) is proportional to 2.3208952599406518e-35\n",
      "P(ham | message) is proportional to 1.8781562825001382e-41\n",
      "This message is predicted to be spam.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "results, prediction = predict(\"you won a prize claim it now by sending credit card details\".split())\n",
    "\n",
    "print(\"Results:\")\n",
    "for label, value in results.iteritems():\n",
    "    print(f\"P({label} | message) is proportional to {value}\")\n",
    "print(f\"This message is predicted to be {prediction}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3101be84",
   "metadata": {},
   "source": [
    "The algorithm determined that $P(Spam|w_1, w_2, \\dots , w_n) \\propto 2.32 \\cdot 10^{-35}$, whereas $P(Ham|w_1, w_2, \\dots , w_n) \\propto 1.88 \\cdot 10^{-41}$. Since the probability for spam was higher, it predicted that the message was spam."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11ad70ca",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "The final step is to evaluate the predictive function. We will use the function to predict labels for the messages in the testing set. Then, we will show the predicted labels side-by-side with the real labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05295fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>SMS. ac Sptv: The New Jersey Devils and the De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! Your Mobile No. was awarded £2000 Bonu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>spam</td>\n",
       "      <td>Sunshine Quiz Wkly Q! Win a top Sony DVD playe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label prediction                                                sms\n",
       "0  spam       spam  England v Macedonia - dont miss the goals/team...\n",
       "1  spam       spam  SMS. ac Sptv: The New Jersey Devils and the De...\n",
       "2  spam       spam  Please call our customer service representativ...\n",
       "3  spam       spam  URGENT! Your Mobile No. was awarded £2000 Bonu...\n",
       "4  spam       spam  Sunshine Quiz Wkly Q! Win a top Sony DVD playe..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "# Preprocess testing messages\n",
    "X_test_preprocessed = preprocess_messages(X_test.sms)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = X_test_preprocessed.apply(predict, out = \"pred\")\n",
    "y_pred.name = \"prediction\"\n",
    "\n",
    "# Concatenate\n",
    "full_test = pd.concat(\n",
    "    [y_test, y_pred, X_test],\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "full_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecae71e0",
   "metadata": {},
   "source": [
    "The table above shows the first 5 rows of the testing set. We can see that the algorithm correctly predicted that the first 5 rows were spam.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "We will now calculate the overall accuracy of the model by dividing the number of correct predictions by the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "351d806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.74%\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "acc = y_test.eq(y_pred).sum() / y_pred.shape[0] * 100\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836d11d9",
   "metadata": {},
   "source": [
    "The model turned out to have a very high accuracy of 98.74%. This shows that it is effective at filtering spam from non-spam.\n",
    "\n",
    "However, considering that spam and non-spam did not have equal representation in the data, with only 13% of all messages being spam, the accuracy may be misleading (Vallantin, 2018). For example, if a model is instructed to always predict that a message is ham, it would still have an accuracy of around 87%. Thus, to get a better picture of the performance of the model, we will make a confusion matrix and use other evaluation metrics such as precision, recall, and F1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5d78626",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that gives insight into how well a model was able to predict labels for datapoints (Brownlee, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c039a737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "cm = confusion_matrix(y_test, y_pred, labels = [\"ham\", \"spam\"])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels = [\"ham\", \"spam\"])\n",
    "disp.plot()\n",
    "\n",
    "plt.title(\"Confusion Matrix of Results\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8662c97",
   "metadata": {},
   "source": [
    "In this case, we only have two labels (spam and ham), so there are two rows and two columns. The vertical axis refers to the true labels, whereas the horizontal axis refers to the predicted labels.\n",
    "\n",
    "For example, if we want to know how many true spam messages were incorrectly predicted to be ham, we can look at the \"spam\" row and \"ham\" column. The value is 13, which means that 13 messages were incorrectly predicted in this way.\n",
    "\n",
    "If we consider ham messages as \"negative\" and spam messages as \"positive,\" then the confusion matrix above shows the True Negatives (TN) in the top left, False Negatives (FN) in the top right, False Positives (FP) in the bottom left, and True Positives (TP) in the bottom right."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26baf92f",
   "metadata": {},
   "source": [
    "### Recall, Precision, F1\n",
    "\n",
    "Recall, Precision, and F1 are other metrics of model performance which may be useful when the labels are not represented equally in the data. Their equations are shown below; these were taken from Brownlee (2020).\n",
    "\n",
    "Recall is the number of true positives divided by the total number of actual positives. It measures the \"ability of the classifier to find all the positive samples. (scikit-learn developers, 2021)\"\n",
    "\n",
    "$Recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "Precision is the number of true positives divided by the total number of predicted positives. It is the \"ability of the classifier not to label as positive a sample that is negative\" (scikit-learn developers, 2021). In other words, it is the ability of the model to avoid Type I error (Bhandari, 2021).\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "Finally, $F_{\\beta}$ is the \"weighted harmonic mean of the precision and recall,\" and $F_1$ is the version where both of these factors are given equal importance (scikit-learn developers, 2021).\n",
    "\n",
    "$F1 = \\frac{2 TP}{2 TP + FP + FN}$\n",
    "\n",
    "Let us calculate these metrics for the spam filter using the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec3cc952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 91.28%\n",
      "precision: 99.27%\n",
      "f1: 95.10%\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "tp = 136\n",
    "tn = 964\n",
    "fp = 1\n",
    "fn = 13\n",
    "\n",
    "metrics = {}\n",
    "metrics[\"recall\"] = tp / (tp + fn)\n",
    "metrics[\"precision\"] = tp / (tp + fp)\n",
    "metrics[\"f1\"] = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "for key in metrics:\n",
    "    print(f\"{key}: {metrics[key] * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90169325",
   "metadata": {},
   "source": [
    "All three metrics are above 90%. In particular, precision is 99.27%, which means that the model is very good at avoiding labelling ham messages as spam.\n",
    "\n",
    "Therefore, we can say that the naive bayes algorithm performs very well with the data that it was given."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e102220",
   "metadata": {},
   "source": [
    "# scikit-learn Classes for Naive Bayes\n",
    "\n",
    "In practice, a data scientist would not code the entire algorithm each time they use it, since this is time-consuming. Instead, one can use prepackaged classes in scikit-learn.\n",
    "\n",
    "For the purpose of classifying spam based on word counts, we can apply the following classes, according to the scikit-learn developers (2021):\n",
    "\n",
    "- `MultinomialNB`: This uses the same logic as the algorithm explained earlier.\n",
    "- `GaussianNB`: \"The likelihood of the features is assumed to be Gaussian,\" so the mean and standard deviation of each features is considered.\n",
    "- `ComplementNB`: It is an \"adaptation of the standard multinomial naive Bayes (MNB) algorithm that is particularly suited for imbalanced data sets.\" This may be useful for our dataset because the percentages of spam and non-spam messages are imbalanced.\n",
    "\n",
    "Other classes which will not be used in this project include `BernoulliNB`, which assumes each feature to be binary (only two categories), and `CategoricalNB`, which assumes that each feature is categorical rather than numeric.\n",
    "\n",
    "Let us try using the first three classes and comparing them. We will also use functions from `sklearn.metrics` to calculate accuracy score and the other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c44dcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial</th>\n",
       "      <td>98.743268</td>\n",
       "      <td>99.270073</td>\n",
       "      <td>91.275168</td>\n",
       "      <td>95.104895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaussian</th>\n",
       "      <td>90.843806</td>\n",
       "      <td>60.444444</td>\n",
       "      <td>91.275168</td>\n",
       "      <td>72.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Complement</th>\n",
       "      <td>97.396768</td>\n",
       "      <td>88.461538</td>\n",
       "      <td>92.617450</td>\n",
       "      <td>90.491803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Accuracy  Precision     Recall         F1\n",
       "Multinomial  98.743268  99.270073  91.275168  95.104895\n",
       "Gaussian     90.843806  60.444444  91.275168  72.727273\n",
       "Complement   97.396768  88.461538  92.617450  90.491803"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "nb_classes = {\n",
    "    \"Multinomial\": MultinomialNB,\n",
    "    \"Gaussian\": GaussianNB,\n",
    "    \"Complement\": ComplementNB,\n",
    "}\n",
    "\n",
    "metric_funcs = {\n",
    "    \"Accuracy\": accuracy_score,\n",
    "    \"Precision\": precision_score,\n",
    "    \"Recall\": recall_score,\n",
    "    \"F1\": f1_score,\n",
    "}\n",
    "\n",
    "result_rows = []\n",
    "\n",
    "test_word_counts = to_word_counts(X_test_preprocessed)\n",
    "\n",
    "for class_name in nb_classes:\n",
    "    nb = nb_classes[class_name]\n",
    "    model = nb()\n",
    "    model.fit(\n",
    "        word_counts,\n",
    "        y_train,\n",
    "    )\n",
    "    y_pred = model.predict(test_word_counts)\n",
    "\n",
    "    metric_results = pd.Series(name = class_name, dtype = \"float64\")\n",
    "\n",
    "    for metric_name, score_func in metric_funcs.items():\n",
    "        if metric_name == \"Accuracy\":\n",
    "            score = score_func(\n",
    "                y_true = y_test,\n",
    "                y_pred = y_pred,\n",
    "            )\n",
    "        else:\n",
    "            score = score_func(\n",
    "                y_true = y_test,\n",
    "                y_pred = y_pred,\n",
    "                pos_label = \"spam\",\n",
    "            )\n",
    "        metric_results[metric_name] = score * 100\n",
    "\n",
    "    result_rows.append(metric_results)\n",
    "\n",
    "result_df = pd.DataFrame(result_rows)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398b08f6",
   "metadata": {},
   "source": [
    "`MultinomialNB` performed the best, as it had the highest accuracy, precision, and F1 score. Its accuracy was 98.74%, which is the same as the accuracy of the algorithm that I manually coded earlier. Thus, I know that what I coded was correct.\n",
    "\n",
    "`ComplementNB` also performed well, though its precision was 88%, meaning that it was more prone to labeling ham messages as spam. However, the `GaussianNB` class was the worst performing one, as its precision was 60% and its F1 was 73%. This likely occurred because the assumption that the features' likelihoods were Gaussian did not hold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0eaef9d2",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project, we obtained a dataset of spam and non-spam messages, used this to calculate the parameters for a multinomial naive bayes algorithm, wrote a function to predict whether a new message is spam or non-spam, evaluated the model using various performance metrics, and tried three variations of the model in scikit-learn.\n",
    "\n",
    "Naive bayes is just one of various classification models, such as SVM, logistic regression, random forest, etc. It is useful as most of its calculations are done during the training stage, so it is fast at making predictions. However, it is not perfect, as the assumption that the features are independent usually does not hold true in real life. (Gupta, 2020)\n",
    "\n",
    "However, models cannot represent reality perfectly. Simplifications like this are good enough if they are useful in making predictions or testing relationships among variables.\n",
    "\n",
    "Another important insight from this project is that accuracy should not be the only metric used to evaluate model performance. We should use other metrics like recall, precision, and F1 score, which provide more nuanced perspectives. Though this was not done for this project, in other scenarios, we may have to adjust the model to favor either recall or precision depending on the specific use case.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fc058d5",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\n",
    "## Data Source\n",
    "\n",
    "Almeida, T. A., & Hidalgo, J. M. G. (2012, June 22). SMS Spam Collection Data Set. UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/sms+spam+collection#\n",
    "\n",
    "## Information Sources\n",
    "\n",
    "Bhandari, P. (2021, January 18). Type I and Type II errors. Scribbr. https://www.scribbr.com/statistics/type-i-and-type-ii-errors/\n",
    "\n",
    "Brownlee, J. (2016, November 17). What is a Confusion Matrix in Machine Learning. Machine Learning Mastery. https://machinelearningmastery.com/confusion-matrix-machine-learning/\n",
    "\n",
    "Brownlee, J. (2020, August 27). How to Calculate Precision, Recall, F1, and More for Deep Learning Models. Machine Learning Mastery. https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/\n",
    "\n",
    "Dataquest. (n.d.). Guided Project: Building A Spam Filter With Naive Bayes. Dataquest. Retrieved December 14, 2021, from https://www.dataquest.io/c/74/m/433/guided-project%3A-building-a-spam-filter-with-naive-bayes\n",
    "\n",
    "Devanesan, J. (2020, August 19). Phishing scams dominate the Philippines cybercrime landscape. Tech Wire Asia. https://techwireasia.com/2020/08/phishing-scams-dominate-the-philippines-cybercrime-landscape/\n",
    "\n",
    "FTC CI. (2020, February 19). How To Recognize and Report Spam Text Messages. Federal Trade Commission Consumer Information. https://www.consumer.ftc.gov/articles/how-recognize-and-report-spam-text-messages\n",
    "\n",
    "Gupta, S. (2020, February 28). Pros and cons of various Classification ML algorithms. Medium. https://towardsdatascience.com/pros-and-cons-of-various-classification-ml-algorithms-3b5bfb3c87d6\n",
    "\n",
    "scikit-learn developers. (2021). 1.9. Naive Bayes. Scikit-Learn. https://scikit-learn/stable/modules/naive_bayes.html\n",
    "\n",
    "scikit-learn developers. (2021). 3.3. Metrics and scoring: Quantifying the quality of predictions. Scikit-Learn. https://scikit-learn/stable/modules/model_evaluation.html\n",
    "\n",
    "Thomas, L. (2020, September 18). How to use stratified sampling. Scribbr. https://www.scribbr.com/methodology/stratified-sampling/\n",
    "Vadapalli, P. (2021, January 5). Naive Bayes Explained: Function, Advantages & Disadvantages, Applications in 2021. UpGrad Blog. https://www.upgrad.com/blog/naive-bayes-explained/\n",
    "\n",
    "Vallantin, L. (2018, September 6). Why you should not trust only in accuracy to measure machine learning performance. Medium. https://medium.com/@limavallantin/why-you-should-not-trust-only-in-accuracy-to-measure-machine-learning-performance-a72cf00b4516\n",
    "\n",
    "Yap, C. (2021, November 25). Millions of spam messages blocked in Philippines as scams surge. Yahoo News. https://ph.news.yahoo.com/millions-of-spam-messages-blocked-in-philippines-as-scams-surge-005658569.html\n",
    "\n",
    "## Image Source\n",
    "\n",
    "Johnson, H. (2020, February 24). Photo by Hannes Johnson on Unsplash. Unsplash. https://unsplash.com/photos/mRgffV3Hc6c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ef021f4",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "Here, I explain how the multinomial naive bayes algorithm was derived from the Bayes Theorem. This was originally placed earlier in the post, but I deemed it to be unnecessary for that part.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "The mathematical explanations below are not my own ideas. I learned these from the Dataquest course on Naive Bayes.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef30e02e",
   "metadata": {},
   "source": [
    "\n",
    "Given two events $A$ and $B$, we can use the theorem to determine the probability that $B$ happened given that $A$ happened. This probability is written as $P(B|A)$.\n",
    "\n",
    "$P(B|A) = \\frac{P(B) \\cdot P(A|B)}{\\Sigma_{i = 1}^n (P(B_i) \\cdot P(A|B_i))}$\n",
    "\n",
    "In this case, $B_1$ is the event that the message is non-spam, and $B_2$ is the event that it is spam. $B$ can refer to either $B_1$ or $B_2$, depending on which probability we want to calculate. Also, $A$ refers to the specific contents of one message.\n",
    "\n",
    "In order to make things clearer, let us say that $Spam$ is the event that the message is spam, and $Ham$ is the event that the message is non-spam.\n",
    "\n",
    "Then, let us expand event $A$ (the message itself) in order to consider the individual words inside it. For example, the first word in a message can be labeled $w_1$. If we have a total of $n$ words, then the words can be labeled as $w_1, w_2, \\dots , w_n$.\n",
    "\n",
    "Thus, we can rewrite the equation. Here is the probability of a given message being spam:\n",
    "\n",
    "$P(Spam|w_1, w_2, \\dots , w_n) = \\frac{P(Spam) \\cdot P(w_1, w_2, \\dots , w_n|Spam)}{\\Sigma_{i = 1}^n (P(B_i) \\cdot P(w_1, w_2, \\dots , w_n|B_i))}$\n",
    "\n",
    "Here is the probability of a given message being non-spam:\n",
    "\n",
    "$P(Ham|w_1, w_2, \\dots , w_n) = \\frac{P(Ham) \\cdot P(w_1, w_2, \\dots , w_n|Ham)}{\\Sigma_{i = 1}^n (P(B_i) \\cdot P(w_1, w_2, \\dots , w_n|B_i))}$\n",
    "\n",
    "Notice that the denominators are the same. Since we only want to compare these two probabilities, we can skip calculating the denominator and just calculate the numerators. We can thus rewrite the equation as follows. Note that the $\\propto$ symbol is used instead of $=$ because the two quantities are not equal but directly proportional.\n",
    "\n",
    "$P(Spam|w_1, w_2, \\dots , w_n) \\propto P(Spam) \\cdot P(w_1, w_2, \\dots , w_n|Spam)$\n",
    "\n",
    "The first factor, $P(Spam)$, is easy to find, as it is simply the number of spam messages divided by the total number of messages. However, $P(w_1, w_2, \\dots , w_n|Spam)$ needs to be further expanded.\n",
    "\n",
    "If we make the assumption that the probability of each word is independent of the probability of the other words, we can use the multiplication rule. The assumption of independence is what makes the algorithm \"naive,\" as it usually doesn't hold true in reality. However, the algorithm is still useful for predictions despite this.\n",
    "\n",
    "$P(Spam) \\cdot P(w_1, w_2, \\dots , w_n|Spam) \\\\ = P(Spam) \\cdot P(w_1 \\cap w_2 \\cap \\dots \\cap w_n | Spam) \\\\ = P(Spam) \\cdot P(w_1|Spam) \\cdot P(w_2|Spam) \\cdot \\dots \\cdot P(w_n|Spam)$\n",
    "\n",
    "Note that we still have to find the probability of each word given $Spam$ because we assume that the presence of each word is dependent on $Spam$.\n",
    "\n",
    "Thus, the final formula is:\n",
    "\n",
    "$P(Spam|w_1, w_2, \\dots , w_n) \\propto P(Spam) \\cdot \\Pi_{i=1}^n P(w_i|Spam)$\n",
    "\n",
    "Likewise, the formula for $Ham$ is:\n",
    "\n",
    "$P(Ham|w_1, w_2, \\dots , w_n) \\propto P(Ham) \\cdot \\Pi_{i=1}^n P(w_i|Ham)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
